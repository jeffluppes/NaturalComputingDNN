_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                80        
_________________________________________________________________
activation_1 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_2 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_3 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 34        
=================================================================
Total params: 658
Trainable params: 658
Non-trainable params: 0
_________________________________________________________________
None
Training for 100000 steps ...
    18/100000: episode: 1, duration: 0.917s, episode steps: 18, steps per second: 20, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.062 [-1.029, 1.588], loss: --, mean_absolute_error: --, mean_q: --
    47/100000: episode: 2, duration: 0.006s, episode steps: 29, steps per second: 5110, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.021 [-1.659, 1.166], loss: --, mean_absolute_error: --, mean_q: --
    81/100000: episode: 3, duration: 0.007s, episode steps: 34, steps per second: 5140, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.058 [-0.933, 1.551], loss: --, mean_absolute_error: --, mean_q: --
   101/100000: episode: 4, duration: 0.004s, episode steps: 20, steps per second: 4779, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.043 [-2.511, 1.583], loss: --, mean_absolute_error: --, mean_q: --
   124/100000: episode: 5, duration: 0.005s, episode steps: 23, steps per second: 5012, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.696 [0.000, 1.000], mean observation: -0.045 [-2.804, 1.934], loss: --, mean_absolute_error: --, mean_q: --
   134/100000: episode: 6, duration: 0.002s, episode steps: 10, steps per second: 4691, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.137 [-0.956, 1.570], loss: --, mean_absolute_error: --, mean_q: --
   175/100000: episode: 7, duration: 0.008s, episode steps: 41, steps per second: 5133, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.390 [0.000, 1.000], mean observation: -0.105 [-1.954, 2.081], loss: --, mean_absolute_error: --, mean_q: --
   188/100000: episode: 8, duration: 0.003s, episode steps: 13, steps per second: 4328, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-1.690, 0.956], loss: --, mean_absolute_error: --, mean_q: --
   199/100000: episode: 9, duration: 0.003s, episode steps: 11, steps per second: 4392, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.121 [-2.064, 1.390], loss: --, mean_absolute_error: --, mean_q: --
   209/100000: episode: 10, duration: 0.002s, episode steps: 10, steps per second: 4769, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.136 [-1.378, 2.109], loss: --, mean_absolute_error: --, mean_q: --
   241/100000: episode: 11, duration: 0.006s, episode steps: 32, steps per second: 5163, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.165 [-0.806, 1.983], loss: --, mean_absolute_error: --, mean_q: --
   255/100000: episode: 12, duration: 0.003s, episode steps: 14, steps per second: 4591, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.104 [-1.184, 1.895], loss: --, mean_absolute_error: --, mean_q: --
   271/100000: episode: 13, duration: 0.003s, episode steps: 16, steps per second: 4963, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.091 [-2.167, 1.348], loss: --, mean_absolute_error: --, mean_q: --
   289/100000: episode: 14, duration: 0.004s, episode steps: 18, steps per second: 5005, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.067 [-2.012, 1.218], loss: --, mean_absolute_error: --, mean_q: --
   309/100000: episode: 15, duration: 0.004s, episode steps: 20, steps per second: 5025, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.080 [-1.383, 0.924], loss: --, mean_absolute_error: --, mean_q: --
   322/100000: episode: 16, duration: 0.003s, episode steps: 13, steps per second: 4593, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.100 [-1.791, 0.957], loss: --, mean_absolute_error: --, mean_q: --
   354/100000: episode: 17, duration: 0.006s, episode steps: 32, steps per second: 5131, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-0.939, 0.560], loss: --, mean_absolute_error: --, mean_q: --
   399/100000: episode: 18, duration: 0.009s, episode steps: 45, steps per second: 5228, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.422 [0.000, 1.000], mean observation: -0.035 [-1.513, 1.633], loss: --, mean_absolute_error: --, mean_q: --
   439/100000: episode: 19, duration: 0.008s, episode steps: 40, steps per second: 4809, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.015 [-0.937, 1.469], loss: --, mean_absolute_error: --, mean_q: --
   453/100000: episode: 20, duration: 0.003s, episode steps: 14, steps per second: 4884, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.076 [-1.392, 2.151], loss: --, mean_absolute_error: --, mean_q: --
   472/100000: episode: 21, duration: 0.004s, episode steps: 19, steps per second: 4951, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.135 [-1.201, 0.372], loss: --, mean_absolute_error: --, mean_q: --
   535/100000: episode: 22, duration: 0.012s, episode steps: 63, steps per second: 5198, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.058 [-1.554, 1.317], loss: --, mean_absolute_error: --, mean_q: --
   556/100000: episode: 23, duration: 0.004s, episode steps: 21, steps per second: 4902, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.131 [-0.413, 1.046], loss: --, mean_absolute_error: --, mean_q: --
   571/100000: episode: 24, duration: 0.003s, episode steps: 15, steps per second: 4833, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.089 [-1.531, 1.006], loss: --, mean_absolute_error: --, mean_q: --
   589/100000: episode: 25, duration: 0.004s, episode steps: 18, steps per second: 4836, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.090 [-1.996, 1.185], loss: --, mean_absolute_error: --, mean_q: --
   604/100000: episode: 26, duration: 0.003s, episode steps: 15, steps per second: 4914, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.106 [-0.583, 1.083], loss: --, mean_absolute_error: --, mean_q: --
   620/100000: episode: 27, duration: 0.003s, episode steps: 16, steps per second: 4950, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.105 [-2.136, 1.204], loss: --, mean_absolute_error: --, mean_q: --
   669/100000: episode: 28, duration: 0.010s, episode steps: 49, steps per second: 5011, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.017 [-0.995, 1.458], loss: --, mean_absolute_error: --, mean_q: --
   678/100000: episode: 29, duration: 0.002s, episode steps: 9, steps per second: 4644, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.136 [-1.223, 1.978], loss: --, mean_absolute_error: --, mean_q: --
   704/100000: episode: 30, duration: 0.005s, episode steps: 26, steps per second: 5145, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.173, 0.787], loss: --, mean_absolute_error: --, mean_q: --
   749/100000: episode: 31, duration: 0.009s, episode steps: 45, steps per second: 5043, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.039 [-1.203, 2.122], loss: --, mean_absolute_error: --, mean_q: --
   765/100000: episode: 32, duration: 0.003s, episode steps: 16, steps per second: 4814, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.089 [-0.764, 1.247], loss: --, mean_absolute_error: --, mean_q: --
   785/100000: episode: 33, duration: 0.004s, episode steps: 20, steps per second: 5010, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.093 [-1.152, 0.611], loss: --, mean_absolute_error: --, mean_q: --
   824/100000: episode: 34, duration: 0.008s, episode steps: 39, steps per second: 4997, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: -0.085 [-2.143, 0.973], loss: --, mean_absolute_error: --, mean_q: --
   839/100000: episode: 35, duration: 0.003s, episode steps: 15, steps per second: 4837, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-1.769, 1.015], loss: --, mean_absolute_error: --, mean_q: --
   857/100000: episode: 36, duration: 0.004s, episode steps: 18, steps per second: 5024, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.081 [-1.399, 2.288], loss: --, mean_absolute_error: --, mean_q: --
   883/100000: episode: 37, duration: 0.005s, episode steps: 26, steps per second: 4806, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.236, 0.463], loss: --, mean_absolute_error: --, mean_q: --
   895/100000: episode: 38, duration: 0.003s, episode steps: 12, steps per second: 4443, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.118 [-2.556, 1.568], loss: --, mean_absolute_error: --, mean_q: --
   910/100000: episode: 39, duration: 0.003s, episode steps: 15, steps per second: 4923, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.093 [-1.185, 1.868], loss: --, mean_absolute_error: --, mean_q: --
   925/100000: episode: 40, duration: 0.003s, episode steps: 15, steps per second: 4925, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.080 [-1.524, 0.967], loss: --, mean_absolute_error: --, mean_q: --
   955/100000: episode: 41, duration: 0.006s, episode steps: 30, steps per second: 4907, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.080 [-0.600, 1.326], loss: --, mean_absolute_error: --, mean_q: --
   976/100000: episode: 42, duration: 0.004s, episode steps: 21, steps per second: 5014, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.112 [-1.850, 0.964], loss: --, mean_absolute_error: --, mean_q: --
   990/100000: episode: 43, duration: 0.003s, episode steps: 14, steps per second: 4936, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.079 [-1.654, 0.990], loss: --, mean_absolute_error: --, mean_q: --
  1022/100000: episode: 44, duration: 1.729s, episode steps: 32, steps per second: 19, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.045 [-1.481, 1.027], loss: 0.472550, mean_absolute_error: 0.538903, mean_q: 0.132222
  1043/100000: episode: 45, duration: 0.030s, episode steps: 21, steps per second: 691, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.066 [-1.478, 0.817], loss: 0.332822, mean_absolute_error: 0.547808, mean_q: 0.324494
  1074/100000: episode: 46, duration: 0.046s, episode steps: 31, steps per second: 669, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.710 [0.000, 1.000], mean observation: 0.054 [-3.046, 2.519], loss: 0.143285, mean_absolute_error: 0.581539, mean_q: 0.722713
  1105/100000: episode: 47, duration: 0.045s, episode steps: 31, steps per second: 682, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.003 [-1.212, 1.702], loss: 0.033043, mean_absolute_error: 0.696793, mean_q: 1.260118
  1123/100000: episode: 48, duration: 0.026s, episode steps: 18, steps per second: 681, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.093 [-1.798, 0.981], loss: 0.020656, mean_absolute_error: 0.763361, mean_q: 1.417142
  1140/100000: episode: 49, duration: 0.024s, episode steps: 17, steps per second: 695, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.080 [-2.164, 1.341], loss: 0.022094, mean_absolute_error: 0.839493, mean_q: 1.582950
  1190/100000: episode: 50, duration: 0.068s, episode steps: 50, steps per second: 732, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.134 [-2.552, 1.382], loss: 0.030117, mean_absolute_error: 0.948899, mean_q: 1.801035
  1213/100000: episode: 51, duration: 0.033s, episode steps: 23, steps per second: 706, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.028 [-1.408, 2.174], loss: 0.041910, mean_absolute_error: 1.102877, mean_q: 2.121446
  1237/100000: episode: 52, duration: 0.035s, episode steps: 24, steps per second: 695, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.033 [-1.579, 2.276], loss: 0.049323, mean_absolute_error: 1.196700, mean_q: 2.308433
  1248/100000: episode: 53, duration: 0.016s, episode steps: 11, steps per second: 701, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.120 [-0.821, 1.426], loss: 0.066522, mean_absolute_error: 1.266075, mean_q: 2.445680
  1261/100000: episode: 54, duration: 0.019s, episode steps: 13, steps per second: 701, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.095 [-1.817, 1.022], loss: 0.067265, mean_absolute_error: 1.338785, mean_q: 2.567065
  1285/100000: episode: 55, duration: 0.034s, episode steps: 24, steps per second: 702, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.051 [-1.645, 0.938], loss: 0.082481, mean_absolute_error: 1.420067, mean_q: 2.705598
  1297/100000: episode: 56, duration: 0.018s, episode steps: 12, steps per second: 653, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.110 [-1.175, 2.081], loss: 0.065401, mean_absolute_error: 1.491301, mean_q: 2.890247
  1325/100000: episode: 57, duration: 0.038s, episode steps: 28, steps per second: 737, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.048 [-1.740, 1.023], loss: 0.073749, mean_absolute_error: 1.573447, mean_q: 3.067246
  1364/100000: episode: 58, duration: 0.053s, episode steps: 39, steps per second: 730, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.013 [-0.948, 1.434], loss: 0.125521, mean_absolute_error: 1.729715, mean_q: 3.308834
  1376/100000: episode: 59, duration: 0.018s, episode steps: 12, steps per second: 657, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.109 [-2.082, 1.200], loss: 0.162643, mean_absolute_error: 1.827174, mean_q: 3.456882
  1406/100000: episode: 60, duration: 0.041s, episode steps: 30, steps per second: 728, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.633 [0.000, 1.000], mean observation: 0.025 [-2.241, 1.764], loss: 0.158170, mean_absolute_error: 1.951064, mean_q: 3.720726
  1419/100000: episode: 61, duration: 0.018s, episode steps: 13, steps per second: 722, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.137 [-2.376, 1.350], loss: 0.216052, mean_absolute_error: 2.064190, mean_q: 3.912278
  1434/100000: episode: 62, duration: 0.021s, episode steps: 15, steps per second: 713, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.108 [-0.549, 1.049], loss: 0.189848, mean_absolute_error: 2.101847, mean_q: 4.028939
  1465/100000: episode: 63, duration: 0.044s, episode steps: 31, steps per second: 710, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.058 [-1.402, 0.808], loss: 0.233454, mean_absolute_error: 2.234228, mean_q: 4.220315
  1482/100000: episode: 64, duration: 0.026s, episode steps: 17, steps per second: 647, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.081 [-1.018, 1.577], loss: 0.232386, mean_absolute_error: 2.350473, mean_q: 4.491486
  1497/100000: episode: 65, duration: 0.021s, episode steps: 15, steps per second: 708, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.078 [-2.177, 1.381], loss: 0.240848, mean_absolute_error: 2.399909, mean_q: 4.549251
  1511/100000: episode: 66, duration: 0.021s, episode steps: 14, steps per second: 682, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.092 [-2.245, 1.410], loss: 0.253689, mean_absolute_error: 2.499149, mean_q: 4.764084
  1522/100000: episode: 67, duration: 0.016s, episode steps: 11, steps per second: 697, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.259, 1.419], loss: 0.262107, mean_absolute_error: 2.551654, mean_q: 4.832337
  1533/100000: episode: 68, duration: 0.016s, episode steps: 11, steps per second: 708, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.105 [-1.798, 2.729], loss: 0.243764, mean_absolute_error: 2.615083, mean_q: 5.023704
  1545/100000: episode: 69, duration: 0.018s, episode steps: 12, steps per second: 664, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.112 [-1.151, 2.015], loss: 0.303709, mean_absolute_error: 2.613492, mean_q: 4.888847
  1554/100000: episode: 70, duration: 0.013s, episode steps: 9, steps per second: 683, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.144 [-1.524, 2.464], loss: 0.307829, mean_absolute_error: 2.692586, mean_q: 5.083174
  1579/100000: episode: 71, duration: 0.035s, episode steps: 25, steps per second: 711, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.088 [-2.021, 0.991], loss: 0.322158, mean_absolute_error: 2.781300, mean_q: 5.273852
  1591/100000: episode: 72, duration: 0.017s, episode steps: 12, steps per second: 702, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.114 [-1.597, 2.519], loss: 0.361110, mean_absolute_error: 2.846108, mean_q: 5.312982
  1638/100000: episode: 73, duration: 0.064s, episode steps: 47, steps per second: 735, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.029 [-1.288, 0.765], loss: 0.362879, mean_absolute_error: 2.969512, mean_q: 5.622171
  1654/100000: episode: 74, duration: 0.024s, episode steps: 16, steps per second: 680, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.097 [-1.304, 0.764], loss: 0.262673, mean_absolute_error: 3.110670, mean_q: 5.937069
  1671/100000: episode: 75, duration: 0.023s, episode steps: 17, steps per second: 729, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.053 [-1.190, 1.679], loss: 0.256920, mean_absolute_error: 3.181541, mean_q: 6.093367
  1702/100000: episode: 76, duration: 0.044s, episode steps: 31, steps per second: 711, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.097 [-0.709, 1.427], loss: 0.369747, mean_absolute_error: 3.266922, mean_q: 6.194497
  1716/100000: episode: 77, duration: 0.022s, episode steps: 14, steps per second: 647, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.121 [-1.201, 2.164], loss: 0.352759, mean_absolute_error: 3.374186, mean_q: 6.445406
  1738/100000: episode: 78, duration: 0.032s, episode steps: 22, steps per second: 690, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: 0.015 [-1.963, 1.584], loss: 0.459103, mean_absolute_error: 3.437410, mean_q: 6.464221
  1750/100000: episode: 79, duration: 0.020s, episode steps: 12, steps per second: 593, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.108 [-1.604, 2.634], loss: 0.465497, mean_absolute_error: 3.553960, mean_q: 6.758226
  1778/100000: episode: 80, duration: 0.041s, episode steps: 28, steps per second: 682, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.041 [-0.809, 1.423], loss: 0.493197, mean_absolute_error: 3.614093, mean_q: 6.830572
  1801/100000: episode: 81, duration: 0.035s, episode steps: 23, steps per second: 665, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.094 [-0.955, 0.550], loss: 0.463528, mean_absolute_error: 3.734037, mean_q: 7.063543
  1811/100000: episode: 82, duration: 0.015s, episode steps: 10, steps per second: 669, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.131 [-1.686, 0.981], loss: 0.354788, mean_absolute_error: 3.826416, mean_q: 7.271724
  1826/100000: episode: 83, duration: 0.023s, episode steps: 15, steps per second: 642, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.052 [-1.361, 1.956], loss: 0.558247, mean_absolute_error: 3.863490, mean_q: 7.236406
  1835/100000: episode: 84, duration: 0.014s, episode steps: 9, steps per second: 654, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.729, 2.747], loss: 0.458728, mean_absolute_error: 3.909574, mean_q: 7.453394
  1850/100000: episode: 85, duration: 0.022s, episode steps: 15, steps per second: 676, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.104 [-1.003, 1.885], loss: 0.399201, mean_absolute_error: 3.979774, mean_q: 7.586785
  1865/100000: episode: 86, duration: 0.022s, episode steps: 15, steps per second: 691, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.080 [-1.023, 1.580], loss: 0.711377, mean_absolute_error: 4.010068, mean_q: 7.503217
  1878/100000: episode: 87, duration: 0.019s, episode steps: 13, steps per second: 692, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.098 [-1.862, 1.151], loss: 0.443330, mean_absolute_error: 4.107790, mean_q: 7.786298
  1894/100000: episode: 88, duration: 0.024s, episode steps: 16, steps per second: 669, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.092 [-2.033, 1.192], loss: 0.635477, mean_absolute_error: 4.163964, mean_q: 7.797949
  1931/100000: episode: 89, duration: 0.053s, episode steps: 37, steps per second: 701, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.351 [0.000, 1.000], mean observation: -0.005 [-2.095, 3.019], loss: 0.679541, mean_absolute_error: 4.271069, mean_q: 7.998414
  1944/100000: episode: 90, duration: 0.022s, episode steps: 13, steps per second: 602, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.097 [-1.574, 0.952], loss: 0.519920, mean_absolute_error: 4.330872, mean_q: 8.167075
  1967/100000: episode: 91, duration: 0.036s, episode steps: 23, steps per second: 648, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.046 [-1.747, 1.147], loss: 0.799641, mean_absolute_error: 4.457373, mean_q: 8.294106
  1986/100000: episode: 92, duration: 0.028s, episode steps: 19, steps per second: 679, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.263 [0.000, 1.000], mean observation: 0.049 [-1.920, 2.755], loss: 0.579555, mean_absolute_error: 4.517957, mean_q: 8.541413
  2038/100000: episode: 93, duration: 0.072s, episode steps: 52, steps per second: 726, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.056 [-1.730, 2.473], loss: 0.648514, mean_absolute_error: 4.657280, mean_q: 8.784474
  2116/100000: episode: 94, duration: 0.108s, episode steps: 78, steps per second: 721, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.071 [-1.560, 1.774], loss: 0.700490, mean_absolute_error: 4.911880, mean_q: 9.265704
  2151/100000: episode: 95, duration: 0.050s, episode steps: 35, steps per second: 706, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.035 [-0.605, 1.247], loss: 0.780616, mean_absolute_error: 5.111997, mean_q: 9.651578
  2179/100000: episode: 96, duration: 0.040s, episode steps: 28, steps per second: 700, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: -0.006 [-1.184, 1.676], loss: 0.750971, mean_absolute_error: 5.239651, mean_q: 9.982154
  2201/100000: episode: 97, duration: 0.032s, episode steps: 22, steps per second: 684, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.048 [-1.355, 0.833], loss: 0.693723, mean_absolute_error: 5.357626, mean_q: 10.210509
  2306/100000: episode: 98, duration: 0.146s, episode steps: 105, steps per second: 721, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.212 [-0.989, 2.014], loss: 0.652758, mean_absolute_error: 5.625710, mean_q: 10.905153
  2455/100000: episode: 99, duration: 0.213s, episode steps: 149, steps per second: 701, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.260 [-1.569, 0.953], loss: 0.904044, mean_absolute_error: 6.267691, mean_q: 12.272618
  2524/100000: episode: 100, duration: 0.098s, episode steps: 69, steps per second: 701, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.290 [-1.502, 0.568], loss: 1.120136, mean_absolute_error: 6.813027, mean_q: 13.306818
  2615/100000: episode: 101, duration: 0.126s, episode steps: 91, steps per second: 724, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: 0.284 [-0.585, 2.012], loss: 1.057692, mean_absolute_error: 7.191014, mean_q: 14.133175
  2660/100000: episode: 102, duration: 0.064s, episode steps: 45, steps per second: 702, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.097 [-1.374, 0.714], loss: 1.348880, mean_absolute_error: 7.535977, mean_q: 14.894849
  2730/100000: episode: 103, duration: 0.101s, episode steps: 70, steps per second: 695, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.153 [-1.471, 0.794], loss: 1.327022, mean_absolute_error: 7.812878, mean_q: 15.399592
  2880/100000: episode: 104, duration: 0.208s, episode steps: 150, steps per second: 720, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.210 [-1.050, 1.870], loss: 1.346907, mean_absolute_error: 8.392276, mean_q: 16.569481
  3028/100000: episode: 105, duration: 0.208s, episode steps: 148, steps per second: 711, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.690, 1.238], loss: 1.440267, mean_absolute_error: 9.186334, mean_q: 18.244780
  3216/100000: episode: 106, duration: 0.267s, episode steps: 188, steps per second: 704, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.224 [-1.091, 1.664], loss: 1.568620, mean_absolute_error: 10.076097, mean_q: 20.112623
  3383/100000: episode: 107, duration: 0.229s, episode steps: 167, steps per second: 728, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.289 [-2.230, 0.724], loss: 1.579816, mean_absolute_error: 11.087170, mean_q: 22.240669
  3583/100000: episode: 108, duration: 0.276s, episode steps: 200, steps per second: 725, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.182 [-1.496, 1.057], loss: 2.084033, mean_absolute_error: 12.131617, mean_q: 24.313457
  3755/100000: episode: 109, duration: 0.242s, episode steps: 172, steps per second: 712, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.277 [-1.429, 1.702], loss: 2.134089, mean_absolute_error: 13.203588, mean_q: 26.511982
  3955/100000: episode: 110, duration: 0.291s, episode steps: 200, steps per second: 687, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.086 [-1.188, 1.002], loss: 2.600159, mean_absolute_error: 14.269597, mean_q: 28.629980
  4155/100000: episode: 111, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.149 [-0.982, 0.983], loss: 2.408878, mean_absolute_error: 15.416389, mean_q: 31.065855
  4354/100000: episode: 112, duration: 0.288s, episode steps: 199, steps per second: 691, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.251 [-0.979, 1.793], loss: 2.567390, mean_absolute_error: 16.519726, mean_q: 33.341579
  4554/100000: episode: 113, duration: 0.278s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.125 [-1.108, 0.862], loss: 2.525445, mean_absolute_error: 17.755047, mean_q: 35.903709
  4754/100000: episode: 114, duration: 0.286s, episode steps: 200, steps per second: 698, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.342 [-2.320, 0.794], loss: 2.935763, mean_absolute_error: 18.952557, mean_q: 38.300724
  4954/100000: episode: 115, duration: 0.287s, episode steps: 200, steps per second: 698, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.092 [-0.931, 0.833], loss: 3.677808, mean_absolute_error: 20.238960, mean_q: 40.919262
  5154/100000: episode: 116, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.057 [-0.970, 1.069], loss: 4.198987, mean_absolute_error: 21.455757, mean_q: 43.346203
  5354/100000: episode: 117, duration: 0.285s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.014 [-0.807, 0.826], loss: 3.659307, mean_absolute_error: 22.819765, mean_q: 46.167461
  5554/100000: episode: 118, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.064, 0.852], loss: 5.600843, mean_absolute_error: 24.120152, mean_q: 48.719116
  5754/100000: episode: 119, duration: 0.276s, episode steps: 200, steps per second: 725, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.088 [-1.191, 0.895], loss: 5.384245, mean_absolute_error: 25.387434, mean_q: 51.243866
  5954/100000: episode: 120, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.148 [-0.884, 1.134], loss: 6.047591, mean_absolute_error: 26.457413, mean_q: 53.405205
  6154/100000: episode: 121, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.232 [-1.552, 0.830], loss: 7.774735, mean_absolute_error: 27.643379, mean_q: 55.756699
  6354/100000: episode: 122, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.147 [-1.033, 1.348], loss: 6.520839, mean_absolute_error: 28.775414, mean_q: 58.072166
  6554/100000: episode: 123, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.018 [-0.909, 0.900], loss: 7.534516, mean_absolute_error: 29.952417, mean_q: 60.359825
  6754/100000: episode: 124, duration: 0.289s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.046 [-0.839, 1.002], loss: 8.396311, mean_absolute_error: 31.046665, mean_q: 62.682636
  6954/100000: episode: 125, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.123 [-0.904, 1.025], loss: 10.266867, mean_absolute_error: 32.144608, mean_q: 64.655853
  7154/100000: episode: 126, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.108 [-0.793, 1.088], loss: 10.337581, mean_absolute_error: 33.166237, mean_q: 66.810341
  7354/100000: episode: 127, duration: 0.283s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.176 [-0.819, 1.327], loss: 9.948745, mean_absolute_error: 34.173935, mean_q: 68.882881
  7554/100000: episode: 128, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.143 [-0.935, 0.860], loss: 10.639522, mean_absolute_error: 35.179493, mean_q: 70.888313
  7754/100000: episode: 129, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.063 [-0.849, 0.808], loss: 11.284298, mean_absolute_error: 36.224522, mean_q: 72.948090
  7954/100000: episode: 130, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.184 [-1.228, 0.745], loss: 10.975478, mean_absolute_error: 37.160477, mean_q: 74.786224
  8154/100000: episode: 131, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.219 [-1.429, 0.825], loss: 12.652980, mean_absolute_error: 37.958885, mean_q: 76.465660
  8354/100000: episode: 132, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.145 [-1.091, 0.567], loss: 12.564557, mean_absolute_error: 38.873638, mean_q: 78.304420
  8554/100000: episode: 133, duration: 0.277s, episode steps: 200, steps per second: 722, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.167 [-0.796, 1.315], loss: 11.908212, mean_absolute_error: 39.912037, mean_q: 80.412514
  8754/100000: episode: 134, duration: 0.284s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.107 [-0.829, 0.650], loss: 11.883781, mean_absolute_error: 40.885548, mean_q: 82.396797
  8954/100000: episode: 135, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.036 [-0.917, 0.947], loss: 11.658386, mean_absolute_error: 41.646339, mean_q: 84.032295
  9154/100000: episode: 136, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.119 [-0.962, 0.928], loss: 15.809595, mean_absolute_error: 42.556152, mean_q: 85.772224
  9354/100000: episode: 137, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.016 [-0.899, 0.967], loss: 15.557195, mean_absolute_error: 43.561417, mean_q: 87.439880
  9554/100000: episode: 138, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.195 [-1.343, 0.798], loss: 13.402996, mean_absolute_error: 44.215439, mean_q: 89.121994
  9754/100000: episode: 139, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.220 [-1.446, 0.552], loss: 11.395987, mean_absolute_error: 45.164696, mean_q: 90.897064
  9928/100000: episode: 140, duration: 0.248s, episode steps: 174, steps per second: 701, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.328 [-0.719, 2.110], loss: 16.028751, mean_absolute_error: 45.892582, mean_q: 92.485283
 10128/100000: episode: 141, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.135 [-0.824, 1.145], loss: 15.631412, mean_absolute_error: 46.340687, mean_q: 93.302147
 10328/100000: episode: 142, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.071 [-0.936, 0.773], loss: 18.089048, mean_absolute_error: 47.355923, mean_q: 95.155899
 10528/100000: episode: 143, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.050 [-0.805, 0.905], loss: 17.428223, mean_absolute_error: 47.856094, mean_q: 96.424355
 10728/100000: episode: 144, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.110 [-0.928, 0.749], loss: 20.898323, mean_absolute_error: 48.781494, mean_q: 98.103424
 10901/100000: episode: 145, duration: 0.239s, episode steps: 173, steps per second: 724, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.304 [-0.691, 1.998], loss: 17.884266, mean_absolute_error: 49.301945, mean_q: 99.367981
 11101/100000: episode: 146, duration: 0.276s, episode steps: 200, steps per second: 725, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.265 [-0.682, 1.985], loss: 19.854422, mean_absolute_error: 50.133385, mean_q: 100.751328
 11301/100000: episode: 147, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.038 [-0.710, 0.844], loss: 17.673874, mean_absolute_error: 50.880692, mean_q: 102.282249
 11501/100000: episode: 148, duration: 0.284s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.044 [-0.911, 0.776], loss: 18.736523, mean_absolute_error: 51.600029, mean_q: 103.770287
 11701/100000: episode: 149, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.045 [-0.657, 0.568], loss: 19.856020, mean_absolute_error: 52.036877, mean_q: 104.870506
 11901/100000: episode: 150, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.229 [-0.814, 1.841], loss: 14.508720, mean_absolute_error: 52.923805, mean_q: 106.677635
 12075/100000: episode: 151, duration: 0.249s, episode steps: 174, steps per second: 699, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.348 [-0.497, 2.274], loss: 14.253609, mean_absolute_error: 53.623096, mean_q: 108.063599
 12225/100000: episode: 152, duration: 0.212s, episode steps: 150, steps per second: 707, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.385 [-0.658, 2.217], loss: 18.246168, mean_absolute_error: 54.094173, mean_q: 108.779991
 12425/100000: episode: 153, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.075 [-0.735, 0.777], loss: 17.528028, mean_absolute_error: 54.671608, mean_q: 109.836746
 12625/100000: episode: 154, duration: 0.276s, episode steps: 200, steps per second: 725, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.058 [-0.583, 0.677], loss: 18.395281, mean_absolute_error: 55.363846, mean_q: 111.294159
 12825/100000: episode: 155, duration: 0.277s, episode steps: 200, steps per second: 722, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.027 [-0.677, 0.766], loss: 19.566397, mean_absolute_error: 55.861069, mean_q: 112.480843
 13009/100000: episode: 156, duration: 0.252s, episode steps: 184, steps per second: 730, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.327 [-0.785, 2.228], loss: 20.246384, mean_absolute_error: 56.842690, mean_q: 114.247902
 13209/100000: episode: 157, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-0.831, 0.604], loss: 25.613882, mean_absolute_error: 57.288521, mean_q: 115.172340
 13409/100000: episode: 158, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.104 [-0.730, 0.664], loss: 20.642052, mean_absolute_error: 57.849709, mean_q: 116.349586
 13600/100000: episode: 159, duration: 0.265s, episode steps: 191, steps per second: 720, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.309 [-0.545, 2.365], loss: 24.484524, mean_absolute_error: 58.114479, mean_q: 116.917778
 13800/100000: episode: 160, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.012 [-0.877, 0.761], loss: 31.265688, mean_absolute_error: 58.743057, mean_q: 117.802109
 14000/100000: episode: 161, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.132 [-0.801, 1.092], loss: 21.450991, mean_absolute_error: 59.024811, mean_q: 118.795021
 14174/100000: episode: 162, duration: 0.247s, episode steps: 174, steps per second: 706, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.339 [-0.502, 2.219], loss: 18.839113, mean_absolute_error: 59.727077, mean_q: 120.215225
 14374/100000: episode: 163, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.071 [-0.640, 0.785], loss: 22.333015, mean_absolute_error: 60.087013, mean_q: 120.762421
 14574/100000: episode: 164, duration: 0.284s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.057 [-0.611, 0.569], loss: 28.734051, mean_absolute_error: 60.794155, mean_q: 122.060402
 14774/100000: episode: 165, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.093 [-0.808, 0.964], loss: 23.290552, mean_absolute_error: 61.030376, mean_q: 122.577461
 14974/100000: episode: 166, duration: 0.291s, episode steps: 200, steps per second: 688, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.075 [-0.613, 0.805], loss: 22.705772, mean_absolute_error: 61.424618, mean_q: 123.371727
 15174/100000: episode: 167, duration: 0.283s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.198 [-1.352, 0.517], loss: 19.324762, mean_absolute_error: 61.770546, mean_q: 124.192986
 15374/100000: episode: 168, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.125 [-0.850, 0.585], loss: 21.584335, mean_absolute_error: 62.232246, mean_q: 125.115837
 15544/100000: episode: 169, duration: 0.238s, episode steps: 170, steps per second: 714, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.340 [-0.552, 2.193], loss: 16.356077, mean_absolute_error: 62.662315, mean_q: 126.175484
 15744/100000: episode: 170, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.108 [-0.736, 0.530], loss: 30.009871, mean_absolute_error: 63.145123, mean_q: 126.681816
 15944/100000: episode: 171, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.053 [-0.600, 0.596], loss: 18.294222, mean_absolute_error: 63.251339, mean_q: 127.258820
 16144/100000: episode: 172, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.201 [-1.519, 0.578], loss: 27.222902, mean_absolute_error: 63.988838, mean_q: 128.542572
 16325/100000: episode: 173, duration: 0.254s, episode steps: 181, steps per second: 714, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.282 [-0.603, 2.027], loss: 32.632275, mean_absolute_error: 64.343262, mean_q: 129.166580
 16512/100000: episode: 174, duration: 0.259s, episode steps: 187, steps per second: 723, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.334 [-0.618, 2.349], loss: 20.879045, mean_absolute_error: 64.378517, mean_q: 129.588593
 16712/100000: episode: 175, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.111 [-0.508, 1.252], loss: 30.070532, mean_absolute_error: 64.671829, mean_q: 130.045151
 16912/100000: episode: 176, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.093 [-0.520, 1.081], loss: 24.464670, mean_absolute_error: 64.879730, mean_q: 130.358383
 17112/100000: episode: 177, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.234 [-0.590, 1.807], loss: 25.650616, mean_absolute_error: 65.354469, mean_q: 131.409973
 17297/100000: episode: 178, duration: 0.256s, episode steps: 185, steps per second: 722, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.332 [-0.517, 2.362], loss: 22.649378, mean_absolute_error: 65.686691, mean_q: 132.125610
 17497/100000: episode: 179, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.159 [-1.089, 0.617], loss: 34.956173, mean_absolute_error: 65.915077, mean_q: 132.296875
 17697/100000: episode: 180, duration: 0.289s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.248 [-0.798, 1.997], loss: 23.629118, mean_absolute_error: 66.344788, mean_q: 133.527817
 17897/100000: episode: 181, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.211 [-1.495, 0.538], loss: 21.862665, mean_absolute_error: 66.444359, mean_q: 133.789352
 18097/100000: episode: 182, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.132 [-0.913, 0.503], loss: 30.468740, mean_absolute_error: 66.760300, mean_q: 134.369141
 18297/100000: episode: 183, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.109 [-0.741, 0.567], loss: 28.319752, mean_absolute_error: 67.046425, mean_q: 134.854294
 18497/100000: episode: 184, duration: 0.276s, episode steps: 200, steps per second: 724, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.195 [-1.348, 0.532], loss: 22.939878, mean_absolute_error: 67.225769, mean_q: 135.320847
 18697/100000: episode: 185, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-0.822, 0.798], loss: 28.884699, mean_absolute_error: 67.299210, mean_q: 135.437180
 18897/100000: episode: 186, duration: 0.277s, episode steps: 200, steps per second: 723, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.140 [-0.596, 1.153], loss: 26.032259, mean_absolute_error: 67.423553, mean_q: 135.723297
 19097/100000: episode: 187, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.102 [-0.697, 0.530], loss: 23.332510, mean_absolute_error: 67.559296, mean_q: 136.128159
 19297/100000: episode: 188, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.194 [-1.516, 0.766], loss: 16.796108, mean_absolute_error: 67.916443, mean_q: 136.815872
 19495/100000: episode: 189, duration: 0.274s, episode steps: 198, steps per second: 724, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.327 [-0.655, 2.444], loss: 24.465826, mean_absolute_error: 68.273430, mean_q: 137.450500
 19677/100000: episode: 190, duration: 0.252s, episode steps: 182, steps per second: 724, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.358 [-0.576, 2.533], loss: 17.560965, mean_absolute_error: 68.690422, mean_q: 138.350876
 19857/100000: episode: 191, duration: 0.250s, episode steps: 180, steps per second: 720, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.320 [-0.514, 2.236], loss: 23.858339, mean_absolute_error: 68.563469, mean_q: 138.025299
 20040/100000: episode: 192, duration: 0.258s, episode steps: 183, steps per second: 709, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.346 [-0.538, 2.408], loss: 21.632191, mean_absolute_error: 68.989021, mean_q: 138.894745
 20240/100000: episode: 193, duration: 0.283s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.260 [-0.640, 2.008], loss: 18.910706, mean_absolute_error: 69.318596, mean_q: 139.612869
 20440/100000: episode: 194, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.268 [-0.567, 2.036], loss: 15.897300, mean_absolute_error: 69.603745, mean_q: 140.253632
 20590/100000: episode: 195, duration: 0.209s, episode steps: 150, steps per second: 719, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.368 [-0.568, 2.225], loss: 20.980049, mean_absolute_error: 69.652527, mean_q: 140.357025
 20790/100000: episode: 196, duration: 0.277s, episode steps: 200, steps per second: 723, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.279 [-0.494, 2.103], loss: 23.216196, mean_absolute_error: 70.083084, mean_q: 141.134048
 20990/100000: episode: 197, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.046 [-0.523, 0.622], loss: 18.894985, mean_absolute_error: 70.169014, mean_q: 141.302658
 21190/100000: episode: 198, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.220 [-1.487, 0.550], loss: 21.849749, mean_absolute_error: 70.539848, mean_q: 141.861969
 21390/100000: episode: 199, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.146 [-0.813, 1.295], loss: 23.201801, mean_absolute_error: 70.440994, mean_q: 141.708374
 21590/100000: episode: 200, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.185 [-1.247, 0.528], loss: 25.505728, mean_absolute_error: 70.483597, mean_q: 141.931992
 21790/100000: episode: 201, duration: 0.276s, episode steps: 200, steps per second: 724, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.127 [-0.860, 0.558], loss: 22.293642, mean_absolute_error: 70.612106, mean_q: 142.236801
 21990/100000: episode: 202, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.099 [-0.792, 0.589], loss: 28.592863, mean_absolute_error: 70.536514, mean_q: 141.946091
 22190/100000: episode: 203, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-0.565, 0.705], loss: 22.227276, mean_absolute_error: 70.687958, mean_q: 142.339737
 22390/100000: episode: 204, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.129 [-0.897, 0.484], loss: 25.268799, mean_absolute_error: 71.251144, mean_q: 143.409866
 22590/100000: episode: 205, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.068 [-0.620, 0.555], loss: 27.147175, mean_absolute_error: 71.157089, mean_q: 143.346069
 22790/100000: episode: 206, duration: 0.284s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.099 [-0.668, 0.522], loss: 18.286507, mean_absolute_error: 71.395119, mean_q: 143.819275
 22990/100000: episode: 207, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.153 [-1.027, 0.519], loss: 24.511391, mean_absolute_error: 71.539131, mean_q: 144.025757
 23190/100000: episode: 208, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.222 [-0.504, 1.841], loss: 26.141113, mean_absolute_error: 71.241890, mean_q: 143.555649
 23390/100000: episode: 209, duration: 0.276s, episode steps: 200, steps per second: 725, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.090 [-0.620, 0.564], loss: 27.586487, mean_absolute_error: 71.636368, mean_q: 144.101898
 23577/100000: episode: 210, duration: 0.260s, episode steps: 187, steps per second: 719, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.344 [-0.586, 2.528], loss: 19.497072, mean_absolute_error: 71.615715, mean_q: 144.283051
 23762/100000: episode: 211, duration: 0.257s, episode steps: 185, steps per second: 719, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.348 [-0.553, 2.432], loss: 29.443813, mean_absolute_error: 71.809479, mean_q: 144.340454
 23962/100000: episode: 212, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.252 [-0.537, 2.027], loss: 21.579292, mean_absolute_error: 71.289162, mean_q: 143.684692
 24136/100000: episode: 213, duration: 0.245s, episode steps: 174, steps per second: 709, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.330 [-0.664, 2.189], loss: 21.399168, mean_absolute_error: 71.846825, mean_q: 144.582565
 24336/100000: episode: 214, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.097 [-0.697, 0.504], loss: 21.527472, mean_absolute_error: 71.526527, mean_q: 143.847748
 24536/100000: episode: 215, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.226 [-1.534, 0.618], loss: 18.720528, mean_absolute_error: 71.977409, mean_q: 144.659241
 24736/100000: episode: 216, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.205 [-1.516, 0.497], loss: 22.001764, mean_absolute_error: 71.525551, mean_q: 143.935226
 24936/100000: episode: 217, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.111 [-0.762, 0.552], loss: 28.392174, mean_absolute_error: 71.390236, mean_q: 143.689621
 25136/100000: episode: 218, duration: 0.277s, episode steps: 200, steps per second: 723, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.220 [-1.653, 0.513], loss: 24.459288, mean_absolute_error: 71.704475, mean_q: 144.364258
 25330/100000: episode: 219, duration: 0.274s, episode steps: 194, steps per second: 709, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.334 [-0.531, 2.574], loss: 19.846737, mean_absolute_error: 71.250412, mean_q: 143.572449
 25530/100000: episode: 220, duration: 0.283s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.107 [-0.756, 0.578], loss: 32.028564, mean_absolute_error: 71.435989, mean_q: 143.655945
 25730/100000: episode: 221, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.050 [-0.544, 0.540], loss: 23.723570, mean_absolute_error: 71.033386, mean_q: 143.037857
 25930/100000: episode: 222, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.128 [-0.873, 0.553], loss: 22.338112, mean_absolute_error: 71.819244, mean_q: 144.609818
 26130/100000: episode: 223, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.172 [-1.119, 0.448], loss: 26.622726, mean_absolute_error: 71.354889, mean_q: 143.675247
 26309/100000: episode: 224, duration: 0.252s, episode steps: 179, steps per second: 710, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.359 [-0.481, 2.443], loss: 20.215174, mean_absolute_error: 70.981056, mean_q: 143.118866
 26509/100000: episode: 225, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.102 [-0.664, 0.494], loss: 19.313429, mean_absolute_error: 71.643158, mean_q: 144.248093
 26674/100000: episode: 226, duration: 0.234s, episode steps: 165, steps per second: 704, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.371 [-0.643, 2.383], loss: 30.331024, mean_absolute_error: 71.258064, mean_q: 143.329742
 26874/100000: episode: 227, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.193 [-0.584, 1.802], loss: 21.979433, mean_absolute_error: 71.209381, mean_q: 143.418732
 27074/100000: episode: 228, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.190 [-1.301, 0.500], loss: 16.120937, mean_absolute_error: 71.331032, mean_q: 143.802261
 27274/100000: episode: 229, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.180 [-1.148, 0.522], loss: 20.488682, mean_absolute_error: 71.794792, mean_q: 144.532715
 27433/100000: episode: 230, duration: 0.222s, episode steps: 159, steps per second: 715, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.393 [-0.652, 2.402], loss: 18.779591, mean_absolute_error: 72.052391, mean_q: 145.017960
 27633/100000: episode: 231, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.174 [-1.141, 0.514], loss: 24.029963, mean_absolute_error: 71.643936, mean_q: 144.065231
 27833/100000: episode: 232, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.256 [-0.797, 1.962], loss: 20.713877, mean_absolute_error: 71.632454, mean_q: 144.199417
 28000/100000: episode: 233, duration: 0.233s, episode steps: 167, steps per second: 716, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.373 [-0.541, 2.405], loss: 21.970665, mean_absolute_error: 71.525177, mean_q: 144.114426
 28200/100000: episode: 234, duration: 0.290s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.095 [-0.584, 1.105], loss: 23.722206, mean_absolute_error: 71.334908, mean_q: 143.553711
 28400/100000: episode: 235, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.039 [-0.539, 0.515], loss: 17.841368, mean_absolute_error: 71.416168, mean_q: 143.929977
 28566/100000: episode: 236, duration: 0.234s, episode steps: 166, steps per second: 708, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.365 [-0.480, 2.321], loss: 22.798195, mean_absolute_error: 71.663811, mean_q: 144.256683
 28766/100000: episode: 237, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-0.515, 0.496], loss: 29.496687, mean_absolute_error: 71.716751, mean_q: 144.255035
 28966/100000: episode: 238, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.063 [-0.617, 0.585], loss: 22.627563, mean_absolute_error: 72.080002, mean_q: 145.165054
 29152/100000: episode: 239, duration: 0.260s, episode steps: 186, steps per second: 714, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.342 [-0.520, 2.400], loss: 25.304646, mean_absolute_error: 71.391563, mean_q: 143.811096
 29352/100000: episode: 240, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.024 [-0.593, 0.594], loss: 26.538965, mean_absolute_error: 71.591019, mean_q: 144.113968
 29552/100000: episode: 241, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-0.543, 0.524], loss: 21.050650, mean_absolute_error: 71.961769, mean_q: 144.923401
 29752/100000: episode: 242, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.110 [-0.738, 0.527], loss: 23.077402, mean_absolute_error: 72.304741, mean_q: 145.377151
 29952/100000: episode: 243, duration: 0.278s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.131 [-0.858, 0.567], loss: 29.586390, mean_absolute_error: 71.891541, mean_q: 144.618271
 30152/100000: episode: 244, duration: 0.284s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.071 [-0.598, 0.464], loss: 17.714336, mean_absolute_error: 71.543739, mean_q: 144.013916
 30352/100000: episode: 245, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.196 [-1.458, 0.531], loss: 22.156872, mean_absolute_error: 71.924637, mean_q: 144.495865
 30552/100000: episode: 246, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.106 [-0.735, 0.542], loss: 28.955849, mean_absolute_error: 71.710632, mean_q: 144.165497
 30752/100000: episode: 247, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.180 [-1.149, 0.587], loss: 25.606464, mean_absolute_error: 71.245293, mean_q: 143.239624
 30942/100000: episode: 248, duration: 0.270s, episode steps: 190, steps per second: 703, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.307 [-0.557, 2.237], loss: 18.356369, mean_absolute_error: 71.093582, mean_q: 143.177872
 31142/100000: episode: 249, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.127 [-0.880, 0.622], loss: 21.829851, mean_absolute_error: 70.859398, mean_q: 142.390671
 31342/100000: episode: 250, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.117 [-0.751, 0.566], loss: 22.596821, mean_absolute_error: 71.386559, mean_q: 143.557083
 31542/100000: episode: 251, duration: 0.285s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.128 [-0.845, 0.640], loss: 16.013269, mean_absolute_error: 71.068604, mean_q: 143.044266
 31742/100000: episode: 252, duration: 0.287s, episode steps: 200, steps per second: 698, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.079 [-0.592, 0.496], loss: 22.838064, mean_absolute_error: 71.252625, mean_q: 143.044495
 31942/100000: episode: 253, duration: 0.284s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.144 [-0.934, 0.521], loss: 15.016065, mean_absolute_error: 70.881569, mean_q: 142.742584
 32125/100000: episode: 254, duration: 0.259s, episode steps: 183, steps per second: 706, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.344 [-0.555, 2.375], loss: 24.126961, mean_absolute_error: 70.855316, mean_q: 142.266479
 32325/100000: episode: 255, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.263 [-2.257, 0.622], loss: 28.994942, mean_absolute_error: 70.549095, mean_q: 141.680038
 32493/100000: episode: 256, duration: 0.232s, episode steps: 168, steps per second: 725, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.372 [-0.458, 2.400], loss: 23.521112, mean_absolute_error: 69.765198, mean_q: 140.314575
 32686/100000: episode: 257, duration: 0.268s, episode steps: 193, steps per second: 719, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.327 [-0.542, 2.388], loss: 25.732422, mean_absolute_error: 70.371056, mean_q: 141.221405
 32886/100000: episode: 258, duration: 0.275s, episode steps: 200, steps per second: 728, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.208 [-1.680, 0.681], loss: 23.938608, mean_absolute_error: 69.769630, mean_q: 139.986252
 33080/100000: episode: 259, duration: 0.273s, episode steps: 194, steps per second: 711, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.332 [-0.547, 2.419], loss: 22.027620, mean_absolute_error: 69.979713, mean_q: 140.647385
 33280/100000: episode: 260, duration: 0.280s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.119 [-0.765, 0.521], loss: 24.622919, mean_absolute_error: 69.825310, mean_q: 140.018951
 33480/100000: episode: 261, duration: 0.277s, episode steps: 200, steps per second: 722, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.083 [-0.568, 0.593], loss: 14.559392, mean_absolute_error: 70.002518, mean_q: 140.828873
 33646/100000: episode: 262, duration: 0.231s, episode steps: 166, steps per second: 718, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.379 [-0.511, 2.580], loss: 24.592009, mean_absolute_error: 70.021652, mean_q: 140.628616
 33803/100000: episode: 263, duration: 0.219s, episode steps: 157, steps per second: 718, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.396 [-0.490, 2.421], loss: 18.336397, mean_absolute_error: 69.406876, mean_q: 139.538269
 34003/100000: episode: 264, duration: 0.284s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.133 [-0.835, 0.513], loss: 21.098408, mean_absolute_error: 69.946426, mean_q: 140.387115
 34203/100000: episode: 265, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.116 [-0.748, 0.563], loss: 25.890554, mean_absolute_error: 69.910484, mean_q: 140.370789
 34381/100000: episode: 266, duration: 0.256s, episode steps: 178, steps per second: 696, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.352 [-0.574, 2.363], loss: 25.350943, mean_absolute_error: 69.479828, mean_q: 139.433487
 34581/100000: episode: 267, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.090 [-0.597, 0.545], loss: 23.496775, mean_absolute_error: 69.084259, mean_q: 138.668335
 34781/100000: episode: 268, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.519, 0.537], loss: 20.729536, mean_absolute_error: 69.150711, mean_q: 138.950729
 34981/100000: episode: 269, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.304 [-0.417, 2.286], loss: 22.843723, mean_absolute_error: 69.275978, mean_q: 139.177307
 35159/100000: episode: 270, duration: 0.250s, episode steps: 178, steps per second: 711, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.359 [-0.743, 2.554], loss: 16.359030, mean_absolute_error: 69.496315, mean_q: 139.512634
 35359/100000: episode: 271, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.074 [-0.583, 0.711], loss: 21.671150, mean_absolute_error: 69.726242, mean_q: 139.971207
 35559/100000: episode: 272, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.048 [-0.547, 0.556], loss: 24.574497, mean_absolute_error: 69.167336, mean_q: 138.963913
 35759/100000: episode: 273, duration: 0.291s, episode steps: 200, steps per second: 688, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-0.724, 0.515], loss: 26.267540, mean_absolute_error: 68.734138, mean_q: 138.094406
 35959/100000: episode: 274, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.091 [-0.554, 1.083], loss: 24.833155, mean_absolute_error: 68.886230, mean_q: 138.241562
 36159/100000: episode: 275, duration: 0.290s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.074 [-0.610, 0.540], loss: 15.153560, mean_absolute_error: 68.778442, mean_q: 138.440720
 36359/100000: episode: 276, duration: 0.283s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.156 [-0.989, 0.595], loss: 19.812458, mean_absolute_error: 68.813835, mean_q: 138.447220
 36559/100000: episode: 277, duration: 0.280s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-0.593, 0.551], loss: 16.405703, mean_absolute_error: 68.826080, mean_q: 138.448944
 36759/100000: episode: 278, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-0.467, 0.493], loss: 22.754166, mean_absolute_error: 68.642067, mean_q: 137.995865
 36959/100000: episode: 279, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.072 [-0.545, 0.631], loss: 25.215063, mean_absolute_error: 68.998772, mean_q: 138.490158
 37123/100000: episode: 280, duration: 0.234s, episode steps: 164, steps per second: 701, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.389 [-0.601, 2.414], loss: 26.358410, mean_absolute_error: 68.606087, mean_q: 137.815781
 37323/100000: episode: 281, duration: 0.285s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.065 [-0.554, 0.617], loss: 19.656713, mean_absolute_error: 68.537064, mean_q: 137.725098
 37523/100000: episode: 282, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.099 [-0.650, 0.550], loss: 19.738262, mean_absolute_error: 68.669403, mean_q: 138.007812
 37723/100000: episode: 283, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.127 [-0.806, 0.474], loss: 21.140493, mean_absolute_error: 68.044296, mean_q: 136.698578
 37923/100000: episode: 284, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.096 [-0.678, 0.600], loss: 18.016098, mean_absolute_error: 67.943855, mean_q: 136.490295
 38123/100000: episode: 285, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.035 [-0.620, 0.585], loss: 13.014848, mean_absolute_error: 67.593849, mean_q: 135.983414
 38323/100000: episode: 286, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.116 [-0.723, 0.531], loss: 23.228722, mean_absolute_error: 67.901291, mean_q: 136.299118
 38523/100000: episode: 287, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.100 [-0.632, 0.647], loss: 18.776644, mean_absolute_error: 67.732239, mean_q: 136.134598
 38723/100000: episode: 288, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.178 [-0.607, 1.703], loss: 14.786721, mean_absolute_error: 67.320107, mean_q: 135.466888
 38923/100000: episode: 289, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-0.529, 0.504], loss: 19.712828, mean_absolute_error: 67.301857, mean_q: 135.407867
 39123/100000: episode: 290, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.580, 0.481], loss: 19.863951, mean_absolute_error: 67.014832, mean_q: 134.684479
 39323/100000: episode: 291, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.073 [-0.462, 0.972], loss: 18.526646, mean_absolute_error: 66.984299, mean_q: 134.408127
 39523/100000: episode: 292, duration: 0.312s, episode steps: 200, steps per second: 642, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.258 [-0.802, 2.217], loss: 19.857035, mean_absolute_error: 67.543900, mean_q: 135.299805
 39723/100000: episode: 293, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.222 [-0.577, 1.879], loss: 18.435219, mean_absolute_error: 66.621429, mean_q: 133.746216
 39923/100000: episode: 294, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.052 [-0.584, 0.856], loss: 17.555286, mean_absolute_error: 67.012199, mean_q: 134.444199
 40123/100000: episode: 295, duration: 0.280s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.249 [-0.774, 2.044], loss: 19.409784, mean_absolute_error: 66.700760, mean_q: 133.804062
 40323/100000: episode: 296, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-0.618, 0.540], loss: 20.525146, mean_absolute_error: 66.990997, mean_q: 134.433426
 40523/100000: episode: 297, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.088 [-0.632, 0.515], loss: 20.406422, mean_absolute_error: 65.847557, mean_q: 132.004517
 40705/100000: episode: 298, duration: 0.262s, episode steps: 182, steps per second: 694, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.317 [-0.478, 2.171], loss: 24.135735, mean_absolute_error: 65.774239, mean_q: 131.927963
 40905/100000: episode: 299, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.209 [-0.686, 1.686], loss: 20.678877, mean_absolute_error: 65.918633, mean_q: 132.133728
 41105/100000: episode: 300, duration: 0.285s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.027 [-0.541, 0.625], loss: 20.297401, mean_absolute_error: 65.716339, mean_q: 131.815552
 41305/100000: episode: 301, duration: 0.283s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.074 [-0.660, 0.562], loss: 18.292330, mean_absolute_error: 65.616570, mean_q: 131.757019
 41505/100000: episode: 302, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.055 [-0.587, 0.538], loss: 19.492769, mean_absolute_error: 65.489220, mean_q: 131.305679
 41705/100000: episode: 303, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.033 [-0.584, 0.588], loss: 19.047216, mean_absolute_error: 65.276588, mean_q: 130.892654
 41877/100000: episode: 304, duration: 0.241s, episode steps: 172, steps per second: 714, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.362 [-0.565, 2.328], loss: 19.256336, mean_absolute_error: 65.055542, mean_q: 130.474655
 42077/100000: episode: 305, duration: 0.280s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.039 [-0.630, 0.540], loss: 15.593697, mean_absolute_error: 64.859589, mean_q: 130.166382
 42250/100000: episode: 306, duration: 0.240s, episode steps: 173, steps per second: 722, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.365 [-0.447, 2.562], loss: 23.166361, mean_absolute_error: 65.047630, mean_q: 130.327240
 42450/100000: episode: 307, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.057 [-0.710, 0.742], loss: 11.460177, mean_absolute_error: 64.749870, mean_q: 129.969086
 42650/100000: episode: 308, duration: 0.276s, episode steps: 200, steps per second: 725, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.091 [-0.539, 1.107], loss: 18.888315, mean_absolute_error: 64.794861, mean_q: 129.907501
 42850/100000: episode: 309, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-0.580, 0.595], loss: 22.707325, mean_absolute_error: 64.519005, mean_q: 129.374451
 43042/100000: episode: 310, duration: 0.268s, episode steps: 192, steps per second: 716, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.336 [-0.694, 2.437], loss: 16.829735, mean_absolute_error: 64.372322, mean_q: 129.297928
 43242/100000: episode: 311, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.101 [-0.633, 0.563], loss: 20.036165, mean_absolute_error: 64.316330, mean_q: 128.977051
 43442/100000: episode: 312, duration: 0.286s, episode steps: 200, steps per second: 698, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.055 [-0.594, 0.732], loss: 16.341743, mean_absolute_error: 64.302025, mean_q: 128.890335
 43642/100000: episode: 313, duration: 0.284s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.131 [-0.712, 1.659], loss: 20.450487, mean_absolute_error: 64.052002, mean_q: 128.429382
 43821/100000: episode: 314, duration: 0.253s, episode steps: 179, steps per second: 708, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.315 [-0.550, 2.109], loss: 13.719331, mean_absolute_error: 63.841228, mean_q: 128.031723
 44021/100000: episode: 315, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.129 [-0.799, 0.585], loss: 23.685013, mean_absolute_error: 63.626465, mean_q: 127.306404
 44215/100000: episode: 316, duration: 0.269s, episode steps: 194, steps per second: 721, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.330 [-0.516, 2.446], loss: 13.976684, mean_absolute_error: 63.386265, mean_q: 127.122581
 44365/100000: episode: 317, duration: 0.212s, episode steps: 150, steps per second: 708, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.389 [-0.513, 2.247], loss: 13.789915, mean_absolute_error: 63.302624, mean_q: 126.938545
 44565/100000: episode: 318, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.035 [-0.607, 0.616], loss: 13.281258, mean_absolute_error: 63.818970, mean_q: 127.976097
 44742/100000: episode: 319, duration: 0.249s, episode steps: 177, steps per second: 711, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.350 [-0.792, 2.336], loss: 16.857958, mean_absolute_error: 63.235954, mean_q: 126.777367
 44942/100000: episode: 320, duration: 0.289s, episode steps: 200, steps per second: 692, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.275 [-0.665, 2.047], loss: 14.665489, mean_absolute_error: 63.503017, mean_q: 127.294144
 45095/100000: episode: 321, duration: 0.217s, episode steps: 153, steps per second: 705, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.367 [-0.584, 2.121], loss: 14.375545, mean_absolute_error: 63.214931, mean_q: 126.800346
 45295/100000: episode: 322, duration: 0.295s, episode steps: 200, steps per second: 679, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.138 [-0.873, 0.806], loss: 20.753942, mean_absolute_error: 62.756294, mean_q: 125.674461
 45495/100000: episode: 323, duration: 0.278s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.052 [-0.600, 0.622], loss: 17.878695, mean_absolute_error: 62.660011, mean_q: 125.534988
 45695/100000: episode: 324, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.089 [-0.788, 0.688], loss: 17.792543, mean_absolute_error: 62.480869, mean_q: 125.211540
 45895/100000: episode: 325, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.177 [-1.170, 0.624], loss: 19.740707, mean_absolute_error: 62.336964, mean_q: 124.801857
 46095/100000: episode: 326, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.051 [-0.605, 0.605], loss: 16.679163, mean_absolute_error: 62.076385, mean_q: 124.367577
 46295/100000: episode: 327, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-0.628, 0.546], loss: 13.731253, mean_absolute_error: 61.865948, mean_q: 124.071777
 46495/100000: episode: 328, duration: 0.289s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.136 [-0.496, 1.428], loss: 22.828537, mean_absolute_error: 61.714577, mean_q: 123.448204
 46695/100000: episode: 329, duration: 0.280s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.091 [-0.650, 0.569], loss: 13.137304, mean_absolute_error: 61.810314, mean_q: 123.865196
 46889/100000: episode: 330, duration: 0.268s, episode steps: 194, steps per second: 724, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.280 [-0.632, 2.039], loss: 17.093119, mean_absolute_error: 61.196537, mean_q: 122.629852
 47089/100000: episode: 331, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.595, 0.570], loss: 18.936987, mean_absolute_error: 61.537220, mean_q: 123.203964
 47289/100000: episode: 332, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.029 [-0.746, 0.596], loss: 12.918147, mean_absolute_error: 61.228672, mean_q: 122.759254
 47489/100000: episode: 333, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.221 [-0.832, 2.208], loss: 16.350470, mean_absolute_error: 61.027626, mean_q: 122.168045
 47689/100000: episode: 334, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.076 [-0.655, 0.764], loss: 13.814662, mean_absolute_error: 61.220703, mean_q: 122.683289
 47889/100000: episode: 335, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.655, 0.569], loss: 13.413831, mean_absolute_error: 61.017590, mean_q: 122.170441
 48089/100000: episode: 336, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-0.590, 0.554], loss: 14.520537, mean_absolute_error: 60.878994, mean_q: 121.912483
 48289/100000: episode: 337, duration: 0.284s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.053 [-0.709, 0.567], loss: 9.526532, mean_absolute_error: 60.990578, mean_q: 122.235748
 48489/100000: episode: 338, duration: 0.284s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.096 [-0.615, 0.603], loss: 12.958626, mean_absolute_error: 60.733517, mean_q: 121.544685
 48689/100000: episode: 339, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-0.619, 0.555], loss: 19.321167, mean_absolute_error: 60.111183, mean_q: 120.254219
 48889/100000: episode: 340, duration: 0.284s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.044 [-0.674, 0.705], loss: 16.896557, mean_absolute_error: 60.133926, mean_q: 120.306541
 49089/100000: episode: 341, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.129 [-0.832, 0.795], loss: 19.978174, mean_absolute_error: 60.164200, mean_q: 120.245407
 49289/100000: episode: 342, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.132 [-0.843, 0.588], loss: 16.253773, mean_absolute_error: 59.580700, mean_q: 119.127472
 49489/100000: episode: 343, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.040 [-0.584, 0.603], loss: 15.470864, mean_absolute_error: 59.182011, mean_q: 118.354256
 49689/100000: episode: 344, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.259 [-0.589, 2.197], loss: 9.564592, mean_absolute_error: 59.382267, mean_q: 118.903458
 49889/100000: episode: 345, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-0.696, 0.554], loss: 19.686260, mean_absolute_error: 59.675838, mean_q: 119.352013
 50089/100000: episode: 346, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.044 [-0.612, 0.548], loss: 12.842615, mean_absolute_error: 59.217075, mean_q: 118.510162
 50289/100000: episode: 347, duration: 0.275s, episode steps: 200, steps per second: 727, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.105 [-0.794, 0.612], loss: 17.635098, mean_absolute_error: 59.013294, mean_q: 117.980858
 50489/100000: episode: 348, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.180 [-0.829, 1.513], loss: 17.053255, mean_absolute_error: 58.836670, mean_q: 117.498581
 50689/100000: episode: 349, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.033 [-0.607, 0.612], loss: 14.328261, mean_absolute_error: 58.591965, mean_q: 117.360916
 50889/100000: episode: 350, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.040 [-0.594, 0.831], loss: 17.334553, mean_absolute_error: 58.411774, mean_q: 116.827087
 51089/100000: episode: 351, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.296 [-0.665, 2.242], loss: 12.001587, mean_absolute_error: 58.529343, mean_q: 117.174469
 51283/100000: episode: 352, duration: 0.281s, episode steps: 194, steps per second: 691, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.326 [-0.812, 2.380], loss: 15.140448, mean_absolute_error: 58.413769, mean_q: 116.872963
 51483/100000: episode: 353, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.125 [-0.793, 0.511], loss: 15.232508, mean_absolute_error: 58.414238, mean_q: 116.811798
 51683/100000: episode: 354, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.090 [-0.740, 0.785], loss: 17.828009, mean_absolute_error: 57.679123, mean_q: 115.345070
 51883/100000: episode: 355, duration: 0.280s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.015 [-0.605, 0.773], loss: 16.139641, mean_absolute_error: 57.522457, mean_q: 115.129349
 52083/100000: episode: 356, duration: 0.290s, episode steps: 200, steps per second: 690, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.082 [-0.589, 0.623], loss: 15.923532, mean_absolute_error: 57.608013, mean_q: 115.243240
 52283/100000: episode: 357, duration: 0.285s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-0.684, 0.664], loss: 14.679019, mean_absolute_error: 57.260330, mean_q: 114.738167
 52471/100000: episode: 358, duration: 0.261s, episode steps: 188, steps per second: 719, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.291 [-0.603, 2.068], loss: 16.557037, mean_absolute_error: 56.946926, mean_q: 113.918983
 52671/100000: episode: 359, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.007 [-0.650, 0.752], loss: 8.570347, mean_absolute_error: 56.906914, mean_q: 114.080292
 52871/100000: episode: 360, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-0.622, 0.791], loss: 18.068079, mean_absolute_error: 57.152714, mean_q: 114.383972
 53071/100000: episode: 361, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.032 [-0.564, 0.634], loss: 16.691957, mean_absolute_error: 56.814667, mean_q: 113.804176
 53271/100000: episode: 362, duration: 0.285s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.039 [-0.667, 0.657], loss: 15.007931, mean_absolute_error: 57.041473, mean_q: 114.247139
 53471/100000: episode: 363, duration: 0.289s, episode steps: 200, steps per second: 693, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.049 [-0.632, 0.835], loss: 16.181673, mean_absolute_error: 57.070889, mean_q: 114.080956
 53671/100000: episode: 364, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.027 [-0.615, 0.825], loss: 9.454629, mean_absolute_error: 56.529171, mean_q: 113.274345
 53871/100000: episode: 365, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.097 [-0.729, 0.558], loss: 15.393546, mean_absolute_error: 56.955200, mean_q: 114.103867
 54071/100000: episode: 366, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-0.611, 0.600], loss: 15.768681, mean_absolute_error: 56.582733, mean_q: 113.323456
 54271/100000: episode: 367, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.142 [-0.816, 1.457], loss: 20.636425, mean_absolute_error: 56.465366, mean_q: 113.041229
 54471/100000: episode: 368, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.046 [-0.624, 0.748], loss: 14.386711, mean_absolute_error: 56.443581, mean_q: 112.974091
 54671/100000: episode: 369, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-0.569, 0.640], loss: 12.202407, mean_absolute_error: 56.440628, mean_q: 113.141701
 54871/100000: episode: 370, duration: 0.293s, episode steps: 200, steps per second: 683, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-0.660, 0.772], loss: 15.182442, mean_absolute_error: 56.173603, mean_q: 112.505798
 55071/100000: episode: 371, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.118 [-0.709, 1.474], loss: 11.855415, mean_absolute_error: 56.465919, mean_q: 113.066444
 55271/100000: episode: 372, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.661, 0.552], loss: 11.534921, mean_absolute_error: 56.108932, mean_q: 112.322502
 55463/100000: episode: 373, duration: 0.278s, episode steps: 192, steps per second: 691, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.334 [-1.097, 2.429], loss: 14.643786, mean_absolute_error: 56.325573, mean_q: 112.755432
 55629/100000: episode: 374, duration: 0.240s, episode steps: 166, steps per second: 691, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.383 [-0.790, 2.419], loss: 13.983140, mean_absolute_error: 56.369423, mean_q: 112.802124
 55829/100000: episode: 375, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.053 [-0.693, 0.711], loss: 12.821829, mean_absolute_error: 56.157330, mean_q: 112.467773
 56029/100000: episode: 376, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.212 [-0.756, 1.645], loss: 12.022033, mean_absolute_error: 56.210766, mean_q: 112.640488
 56229/100000: episode: 377, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.072 [-0.677, 0.776], loss: 14.941355, mean_absolute_error: 56.151382, mean_q: 112.420700
 56429/100000: episode: 378, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.014 [-0.644, 0.772], loss: 13.207167, mean_absolute_error: 56.015026, mean_q: 112.188591
 56625/100000: episode: 379, duration: 0.279s, episode steps: 196, steps per second: 704, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.325 [-0.711, 2.407], loss: 13.755592, mean_absolute_error: 55.692810, mean_q: 111.616173
 56825/100000: episode: 380, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.077 [-0.705, 0.578], loss: 12.525393, mean_absolute_error: 56.000557, mean_q: 112.178406
 57008/100000: episode: 381, duration: 0.263s, episode steps: 183, steps per second: 697, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.342 [-0.953, 2.797], loss: 13.848412, mean_absolute_error: 55.881638, mean_q: 111.892227
 57208/100000: episode: 382, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.047 [-0.792, 0.774], loss: 16.951933, mean_absolute_error: 55.608746, mean_q: 111.258209
 57408/100000: episode: 383, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.024 [-0.770, 0.616], loss: 17.321165, mean_absolute_error: 55.693836, mean_q: 111.377296
 57608/100000: episode: 384, duration: 0.289s, episode steps: 200, steps per second: 692, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.038 [-0.688, 0.536], loss: 14.363125, mean_absolute_error: 55.747635, mean_q: 111.589241
 57808/100000: episode: 385, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.117 [-0.664, 1.146], loss: 11.469514, mean_absolute_error: 55.327709, mean_q: 110.778992
 58008/100000: episode: 386, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-0.866, 0.828], loss: 12.953924, mean_absolute_error: 55.108349, mean_q: 110.420204
 58208/100000: episode: 387, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-1.109, 0.829], loss: 12.833543, mean_absolute_error: 55.296112, mean_q: 110.793045
 58380/100000: episode: 388, duration: 0.244s, episode steps: 172, steps per second: 704, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.358 [-0.972, 2.740], loss: 13.772873, mean_absolute_error: 55.299873, mean_q: 110.546951
 58580/100000: episode: 389, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.068 [-0.687, 0.759], loss: 15.875916, mean_absolute_error: 55.239628, mean_q: 110.476540
 58780/100000: episode: 390, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.034 [-0.870, 0.753], loss: 7.969204, mean_absolute_error: 55.057335, mean_q: 110.420120
 58980/100000: episode: 391, duration: 0.277s, episode steps: 200, steps per second: 722, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.152 [-1.088, 1.697], loss: 9.767694, mean_absolute_error: 54.941376, mean_q: 110.136429
 59180/100000: episode: 392, duration: 0.283s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-0.649, 0.757], loss: 15.175119, mean_absolute_error: 55.029415, mean_q: 110.118767
 59380/100000: episode: 393, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.150 [-1.022, 0.683], loss: 18.363813, mean_absolute_error: 55.216362, mean_q: 110.422707
 59580/100000: episode: 394, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.059 [-0.820, 0.668], loss: 11.793967, mean_absolute_error: 54.977909, mean_q: 110.066956
 59780/100000: episode: 395, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.237 [-0.859, 2.065], loss: 19.523792, mean_absolute_error: 54.885853, mean_q: 109.774963
 59980/100000: episode: 396, duration: 0.275s, episode steps: 200, steps per second: 727, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-0.638, 0.594], loss: 13.316255, mean_absolute_error: 54.801159, mean_q: 109.586700
 60180/100000: episode: 397, duration: 0.278s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.033 [-0.627, 0.592], loss: 11.977382, mean_absolute_error: 54.604916, mean_q: 109.394196
 60380/100000: episode: 398, duration: 0.275s, episode steps: 200, steps per second: 726, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-0.900, 0.815], loss: 15.426085, mean_absolute_error: 55.072441, mean_q: 110.158913
 60580/100000: episode: 399, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.066 [-0.780, 0.749], loss: 13.996376, mean_absolute_error: 54.712685, mean_q: 109.618240
 60777/100000: episode: 400, duration: 0.281s, episode steps: 197, steps per second: 701, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.325 [-0.803, 2.405], loss: 9.407554, mean_absolute_error: 55.061905, mean_q: 110.404404
 60977/100000: episode: 401, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.658, 0.799], loss: 16.472012, mean_absolute_error: 54.664513, mean_q: 109.403206
 61177/100000: episode: 402, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.311 [-0.887, 2.373], loss: 10.789565, mean_absolute_error: 54.647774, mean_q: 109.491722
 61377/100000: episode: 403, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.626, 0.815], loss: 16.125008, mean_absolute_error: 54.663940, mean_q: 109.356972
 61534/100000: episode: 404, duration: 0.217s, episode steps: 157, steps per second: 723, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.400 [-0.755, 2.333], loss: 19.060713, mean_absolute_error: 54.904743, mean_q: 109.729675
 61734/100000: episode: 405, duration: 0.276s, episode steps: 200, steps per second: 725, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.177 [-0.803, 1.455], loss: 10.887274, mean_absolute_error: 54.981270, mean_q: 110.116394
 61934/100000: episode: 406, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.308 [-1.031, 2.292], loss: 11.820852, mean_absolute_error: 54.795586, mean_q: 109.780060
 62134/100000: episode: 407, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.011 [-0.852, 0.829], loss: 20.201138, mean_absolute_error: 54.658939, mean_q: 109.196426
 62334/100000: episode: 408, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.030 [-0.687, 0.715], loss: 11.362665, mean_absolute_error: 54.760956, mean_q: 109.679794
 62534/100000: episode: 409, duration: 0.278s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.064 [-0.838, 0.758], loss: 14.334128, mean_absolute_error: 55.015030, mean_q: 110.215179
 62734/100000: episode: 410, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.042 [-0.605, 0.603], loss: 15.300346, mean_absolute_error: 54.945152, mean_q: 109.927139
 62934/100000: episode: 411, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.615, 0.826], loss: 10.817460, mean_absolute_error: 54.791134, mean_q: 109.769745
 63134/100000: episode: 412, duration: 0.277s, episode steps: 200, steps per second: 722, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.012 [-0.876, 0.868], loss: 15.210249, mean_absolute_error: 55.074364, mean_q: 110.237511
 63334/100000: episode: 413, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.109 [-0.751, 1.496], loss: 15.199453, mean_absolute_error: 54.826328, mean_q: 109.761940
 63534/100000: episode: 414, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.208 [-0.727, 1.687], loss: 12.526851, mean_absolute_error: 54.857853, mean_q: 109.855850
 63734/100000: episode: 415, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.013 [-0.645, 0.609], loss: 16.886847, mean_absolute_error: 55.145119, mean_q: 110.446823
 63934/100000: episode: 416, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-0.816, 0.790], loss: 14.755750, mean_absolute_error: 55.183800, mean_q: 110.475830
 64134/100000: episode: 417, duration: 0.291s, episode steps: 200, steps per second: 686, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.054 [-0.812, 0.752], loss: 11.549738, mean_absolute_error: 55.079224, mean_q: 110.441780
 64334/100000: episode: 418, duration: 0.291s, episode steps: 200, steps per second: 686, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.003 [-1.059, 0.861], loss: 16.184502, mean_absolute_error: 55.067345, mean_q: 110.171387
 64521/100000: episode: 419, duration: 0.263s, episode steps: 187, steps per second: 711, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.345 [-0.685, 2.434], loss: 18.838753, mean_absolute_error: 55.373566, mean_q: 110.835854
 64682/100000: episode: 420, duration: 0.231s, episode steps: 161, steps per second: 696, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.350 [-0.762, 2.082], loss: 16.824806, mean_absolute_error: 55.364578, mean_q: 110.826187
 64882/100000: episode: 421, duration: 0.291s, episode steps: 200, steps per second: 687, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.088 [-0.657, 1.092], loss: 19.433788, mean_absolute_error: 55.184582, mean_q: 110.350334
 65070/100000: episode: 422, duration: 0.269s, episode steps: 188, steps per second: 699, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.331 [-1.031, 2.334], loss: 14.113248, mean_absolute_error: 55.308720, mean_q: 110.646912
 65270/100000: episode: 423, duration: 0.289s, episode steps: 200, steps per second: 692, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-0.827, 0.577], loss: 14.876215, mean_absolute_error: 54.981495, mean_q: 110.006554
 65470/100000: episode: 424, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.095 [-0.649, 1.164], loss: 13.705986, mean_absolute_error: 55.350430, mean_q: 110.814392
 65670/100000: episode: 425, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-0.770, 0.835], loss: 12.764468, mean_absolute_error: 55.310768, mean_q: 110.827873
 65855/100000: episode: 426, duration: 0.260s, episode steps: 185, steps per second: 713, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.342 [-0.941, 2.402], loss: 21.846415, mean_absolute_error: 55.359882, mean_q: 110.620293
 66026/100000: episode: 427, duration: 0.238s, episode steps: 171, steps per second: 718, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.377 [-0.932, 2.384], loss: 11.319091, mean_absolute_error: 55.013088, mean_q: 110.367783
 66226/100000: episode: 428, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.015 [-0.835, 0.708], loss: 11.284405, mean_absolute_error: 54.969521, mean_q: 110.288849
 66426/100000: episode: 429, duration: 0.284s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.021 [-0.798, 0.814], loss: 17.288424, mean_absolute_error: 55.175480, mean_q: 110.389542
 66626/100000: episode: 430, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.263 [-1.134, 2.261], loss: 14.615144, mean_absolute_error: 55.674999, mean_q: 111.333717
 66826/100000: episode: 431, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.106 [-0.772, 1.141], loss: 13.357980, mean_absolute_error: 55.742821, mean_q: 111.502731
 67026/100000: episode: 432, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.248 [-0.928, 2.013], loss: 15.315224, mean_absolute_error: 55.050869, mean_q: 110.214729
 67226/100000: episode: 433, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-0.646, 0.586], loss: 15.747658, mean_absolute_error: 55.496162, mean_q: 111.032234
 67426/100000: episode: 434, duration: 0.287s, episode steps: 200, steps per second: 698, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.145 [-0.935, 1.911], loss: 14.990522, mean_absolute_error: 55.559715, mean_q: 111.274025
 67626/100000: episode: 435, duration: 0.290s, episode steps: 200, steps per second: 689, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-0.829, 0.831], loss: 17.329782, mean_absolute_error: 55.705299, mean_q: 111.280251
 67826/100000: episode: 436, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.213 [-1.016, 2.198], loss: 14.490201, mean_absolute_error: 55.190548, mean_q: 110.390320
 68026/100000: episode: 437, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.873, 0.876], loss: 13.809262, mean_absolute_error: 55.065639, mean_q: 110.237793
 68226/100000: episode: 438, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.085 [-0.677, 0.813], loss: 11.297020, mean_absolute_error: 55.227615, mean_q: 110.495781
 68413/100000: episode: 439, duration: 0.260s, episode steps: 187, steps per second: 720, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.344 [-0.968, 2.401], loss: 10.200475, mean_absolute_error: 55.209236, mean_q: 110.641052
 68604/100000: episode: 440, duration: 0.267s, episode steps: 191, steps per second: 716, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.325 [-1.063, 2.439], loss: 11.511220, mean_absolute_error: 55.592384, mean_q: 111.365891
 68804/100000: episode: 441, duration: 0.291s, episode steps: 200, steps per second: 688, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.122 [-1.092, 1.703], loss: 16.378586, mean_absolute_error: 55.380791, mean_q: 110.912071
 69004/100000: episode: 442, duration: 0.324s, episode steps: 200, steps per second: 617, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.233 [-1.101, 2.554], loss: 15.477528, mean_absolute_error: 55.506592, mean_q: 110.972107
 69204/100000: episode: 443, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.767, 0.717], loss: 15.027042, mean_absolute_error: 55.033398, mean_q: 110.140800
 69404/100000: episode: 444, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-0.834, 0.846], loss: 18.052269, mean_absolute_error: 55.031864, mean_q: 110.071159
 69604/100000: episode: 445, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-0.797, 0.842], loss: 12.926700, mean_absolute_error: 55.043789, mean_q: 110.202713
 69792/100000: episode: 446, duration: 0.271s, episode steps: 188, steps per second: 693, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.331 [-1.157, 2.802], loss: 10.675730, mean_absolute_error: 55.339451, mean_q: 110.817390
 69992/100000: episode: 447, duration: 0.290s, episode steps: 200, steps per second: 689, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.006 [-0.748, 0.686], loss: 17.813745, mean_absolute_error: 55.542870, mean_q: 111.045807
 70192/100000: episode: 448, duration: 0.278s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.004 [-0.612, 0.872], loss: 11.328382, mean_absolute_error: 55.136806, mean_q: 110.447693
 70392/100000: episode: 449, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.056 [-0.864, 0.798], loss: 14.254390, mean_absolute_error: 55.347534, mean_q: 110.687637
 70592/100000: episode: 450, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.166 [-0.868, 2.016], loss: 14.901037, mean_absolute_error: 55.389931, mean_q: 110.755295
 70792/100000: episode: 451, duration: 0.277s, episode steps: 200, steps per second: 722, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.013 [-1.149, 0.830], loss: 12.756660, mean_absolute_error: 55.299335, mean_q: 110.749046
 70992/100000: episode: 452, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-0.669, 0.791], loss: 10.247047, mean_absolute_error: 55.312557, mean_q: 110.782677
 71192/100000: episode: 453, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.071 [-0.708, 0.928], loss: 18.079058, mean_absolute_error: 55.472919, mean_q: 110.737434
 71392/100000: episode: 454, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.062 [-0.709, 0.723], loss: 10.864910, mean_absolute_error: 55.163502, mean_q: 110.414825
 71592/100000: episode: 455, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.839, 0.639], loss: 15.638352, mean_absolute_error: 55.168320, mean_q: 110.293312
 71763/100000: episode: 456, duration: 0.244s, episode steps: 171, steps per second: 702, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.320 [-0.700, 2.039], loss: 9.757490, mean_absolute_error: 55.340748, mean_q: 110.633614
 71963/100000: episode: 457, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.004 [-0.841, 0.644], loss: 13.708535, mean_absolute_error: 55.270107, mean_q: 110.515526
 72163/100000: episode: 458, duration: 0.286s, episode steps: 200, steps per second: 698, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-0.922, 0.760], loss: 17.913799, mean_absolute_error: 55.078827, mean_q: 109.955681
 72363/100000: episode: 459, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.243 [-1.283, 2.276], loss: 12.422608, mean_absolute_error: 55.034634, mean_q: 110.048470
 72548/100000: episode: 460, duration: 0.259s, episode steps: 185, steps per second: 715, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.344 [-1.290, 2.401], loss: 17.278982, mean_absolute_error: 54.910065, mean_q: 109.748436
 72748/100000: episode: 461, duration: 0.291s, episode steps: 200, steps per second: 688, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.099 [-0.930, 0.776], loss: 14.622529, mean_absolute_error: 54.857410, mean_q: 109.753593
 72888/100000: episode: 462, duration: 0.200s, episode steps: 140, steps per second: 700, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.260 [-1.084, 1.915], loss: 18.693600, mean_absolute_error: 54.855026, mean_q: 109.463142
 73088/100000: episode: 463, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.012 [-0.786, 0.841], loss: 21.831858, mean_absolute_error: 55.023624, mean_q: 109.765564
 73288/100000: episode: 464, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.042 [-0.997, 0.813], loss: 13.554824, mean_absolute_error: 54.610615, mean_q: 109.248215
 73488/100000: episode: 465, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.020 [-0.886, 0.809], loss: 9.688572, mean_absolute_error: 54.630718, mean_q: 109.337227
 73668/100000: episode: 466, duration: 0.250s, episode steps: 180, steps per second: 719, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.296 [-1.252, 2.286], loss: 15.965974, mean_absolute_error: 54.798458, mean_q: 109.478638
 73868/100000: episode: 467, duration: 0.290s, episode steps: 200, steps per second: 689, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-0.866, 0.856], loss: 17.371111, mean_absolute_error: 54.840714, mean_q: 109.453865
 74068/100000: episode: 468, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.028 [-1.081, 0.881], loss: 19.562668, mean_absolute_error: 54.611343, mean_q: 108.982742
 74268/100000: episode: 469, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.583, 0.636], loss: 14.659069, mean_absolute_error: 54.697018, mean_q: 109.331253
 74468/100000: episode: 470, duration: 0.292s, episode steps: 200, steps per second: 685, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-0.709, 0.783], loss: 13.068743, mean_absolute_error: 54.349503, mean_q: 108.736717
 74668/100000: episode: 471, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.038 [-0.779, 0.866], loss: 20.896311, mean_absolute_error: 54.287098, mean_q: 108.408066
 74868/100000: episode: 472, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-0.822, 0.618], loss: 11.527565, mean_absolute_error: 53.999825, mean_q: 108.090256
 75068/100000: episode: 473, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.004 [-0.685, 0.777], loss: 15.190010, mean_absolute_error: 54.029160, mean_q: 108.012383
 75268/100000: episode: 474, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-0.855, 0.745], loss: 7.676896, mean_absolute_error: 54.079235, mean_q: 108.243477
 75468/100000: episode: 475, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-0.647, 0.767], loss: 15.783225, mean_absolute_error: 54.303795, mean_q: 108.568954
 75668/100000: episode: 476, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-0.804, 0.637], loss: 12.527322, mean_absolute_error: 54.270546, mean_q: 108.507439
 75868/100000: episode: 477, duration: 0.278s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.107 [-0.867, 1.481], loss: 14.622128, mean_absolute_error: 53.910587, mean_q: 107.719002
 76068/100000: episode: 478, duration: 0.278s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.027 [-0.756, 0.868], loss: 9.988708, mean_absolute_error: 54.055279, mean_q: 108.107346
 76268/100000: episode: 479, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.768, 0.819], loss: 12.950122, mean_absolute_error: 54.005001, mean_q: 107.914803
 76468/100000: episode: 480, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.892, 0.808], loss: 13.114890, mean_absolute_error: 54.260155, mean_q: 108.504417
 76667/100000: episode: 481, duration: 0.282s, episode steps: 199, steps per second: 707, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.272 [-1.201, 2.366], loss: 10.398625, mean_absolute_error: 54.156219, mean_q: 108.353104
 76867/100000: episode: 482, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.154 [-1.192, 2.025], loss: 16.314703, mean_absolute_error: 54.276573, mean_q: 108.471039
 77067/100000: episode: 483, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.014 [-1.077, 0.876], loss: 14.662407, mean_absolute_error: 54.225510, mean_q: 108.326668
 77267/100000: episode: 484, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.003 [-0.650, 0.766], loss: 15.918809, mean_absolute_error: 53.930157, mean_q: 107.740761
 77467/100000: episode: 485, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.007 [-1.049, 0.833], loss: 13.391191, mean_absolute_error: 54.490063, mean_q: 108.986320
 77667/100000: episode: 486, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.045 [-0.994, 0.749], loss: 16.336872, mean_absolute_error: 54.283955, mean_q: 108.360626
 77867/100000: episode: 487, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-0.987, 0.756], loss: 14.266853, mean_absolute_error: 54.397671, mean_q: 108.708084
 78067/100000: episode: 488, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.007 [-0.868, 0.830], loss: 16.621449, mean_absolute_error: 53.914532, mean_q: 107.637794
 78264/100000: episode: 489, duration: 0.279s, episode steps: 197, steps per second: 706, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.221 [-1.343, 2.289], loss: 8.214203, mean_absolute_error: 54.159000, mean_q: 108.258797
 78464/100000: episode: 490, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.020 [-0.782, 0.741], loss: 13.701896, mean_absolute_error: 53.759716, mean_q: 107.314529
 78664/100000: episode: 491, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.018 [-0.971, 0.878], loss: 14.328300, mean_absolute_error: 53.914242, mean_q: 107.525978
 78864/100000: episode: 492, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.234 [-1.401, 2.219], loss: 10.830205, mean_absolute_error: 53.515079, mean_q: 106.993301
 79064/100000: episode: 493, duration: 0.289s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.941, 0.882], loss: 12.724600, mean_absolute_error: 53.907913, mean_q: 107.700485
 79264/100000: episode: 494, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.104 [-1.135, 1.486], loss: 14.056553, mean_absolute_error: 53.759628, mean_q: 107.411369
 79464/100000: episode: 495, duration: 0.284s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.849, 0.870], loss: 15.952759, mean_absolute_error: 53.813534, mean_q: 107.494919
 79664/100000: episode: 496, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.037 [-1.075, 0.918], loss: 13.859162, mean_absolute_error: 53.668320, mean_q: 107.299744
 79864/100000: episode: 497, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.012 [-1.161, 0.818], loss: 13.675368, mean_absolute_error: 53.709023, mean_q: 107.276947
 80064/100000: episode: 498, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-0.815, 0.891], loss: 21.260735, mean_absolute_error: 53.480412, mean_q: 106.453751
 80264/100000: episode: 499, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.080 [-0.834, 0.646], loss: 17.170788, mean_absolute_error: 53.162235, mean_q: 106.034492
 80464/100000: episode: 500, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.046 [-0.874, 0.873], loss: 9.338195, mean_absolute_error: 53.063091, mean_q: 106.195824
 80664/100000: episode: 501, duration: 0.278s, episode steps: 200, steps per second: 720, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.047 [-0.926, 0.726], loss: 5.824575, mean_absolute_error: 52.965244, mean_q: 105.933929
 80864/100000: episode: 502, duration: 0.289s, episode steps: 200, steps per second: 693, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.858, 0.849], loss: 13.060891, mean_absolute_error: 53.104630, mean_q: 106.081848
 81064/100000: episode: 503, duration: 0.289s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.130 [-0.973, 0.798], loss: 14.811802, mean_absolute_error: 53.183876, mean_q: 106.126404
 81264/100000: episode: 504, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.815, 0.842], loss: 11.565879, mean_absolute_error: 53.169239, mean_q: 106.289169
 81464/100000: episode: 505, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.168 [-1.221, 0.696], loss: 13.402691, mean_absolute_error: 52.803154, mean_q: 105.462997
 81664/100000: episode: 506, duration: 0.292s, episode steps: 200, steps per second: 685, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.308 [-1.477, 2.818], loss: 12.426968, mean_absolute_error: 52.694569, mean_q: 105.193321
 81864/100000: episode: 507, duration: 0.290s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.104 [-1.115, 1.719], loss: 15.318525, mean_absolute_error: 52.965904, mean_q: 105.697296
 82037/100000: episode: 508, duration: 0.249s, episode steps: 173, steps per second: 695, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.335 [-1.325, 2.257], loss: 20.314222, mean_absolute_error: 52.754345, mean_q: 105.102379
 82237/100000: episode: 509, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.050 [-0.780, 1.094], loss: 13.669221, mean_absolute_error: 52.515507, mean_q: 104.990135
 82424/100000: episode: 510, duration: 0.275s, episode steps: 187, steps per second: 680, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.285 [-1.182, 2.400], loss: 11.846835, mean_absolute_error: 52.792542, mean_q: 105.526489
 82624/100000: episode: 511, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.890, 0.790], loss: 15.536648, mean_absolute_error: 52.753048, mean_q: 105.356750
 82824/100000: episode: 512, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.002 [-0.834, 0.617], loss: 14.168221, mean_absolute_error: 52.812973, mean_q: 105.538200
 83024/100000: episode: 513, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-1.006, 0.914], loss: 12.878356, mean_absolute_error: 52.520088, mean_q: 105.131142
 83224/100000: episode: 514, duration: 0.283s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.000 [-0.744, 0.837], loss: 14.917650, mean_absolute_error: 52.484230, mean_q: 104.829445
 83424/100000: episode: 515, duration: 0.284s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.013 [-0.889, 0.814], loss: 17.206259, mean_absolute_error: 52.737999, mean_q: 105.266068
 83624/100000: episode: 516, duration: 0.291s, episode steps: 200, steps per second: 687, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.004 [-0.621, 0.823], loss: 16.559952, mean_absolute_error: 52.554562, mean_q: 104.974472
 83824/100000: episode: 517, duration: 0.284s, episode steps: 200, steps per second: 705, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.003 [-0.901, 0.803], loss: 12.354539, mean_absolute_error: 52.394630, mean_q: 104.773926
 84024/100000: episode: 518, duration: 0.289s, episode steps: 200, steps per second: 692, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-1.052, 0.883], loss: 12.188951, mean_absolute_error: 52.603928, mean_q: 105.218674
 84224/100000: episode: 519, duration: 0.287s, episode steps: 200, steps per second: 697, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.143 [-1.038, 0.587], loss: 10.448433, mean_absolute_error: 52.716221, mean_q: 105.487030
 84424/100000: episode: 520, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-0.884, 0.954], loss: 16.935858, mean_absolute_error: 52.610664, mean_q: 105.011681
 84624/100000: episode: 521, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.006 [-1.126, 0.875], loss: 12.156513, mean_absolute_error: 52.820763, mean_q: 105.540527
 84824/100000: episode: 522, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.991, 0.870], loss: 16.463547, mean_absolute_error: 52.420742, mean_q: 104.739021
 85024/100000: episode: 523, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.067 [-1.008, 0.806], loss: 16.689144, mean_absolute_error: 52.600636, mean_q: 105.120255
 85224/100000: episode: 524, duration: 0.293s, episode steps: 200, steps per second: 683, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.822, 0.748], loss: 8.908755, mean_absolute_error: 52.646488, mean_q: 105.468338
 85424/100000: episode: 525, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.001 [-0.814, 0.770], loss: 18.274672, mean_absolute_error: 52.495941, mean_q: 104.860672
 85624/100000: episode: 526, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.008 [-0.917, 1.014], loss: 19.007587, mean_absolute_error: 52.401169, mean_q: 104.491669
 85824/100000: episode: 527, duration: 0.291s, episode steps: 200, steps per second: 686, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.001 [-1.098, 0.873], loss: 12.142539, mean_absolute_error: 52.278446, mean_q: 104.551071
 86024/100000: episode: 528, duration: 0.288s, episode steps: 200, steps per second: 695, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-1.044, 0.957], loss: 9.299917, mean_absolute_error: 52.502365, mean_q: 104.978401
 86224/100000: episode: 529, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-0.871, 0.886], loss: 10.337482, mean_absolute_error: 52.563873, mean_q: 105.065979
 86424/100000: episode: 530, duration: 0.279s, episode steps: 200, steps per second: 718, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.048 [-0.939, 0.775], loss: 16.859116, mean_absolute_error: 52.392811, mean_q: 104.446602
 86624/100000: episode: 531, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.199 [-1.386, 0.765], loss: 13.933826, mean_absolute_error: 52.289230, mean_q: 104.294334
 86824/100000: episode: 532, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.230 [-1.668, 0.799], loss: 15.871995, mean_absolute_error: 52.127316, mean_q: 103.985939
 87024/100000: episode: 533, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.091 [-0.872, 1.509], loss: 21.267620, mean_absolute_error: 52.180958, mean_q: 104.021545
 87224/100000: episode: 534, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-0.891, 0.713], loss: 18.461227, mean_absolute_error: 52.006535, mean_q: 103.754646
 87424/100000: episode: 535, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.177 [-1.191, 1.861], loss: 15.052046, mean_absolute_error: 51.959084, mean_q: 103.819435
 87624/100000: episode: 536, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.132 [-0.950, 0.751], loss: 18.172623, mean_absolute_error: 52.014347, mean_q: 103.781174
 87824/100000: episode: 537, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.819, 0.768], loss: 10.851387, mean_absolute_error: 51.587242, mean_q: 103.138649
 88024/100000: episode: 538, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.094 [-0.891, 0.841], loss: 19.355476, mean_absolute_error: 51.600353, mean_q: 103.169502
 88224/100000: episode: 539, duration: 0.293s, episode steps: 200, steps per second: 682, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.043 [-0.850, 0.804], loss: 8.795984, mean_absolute_error: 51.905548, mean_q: 103.942635
 88406/100000: episode: 540, duration: 0.260s, episode steps: 182, steps per second: 700, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.332 [-0.804, 2.427], loss: 20.212948, mean_absolute_error: 52.173111, mean_q: 103.930367
 88606/100000: episode: 541, duration: 0.291s, episode steps: 200, steps per second: 688, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.078 [-0.815, 1.139], loss: 10.747448, mean_absolute_error: 51.910484, mean_q: 103.847504
 88806/100000: episode: 542, duration: 0.280s, episode steps: 200, steps per second: 714, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.851, 0.847], loss: 17.678720, mean_absolute_error: 51.737186, mean_q: 103.258232
 89006/100000: episode: 543, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.005 [-1.150, 0.860], loss: 13.050550, mean_absolute_error: 51.626217, mean_q: 103.091560
 89206/100000: episode: 544, duration: 0.281s, episode steps: 200, steps per second: 711, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-0.919, 0.781], loss: 17.998795, mean_absolute_error: 51.853065, mean_q: 103.497482
 89406/100000: episode: 545, duration: 0.292s, episode steps: 200, steps per second: 685, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.030 [-0.826, 0.710], loss: 14.679370, mean_absolute_error: 52.122753, mean_q: 104.044579
 89587/100000: episode: 546, duration: 0.262s, episode steps: 181, steps per second: 691, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.348 [-0.934, 2.790], loss: 22.131439, mean_absolute_error: 51.675167, mean_q: 102.980446
 89787/100000: episode: 547, duration: 0.293s, episode steps: 200, steps per second: 682, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.195 [-1.279, 0.732], loss: 17.209591, mean_absolute_error: 51.597809, mean_q: 102.857239
 89987/100000: episode: 548, duration: 0.289s, episode steps: 200, steps per second: 693, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-1.082, 0.820], loss: 12.524727, mean_absolute_error: 52.029186, mean_q: 103.889496
 90187/100000: episode: 549, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.058 [-0.881, 0.815], loss: 13.005200, mean_absolute_error: 51.461395, mean_q: 102.831406
 90387/100000: episode: 550, duration: 0.283s, episode steps: 200, steps per second: 706, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.010 [-0.618, 0.867], loss: 13.212877, mean_absolute_error: 51.664459, mean_q: 103.229904
 90587/100000: episode: 551, duration: 0.287s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-0.688, 0.721], loss: 15.200344, mean_absolute_error: 51.687012, mean_q: 103.188644
 90787/100000: episode: 552, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.929, 0.803], loss: 16.496698, mean_absolute_error: 51.660179, mean_q: 103.131462
 90987/100000: episode: 553, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.028 [-0.822, 1.092], loss: 13.424878, mean_absolute_error: 51.640087, mean_q: 103.112206
 91187/100000: episode: 554, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.096 [-0.906, 1.158], loss: 20.013350, mean_absolute_error: 51.648251, mean_q: 102.818497
 91387/100000: episode: 555, duration: 0.280s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.738, 0.944], loss: 18.243013, mean_absolute_error: 51.893116, mean_q: 103.557411
 91587/100000: episode: 556, duration: 0.286s, episode steps: 200, steps per second: 698, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.009 [-0.847, 0.857], loss: 13.303645, mean_absolute_error: 51.568703, mean_q: 103.105270
 91787/100000: episode: 557, duration: 0.289s, episode steps: 200, steps per second: 691, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.789, 0.897], loss: 9.306112, mean_absolute_error: 51.473740, mean_q: 103.042618
 91961/100000: episode: 558, duration: 0.250s, episode steps: 174, steps per second: 696, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.354 [-1.193, 2.436], loss: 16.764296, mean_absolute_error: 51.844727, mean_q: 103.485870
 92161/100000: episode: 559, duration: 0.285s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.801, 0.863], loss: 20.619707, mean_absolute_error: 51.788273, mean_q: 103.234482
 92361/100000: episode: 560, duration: 0.289s, episode steps: 200, steps per second: 692, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.066 [-0.676, 0.977], loss: 10.051722, mean_absolute_error: 51.653553, mean_q: 103.305237
 92561/100000: episode: 561, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.001 [-0.801, 0.772], loss: 8.751500, mean_absolute_error: 51.917236, mean_q: 103.839523
 92761/100000: episode: 562, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.001 [-0.851, 0.854], loss: 15.506221, mean_absolute_error: 51.992432, mean_q: 103.840469
 92961/100000: episode: 563, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.055 [-0.820, 0.793], loss: 13.175392, mean_absolute_error: 51.714424, mean_q: 103.322357
 93161/100000: episode: 564, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.027 [-0.828, 0.790], loss: 11.486469, mean_absolute_error: 51.869259, mean_q: 103.740578
 93361/100000: episode: 565, duration: 0.277s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-0.811, 0.808], loss: 19.044527, mean_absolute_error: 51.701614, mean_q: 103.249290
 93561/100000: episode: 566, duration: 0.288s, episode steps: 200, steps per second: 695, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.086 [-0.875, 1.343], loss: 23.653862, mean_absolute_error: 51.767559, mean_q: 103.250443
 93761/100000: episode: 567, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.011 [-0.875, 0.789], loss: 15.268414, mean_absolute_error: 51.558937, mean_q: 102.996895
 93961/100000: episode: 568, duration: 0.284s, episode steps: 200, steps per second: 704, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.146 [-0.902, 1.696], loss: 19.027992, mean_absolute_error: 51.378311, mean_q: 102.639343
 94161/100000: episode: 569, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-0.906, 0.997], loss: 19.179403, mean_absolute_error: 51.392525, mean_q: 102.580391
 94361/100000: episode: 570, duration: 0.281s, episode steps: 200, steps per second: 713, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.843, 0.917], loss: 16.651247, mean_absolute_error: 51.284855, mean_q: 102.444824
 94561/100000: episode: 571, duration: 0.283s, episode steps: 200, steps per second: 707, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.781, 0.831], loss: 17.146276, mean_absolute_error: 51.775352, mean_q: 103.297653
 94761/100000: episode: 572, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.014 [-0.768, 0.951], loss: 12.738342, mean_absolute_error: 51.040314, mean_q: 101.977737
 94961/100000: episode: 573, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.088 [-0.841, 0.893], loss: 15.616738, mean_absolute_error: 51.207256, mean_q: 102.330330
 95161/100000: episode: 574, duration: 0.288s, episode steps: 200, steps per second: 696, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.812, 0.892], loss: 21.040377, mean_absolute_error: 51.233818, mean_q: 102.218086
 95361/100000: episode: 575, duration: 0.285s, episode steps: 200, steps per second: 702, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.103 [-1.008, 0.917], loss: 12.330588, mean_absolute_error: 51.337139, mean_q: 102.516785
 95561/100000: episode: 576, duration: 0.290s, episode steps: 200, steps per second: 690, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.160 [-1.078, 0.822], loss: 11.881835, mean_absolute_error: 51.019566, mean_q: 101.928459
 95761/100000: episode: 577, duration: 0.288s, episode steps: 200, steps per second: 695, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.211 [-1.223, 2.460], loss: 16.384075, mean_absolute_error: 51.019352, mean_q: 101.710175
 95961/100000: episode: 578, duration: 0.288s, episode steps: 200, steps per second: 694, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.013 [-0.841, 1.032], loss: 14.279407, mean_absolute_error: 51.083603, mean_q: 101.803017
 96161/100000: episode: 579, duration: 0.282s, episode steps: 200, steps per second: 708, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.141 [-1.156, 0.883], loss: 17.470844, mean_absolute_error: 51.055576, mean_q: 101.859596
 96361/100000: episode: 580, duration: 0.282s, episode steps: 200, steps per second: 710, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-1.146, 1.039], loss: 12.737042, mean_absolute_error: 50.864464, mean_q: 101.700287
 96561/100000: episode: 581, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.969, 0.995], loss: 13.144026, mean_absolute_error: 50.744434, mean_q: 101.297966
 96761/100000: episode: 582, duration: 0.288s, episode steps: 200, steps per second: 695, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.164 [-1.143, 0.806], loss: 15.929785, mean_absolute_error: 50.668800, mean_q: 101.215294
 96961/100000: episode: 583, duration: 0.281s, episode steps: 200, steps per second: 712, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-0.920, 0.835], loss: 11.181677, mean_absolute_error: 50.959770, mean_q: 101.957069
 97161/100000: episode: 584, duration: 0.286s, episode steps: 200, steps per second: 700, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.000 [-0.836, 0.856], loss: 17.992002, mean_absolute_error: 50.883579, mean_q: 101.550705
 97361/100000: episode: 585, duration: 0.279s, episode steps: 200, steps per second: 717, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.051 [-1.378, 0.954], loss: 15.498814, mean_absolute_error: 50.675583, mean_q: 101.179138
 97561/100000: episode: 586, duration: 0.284s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.033 [-1.061, 1.072], loss: 20.093981, mean_absolute_error: 50.801991, mean_q: 101.283424
 97761/100000: episode: 587, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.000 [-0.817, 0.891], loss: 14.105721, mean_absolute_error: 50.877689, mean_q: 101.595680
 97961/100000: episode: 588, duration: 0.278s, episode steps: 200, steps per second: 719, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.806, 0.703], loss: 14.549704, mean_absolute_error: 50.541161, mean_q: 100.987755
 98161/100000: episode: 589, duration: 0.285s, episode steps: 200, steps per second: 703, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.081 [-0.995, 0.909], loss: 17.496481, mean_absolute_error: 50.391361, mean_q: 100.594925
 98361/100000: episode: 590, duration: 0.286s, episode steps: 200, steps per second: 699, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.941, 0.768], loss: 19.977047, mean_absolute_error: 50.387554, mean_q: 100.379242
 98561/100000: episode: 591, duration: 0.285s, episode steps: 200, steps per second: 701, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.751, 0.847], loss: 13.550005, mean_absolute_error: 50.368622, mean_q: 100.612701
 98761/100000: episode: 592, duration: 0.279s, episode steps: 200, steps per second: 716, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.858, 1.015], loss: 16.305960, mean_absolute_error: 50.190086, mean_q: 100.270935
 98961/100000: episode: 593, duration: 0.278s, episode steps: 200, steps per second: 721, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-1.410, 1.004], loss: 10.521289, mean_absolute_error: 50.642891, mean_q: 101.308304
 99161/100000: episode: 594, duration: 0.288s, episode steps: 200, steps per second: 693, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-1.052, 0.949], loss: 14.929597, mean_absolute_error: 50.445770, mean_q: 100.747093
 99361/100000: episode: 595, duration: 0.297s, episode steps: 200, steps per second: 673, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.732, 0.829], loss: 12.395757, mean_absolute_error: 50.503017, mean_q: 100.864998
 99561/100000: episode: 596, duration: 0.290s, episode steps: 200, steps per second: 690, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.021 [-0.762, 0.933], loss: 5.758638, mean_absolute_error: 50.398945, mean_q: 100.908211
 99761/100000: episode: 597, duration: 0.300s, episode steps: 200, steps per second: 666, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.821, 0.957], loss: 14.485915, mean_absolute_error: 50.315216, mean_q: 100.507042
 99961/100000: episode: 598, duration: 0.282s, episode steps: 200, steps per second: 709, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.013 [-1.057, 0.988], loss: 15.028800, mean_absolute_error: 50.789833, mean_q: 101.278366
done, took 143.376 seconds
Testing for 10 episodes ...
Episode 1: reward: 200.000, steps: 200
Episode 2: reward: 200.000, steps: 200
Episode 3: reward: 200.000, steps: 200
Episode 4: reward: 200.000, steps: 200
Episode 5: reward: 200.000, steps: 200
Episode 6: reward: 200.000, steps: 200
Episode 7: reward: 200.000, steps: 200
Episode 8: reward: 200.000, steps: 200
Episode 9: reward: 200.000, steps: 200
Episode 10: reward: 200.000, steps: 200
