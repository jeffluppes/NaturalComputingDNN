_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                80        
_________________________________________________________________
activation_1 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_2 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_3 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 34        
_________________________________________________________________
activation_4 (Activation)    (None, 2)                 0         
=================================================================
Total params: 658
Trainable params: 658
Non-trainable params: 0
_________________________________________________________________
None
Training for 100000 steps ...
    25/100000: episode: 1, duration: 0.896s, episode steps: 25, steps per second: 28, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.118 [-0.840, 0.355], mean_best_reward: --
    36/100000: episode: 2, duration: 0.002s, episode steps: 11, steps per second: 4964, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-1.173, 1.774], mean_best_reward: --
    55/100000: episode: 3, duration: 0.003s, episode steps: 19, steps per second: 5630, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.093 [-1.363, 2.443], mean_best_reward: --
    65/100000: episode: 4, duration: 0.002s, episode steps: 10, steps per second: 4344, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.124 [-2.534, 1.569], mean_best_reward: --
    76/100000: episode: 5, duration: 0.002s, episode steps: 11, steps per second: 5101, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.917, 1.132], mean_best_reward: --
    89/100000: episode: 6, duration: 0.002s, episode steps: 13, steps per second: 5286, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.090 [-1.373, 2.212], mean_best_reward: --
    99/100000: episode: 7, duration: 0.002s, episode steps: 10, steps per second: 5006, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.112 [-1.912, 1.157], mean_best_reward: --
   121/100000: episode: 8, duration: 0.004s, episode steps: 22, steps per second: 5757, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.610, 1.022], mean_best_reward: --
   147/100000: episode: 9, duration: 0.006s, episode steps: 26, steps per second: 4711, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.065 [-1.164, 2.093], mean_best_reward: --
   160/100000: episode: 10, duration: 0.002s, episode steps: 13, steps per second: 5233, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.094 [-1.702, 0.985], mean_best_reward: --
   181/100000: episode: 11, duration: 0.004s, episode steps: 21, steps per second: 5727, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.068 [-0.994, 0.439], mean_best_reward: --
   194/100000: episode: 12, duration: 0.002s, episode steps: 13, steps per second: 5265, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.101 [-2.777, 1.766], mean_best_reward: --
   210/100000: episode: 13, duration: 0.003s, episode steps: 16, steps per second: 5490, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.083 [-1.185, 2.071], mean_best_reward: --
   229/100000: episode: 14, duration: 0.004s, episode steps: 19, steps per second: 5025, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.069 [-1.421, 2.410], mean_best_reward: --
   238/100000: episode: 15, duration: 0.002s, episode steps: 9, steps per second: 4888, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.172 [-2.852, 1.720], mean_best_reward: --
   249/100000: episode: 16, duration: 0.002s, episode steps: 11, steps per second: 5138, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.126 [-1.749, 2.848], mean_best_reward: --
   261/100000: episode: 17, duration: 0.002s, episode steps: 12, steps per second: 5225, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.136 [-2.572, 1.564], mean_best_reward: --
   289/100000: episode: 18, duration: 0.005s, episode steps: 28, steps per second: 5894, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: 0.006 [-2.442, 1.909], mean_best_reward: --
   298/100000: episode: 19, duration: 0.002s, episode steps: 9, steps per second: 4163, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.125 [-1.423, 2.313], mean_best_reward: --
   322/100000: episode: 20, duration: 0.004s, episode steps: 24, steps per second: 5750, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.059 [-1.974, 1.208], mean_best_reward: --
   332/100000: episode: 21, duration: 0.002s, episode steps: 10, steps per second: 5004, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.117 [-0.999, 1.669], mean_best_reward: --
   342/100000: episode: 22, duration: 0.002s, episode steps: 10, steps per second: 5035, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.155 [-1.546, 2.585], mean_best_reward: --
   357/100000: episode: 23, duration: 0.003s, episode steps: 15, steps per second: 5456, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.077 [-1.756, 2.718], mean_best_reward: --
   376/100000: episode: 24, duration: 0.003s, episode steps: 19, steps per second: 5476, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.087 [-0.975, 1.841], mean_best_reward: --
   389/100000: episode: 25, duration: 0.003s, episode steps: 13, steps per second: 4147, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.115 [-1.399, 2.387], mean_best_reward: --
   404/100000: episode: 26, duration: 0.003s, episode steps: 15, steps per second: 5447, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.065 [-2.681, 1.746], mean_best_reward: --
   424/100000: episode: 27, duration: 0.004s, episode steps: 20, steps per second: 5663, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.047 [-3.031, 1.988], mean_best_reward: --
   436/100000: episode: 28, duration: 0.002s, episode steps: 12, steps per second: 5082, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.102 [-2.548, 1.590], mean_best_reward: --
   446/100000: episode: 29, duration: 0.002s, episode steps: 10, steps per second: 4991, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.199, 2.058], mean_best_reward: --
   457/100000: episode: 30, duration: 0.002s, episode steps: 11, steps per second: 4984, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.136 [-1.777, 2.808], mean_best_reward: --
   561/100000: episode: 31, duration: 0.016s, episode steps: 104, steps per second: 6352, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.010 [-1.095, 0.823], mean_best_reward: --
   572/100000: episode: 32, duration: 0.002s, episode steps: 11, steps per second: 5042, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.104 [-2.247, 1.381], mean_best_reward: --
   593/100000: episode: 33, duration: 0.004s, episode steps: 21, steps per second: 5722, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.096 [-0.548, 1.083], mean_best_reward: --
   613/100000: episode: 34, duration: 0.004s, episode steps: 20, steps per second: 5682, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.084 [-0.629, 1.217], mean_best_reward: --
   637/100000: episode: 35, duration: 0.004s, episode steps: 24, steps per second: 5605, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.044 [-1.156, 0.762], mean_best_reward: --
   652/100000: episode: 36, duration: 0.003s, episode steps: 15, steps per second: 4342, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.093 [-1.721, 1.006], mean_best_reward: --
   666/100000: episode: 37, duration: 0.003s, episode steps: 14, steps per second: 5375, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.109 [-3.043, 1.944], mean_best_reward: --
   697/100000: episode: 38, duration: 0.005s, episode steps: 31, steps per second: 5940, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.355 [0.000, 1.000], mean observation: -0.016 [-1.952, 2.734], mean_best_reward: --
   708/100000: episode: 39, duration: 0.002s, episode steps: 11, steps per second: 5131, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.150 [-0.943, 1.795], mean_best_reward: --
   720/100000: episode: 40, duration: 0.002s, episode steps: 12, steps per second: 5207, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.117 [-1.971, 1.154], mean_best_reward: --
   730/100000: episode: 41, duration: 0.002s, episode steps: 10, steps per second: 4797, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.128 [-2.076, 1.167], mean_best_reward: --
   745/100000: episode: 42, duration: 0.003s, episode steps: 15, steps per second: 5412, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.084 [-1.553, 2.443], mean_best_reward: --
   757/100000: episode: 43, duration: 0.002s, episode steps: 12, steps per second: 5229, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.068 [-1.964, 1.409], mean_best_reward: --
   772/100000: episode: 44, duration: 0.003s, episode steps: 15, steps per second: 5116, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.106 [-2.292, 1.354], mean_best_reward: --
   792/100000: episode: 45, duration: 0.004s, episode steps: 20, steps per second: 5655, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.074 [-1.857, 1.180], mean_best_reward: --
   802/100000: episode: 46, duration: 0.002s, episode steps: 10, steps per second: 4769, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.128 [-1.544, 2.491], mean_best_reward: --
   811/100000: episode: 47, duration: 0.002s, episode steps: 9, steps per second: 4778, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.350, 1.421], mean_best_reward: --
   819/100000: episode: 48, duration: 0.002s, episode steps: 8, steps per second: 4788, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.149 [-2.230, 1.395], mean_best_reward: --
   828/100000: episode: 49, duration: 0.002s, episode steps: 9, steps per second: 4924, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.138 [-2.226, 1.379], mean_best_reward: --
   841/100000: episode: 50, duration: 0.002s, episode steps: 13, steps per second: 5312, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.128 [-1.373, 2.362], mean_best_reward: --
   851/100000: episode: 51, duration: 0.002s, episode steps: 10, steps per second: 5012, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.110 [-2.494, 1.597], mean_best_reward: --
   863/100000: episode: 52, duration: 0.002s, episode steps: 12, steps per second: 5234, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.122 [-1.546, 2.563], mean_best_reward: --
   892/100000: episode: 53, duration: 0.006s, episode steps: 29, steps per second: 4690, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.077 [-1.396, 0.571], mean_best_reward: --
   917/100000: episode: 54, duration: 0.004s, episode steps: 25, steps per second: 5818, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.082 [-0.599, 1.485], mean_best_reward: --
   933/100000: episode: 55, duration: 0.003s, episode steps: 16, steps per second: 5468, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.082 [-1.223, 2.107], mean_best_reward: --
   943/100000: episode: 56, duration: 0.002s, episode steps: 10, steps per second: 5009, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.126 [-1.200, 1.932], mean_best_reward: --
   958/100000: episode: 57, duration: 0.003s, episode steps: 15, steps per second: 4988, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.089 [-1.324, 2.225], mean_best_reward: --
   975/100000: episode: 58, duration: 0.003s, episode steps: 17, steps per second: 5441, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.098 [-1.182, 0.633], mean_best_reward: --
   985/100000: episode: 59, duration: 0.002s, episode steps: 10, steps per second: 5027, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.111 [-2.530, 1.605], mean_best_reward: --
  1001/100000: episode: 60, duration: 0.003s, episode steps: 16, steps per second: 5496, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.076 [-0.981, 1.601], mean_best_reward: --
  1017/100000: episode: 61, duration: 0.003s, episode steps: 16, steps per second: 5090, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.066 [-1.994, 3.066], mean_best_reward: --
  1030/100000: episode: 62, duration: 0.003s, episode steps: 13, steps per second: 5189, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.099 [-1.213, 1.901], mean_best_reward: --
  1051/100000: episode: 63, duration: 0.004s, episode steps: 21, steps per second: 5352, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.070 [-1.676, 0.941], mean_best_reward: --
  1061/100000: episode: 64, duration: 0.002s, episode steps: 10, steps per second: 5009, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.118 [-2.462, 1.597], mean_best_reward: --
  1077/100000: episode: 65, duration: 0.003s, episode steps: 16, steps per second: 5498, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.063 [-1.573, 2.468], mean_best_reward: --
  1087/100000: episode: 66, duration: 0.003s, episode steps: 10, steps per second: 3726, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.091 [-1.023, 1.628], mean_best_reward: --
  1104/100000: episode: 67, duration: 0.003s, episode steps: 17, steps per second: 5469, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.091 [-0.780, 1.334], mean_best_reward: --
  1125/100000: episode: 68, duration: 0.004s, episode steps: 21, steps per second: 4794, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.066 [-1.728, 2.804], mean_best_reward: --
  1135/100000: episode: 69, duration: 0.002s, episode steps: 10, steps per second: 4996, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.110 [-1.614, 0.969], mean_best_reward: --
  1151/100000: episode: 70, duration: 0.003s, episode steps: 16, steps per second: 5488, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.586, 1.188], mean_best_reward: --
  1168/100000: episode: 71, duration: 0.003s, episode steps: 17, steps per second: 5415, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.125 [-0.545, 0.936], mean_best_reward: --
  1182/100000: episode: 72, duration: 0.003s, episode steps: 14, steps per second: 5395, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.081 [-1.730, 2.614], mean_best_reward: --
  1197/100000: episode: 73, duration: 0.003s, episode steps: 15, steps per second: 5356, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.090 [-1.522, 0.937], mean_best_reward: --
  1207/100000: episode: 74, duration: 0.002s, episode steps: 10, steps per second: 4985, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.154 [-1.137, 2.062], mean_best_reward: --
  1221/100000: episode: 75, duration: 0.003s, episode steps: 14, steps per second: 5383, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.108 [-1.515, 0.757], mean_best_reward: --
  1240/100000: episode: 76, duration: 0.003s, episode steps: 19, steps per second: 5634, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.044 [-1.055, 0.642], mean_best_reward: --
  1249/100000: episode: 77, duration: 0.002s, episode steps: 9, steps per second: 4818, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.155 [-1.326, 2.351], mean_best_reward: --
  1257/100000: episode: 78, duration: 0.002s, episode steps: 8, steps per second: 4750, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.615, 2.517], mean_best_reward: --
  1267/100000: episode: 79, duration: 0.002s, episode steps: 10, steps per second: 5029, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.942, 2.994], mean_best_reward: --
  1282/100000: episode: 80, duration: 0.003s, episode steps: 15, steps per second: 5347, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.076 [-2.790, 1.803], mean_best_reward: --
  1292/100000: episode: 81, duration: 0.002s, episode steps: 10, steps per second: 4994, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.933, 3.034], mean_best_reward: --
  1318/100000: episode: 82, duration: 0.004s, episode steps: 26, steps per second: 5857, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.067 [-0.644, 1.097], mean_best_reward: --
  1339/100000: episode: 83, duration: 0.004s, episode steps: 21, steps per second: 5729, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.078 [-2.422, 1.342], mean_best_reward: --
  1355/100000: episode: 84, duration: 0.003s, episode steps: 16, steps per second: 5258, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.116 [-0.958, 1.781], mean_best_reward: --
  1365/100000: episode: 85, duration: 0.003s, episode steps: 10, steps per second: 3762, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.154 [-1.519, 2.600], mean_best_reward: --
  1374/100000: episode: 86, duration: 0.002s, episode steps: 9, steps per second: 4880, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.148 [-1.351, 2.259], mean_best_reward: --
  1386/100000: episode: 87, duration: 0.002s, episode steps: 12, steps per second: 5218, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.110 [-1.441, 0.835], mean_best_reward: --
  1406/100000: episode: 88, duration: 0.004s, episode steps: 20, steps per second: 5602, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.044 [-2.539, 1.606], mean_best_reward: --
  1427/100000: episode: 89, duration: 0.004s, episode steps: 21, steps per second: 5662, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.078 [-2.519, 1.417], mean_best_reward: --
  1443/100000: episode: 90, duration: 0.003s, episode steps: 16, steps per second: 5026, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.091 [-1.361, 0.800], mean_best_reward: --
  1472/100000: episode: 91, duration: 0.005s, episode steps: 29, steps per second: 5894, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.073 [-1.207, 0.767], mean_best_reward: --
  1482/100000: episode: 92, duration: 0.002s, episode steps: 10, steps per second: 4934, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.121 [-2.466, 1.607], mean_best_reward: --
  1499/100000: episode: 93, duration: 0.003s, episode steps: 17, steps per second: 5446, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.176 [0.000, 1.000], mean observation: 0.066 [-2.157, 3.183], mean_best_reward: --
  1524/100000: episode: 94, duration: 0.004s, episode steps: 25, steps per second: 5771, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.032 [-2.049, 1.326], mean_best_reward: --
  1535/100000: episode: 95, duration: 0.002s, episode steps: 11, steps per second: 5118, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.119 [-1.712, 2.759], mean_best_reward: --
  1568/100000: episode: 96, duration: 0.006s, episode steps: 33, steps per second: 5784, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: 0.011 [-1.597, 2.439], mean_best_reward: --
  1581/100000: episode: 97, duration: 0.002s, episode steps: 13, steps per second: 5273, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.098 [-2.910, 1.912], mean_best_reward: --
  1606/100000: episode: 98, duration: 0.006s, episode steps: 25, steps per second: 4297, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.077 [-1.016, 0.628], mean_best_reward: --
  1616/100000: episode: 99, duration: 0.002s, episode steps: 10, steps per second: 4743, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.147 [-2.601, 1.560], mean_best_reward: --
  1627/100000: episode: 100, duration: 0.002s, episode steps: 11, steps per second: 5077, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.103 [-1.759, 2.678], mean_best_reward: --
  1636/100000: episode: 101, duration: 0.002s, episode steps: 9, steps per second: 4789, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.840, 1.731], mean_best_reward: --
  1650/100000: episode: 102, duration: 0.003s, episode steps: 14, steps per second: 5092, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.113 [-1.140, 2.029], mean_best_reward: --
  1662/100000: episode: 103, duration: 0.002s, episode steps: 12, steps per second: 5132, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.105 [-2.577, 1.571], mean_best_reward: --
  1675/100000: episode: 104, duration: 0.003s, episode steps: 13, steps per second: 4881, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.081 [-1.653, 1.029], mean_best_reward: --
  1688/100000: episode: 105, duration: 0.003s, episode steps: 13, steps per second: 5150, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.094 [-0.606, 1.119], mean_best_reward: --
  1705/100000: episode: 106, duration: 0.003s, episode steps: 17, steps per second: 5270, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.077 [-1.602, 0.963], mean_best_reward: --
  1730/100000: episode: 107, duration: 0.004s, episode steps: 25, steps per second: 5813, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.056 [-1.377, 0.962], mean_best_reward: --
  1745/100000: episode: 108, duration: 0.003s, episode steps: 15, steps per second: 5153, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.075 [-2.807, 1.801], mean_best_reward: --
  1779/100000: episode: 109, duration: 0.006s, episode steps: 34, steps per second: 5941, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.676 [0.000, 1.000], mean observation: 0.047 [-3.022, 2.350], mean_best_reward: --
  1805/100000: episode: 110, duration: 0.004s, episode steps: 26, steps per second: 5851, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.426, 0.841], mean_best_reward: --
  1814/100000: episode: 111, duration: 0.002s, episode steps: 9, steps per second: 4914, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.890, 1.807], mean_best_reward: --
  1824/100000: episode: 112, duration: 0.002s, episode steps: 10, steps per second: 5042, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.148 [-2.543, 1.520], mean_best_reward: --
  1847/100000: episode: 113, duration: 0.005s, episode steps: 23, steps per second: 4713, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.021 [-1.405, 2.134], mean_best_reward: --
  1858/100000: episode: 114, duration: 0.002s, episode steps: 11, steps per second: 4940, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.133 [-2.316, 1.353], mean_best_reward: --
  1929/100000: episode: 115, duration: 0.011s, episode steps: 71, steps per second: 6232, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.066 [-0.912, 0.586], mean_best_reward: --
  1940/100000: episode: 116, duration: 0.002s, episode steps: 11, steps per second: 4975, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.135 [-1.352, 2.303], mean_best_reward: --
  1949/100000: episode: 117, duration: 0.002s, episode steps: 9, steps per second: 4642, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.127 [-2.308, 1.408], mean_best_reward: --
  2000/100000: episode: 118, duration: 0.008s, episode steps: 51, steps per second: 6233, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.058 [-0.705, 0.988], mean_best_reward: --
  2013/100000: episode: 119, duration: 0.002s, episode steps: 13, steps per second: 5318, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.121 [-1.332, 2.374], mean_best_reward: --
  2045/100000: episode: 120, duration: 0.006s, episode steps: 32, steps per second: 5736, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-0.941, 0.618], mean_best_reward: --
  2092/100000: episode: 121, duration: 0.008s, episode steps: 47, steps per second: 6131, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.617 [0.000, 1.000], mean observation: -0.010 [-2.536, 2.069], mean_best_reward: --
  2111/100000: episode: 122, duration: 0.004s, episode steps: 19, steps per second: 5418, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.120 [-1.627, 0.648], mean_best_reward: --
  2133/100000: episode: 123, duration: 0.005s, episode steps: 22, steps per second: 4298, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.050 [-2.533, 1.585], mean_best_reward: --
  2145/100000: episode: 124, duration: 0.002s, episode steps: 12, steps per second: 5215, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.101 [-0.839, 1.425], mean_best_reward: --
  2155/100000: episode: 125, duration: 0.002s, episode steps: 10, steps per second: 4973, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.146 [-1.566, 2.590], mean_best_reward: --
  2171/100000: episode: 126, duration: 0.003s, episode steps: 16, steps per second: 5465, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.087 [-1.161, 1.943], mean_best_reward: --
  2191/100000: episode: 127, duration: 0.004s, episode steps: 20, steps per second: 5632, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.044 [-1.379, 0.825], mean_best_reward: --
  2201/100000: episode: 128, duration: 0.002s, episode steps: 10, steps per second: 4919, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.128 [-2.237, 1.408], mean_best_reward: --
  2214/100000: episode: 129, duration: 0.002s, episode steps: 13, steps per second: 5282, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.102 [-1.732, 0.987], mean_best_reward: --
  2226/100000: episode: 130, duration: 0.002s, episode steps: 12, steps per second: 5205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-0.978, 1.751], mean_best_reward: --
  2237/100000: episode: 131, duration: 0.002s, episode steps: 11, steps per second: 5158, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.128 [-1.377, 2.286], mean_best_reward: --
  2249/100000: episode: 132, duration: 0.002s, episode steps: 12, steps per second: 5241, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.117 [-2.012, 1.174], mean_best_reward: --
  2283/100000: episode: 133, duration: 0.006s, episode steps: 34, steps per second: 5986, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.091 [-0.767, 1.885], mean_best_reward: --
  2295/100000: episode: 134, duration: 0.002s, episode steps: 12, steps per second: 5191, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.110 [-1.132, 1.840], mean_best_reward: --
  2312/100000: episode: 135, duration: 0.003s, episode steps: 17, steps per second: 5531, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.105 [-2.426, 1.391], mean_best_reward: --
  2335/100000: episode: 136, duration: 0.004s, episode steps: 23, steps per second: 5778, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.015 [-1.903, 2.551], mean_best_reward: --
  2347/100000: episode: 137, duration: 0.002s, episode steps: 12, steps per second: 5217, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.111 [-1.573, 2.505], mean_best_reward: --
  2359/100000: episode: 138, duration: 0.003s, episode steps: 12, steps per second: 3670, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.075 [-1.020, 1.568], mean_best_reward: --
  2374/100000: episode: 139, duration: 0.003s, episode steps: 15, steps per second: 5327, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.100 [-2.365, 1.409], mean_best_reward: --
  2386/100000: episode: 140, duration: 0.002s, episode steps: 12, steps per second: 5154, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.116 [-2.143, 1.328], mean_best_reward: --
  2403/100000: episode: 141, duration: 0.003s, episode steps: 17, steps per second: 5448, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.067 [-2.292, 1.410], mean_best_reward: --
  2421/100000: episode: 142, duration: 0.003s, episode steps: 18, steps per second: 5572, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.073 [-1.432, 0.926], mean_best_reward: --
  2451/100000: episode: 143, duration: 0.005s, episode steps: 30, steps per second: 5876, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.008 [-1.362, 1.836], mean_best_reward: --
  2477/100000: episode: 144, duration: 0.004s, episode steps: 26, steps per second: 5855, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.094 [-1.642, 0.751], mean_best_reward: --
  2488/100000: episode: 145, duration: 0.002s, episode steps: 11, steps per second: 5134, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.149 [-2.852, 1.731], mean_best_reward: --
  2499/100000: episode: 146, duration: 0.002s, episode steps: 11, steps per second: 4905, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.113 [-1.567, 2.421], mean_best_reward: --
  2516/100000: episode: 147, duration: 0.003s, episode steps: 17, steps per second: 5563, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.067 [-1.603, 2.437], mean_best_reward: --
  2534/100000: episode: 148, duration: 0.003s, episode steps: 18, steps per second: 5321, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.066 [-1.201, 1.955], mean_best_reward: --
  2582/100000: episode: 149, duration: 0.008s, episode steps: 48, steps per second: 5955, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.025 [-1.293, 1.136], mean_best_reward: --
  2592/100000: episode: 150, duration: 0.002s, episode steps: 10, steps per second: 4946, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.123 [-2.022, 1.145], mean_best_reward: --
  2605/100000: episode: 151, duration: 0.004s, episode steps: 13, steps per second: 3376, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.114 [-1.801, 1.008], mean_best_reward: 69.000000
  2618/100000: episode: 152, duration: 0.003s, episode steps: 13, steps per second: 4978, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.103 [-1.338, 0.788], mean_best_reward: --
  2636/100000: episode: 153, duration: 0.003s, episode steps: 18, steps per second: 5488, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.055 [-1.160, 1.712], mean_best_reward: --
  2661/100000: episode: 154, duration: 0.004s, episode steps: 25, steps per second: 5701, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.058 [-2.210, 1.362], mean_best_reward: --
  2694/100000: episode: 155, duration: 0.006s, episode steps: 33, steps per second: 5899, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: 0.048 [-1.189, 1.964], mean_best_reward: --
  2708/100000: episode: 156, duration: 0.003s, episode steps: 14, steps per second: 5332, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.120 [-2.175, 1.214], mean_best_reward: --
  2718/100000: episode: 157, duration: 0.002s, episode steps: 10, steps per second: 5016, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-3.097, 1.928], mean_best_reward: --
  2728/100000: episode: 158, duration: 0.002s, episode steps: 10, steps per second: 5023, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.116 [-1.941, 1.226], mean_best_reward: --
  2743/100000: episode: 159, duration: 0.003s, episode steps: 15, steps per second: 5435, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.115 [-1.898, 1.001], mean_best_reward: --
  2803/100000: episode: 160, duration: 0.010s, episode steps: 60, steps per second: 5793, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.122 [-0.629, 1.367], mean_best_reward: --
  2815/100000: episode: 161, duration: 0.002s, episode steps: 12, steps per second: 4902, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.096 [-1.616, 1.019], mean_best_reward: --
  2828/100000: episode: 162, duration: 0.002s, episode steps: 13, steps per second: 5257, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.116 [-1.336, 2.288], mean_best_reward: --
  2839/100000: episode: 163, duration: 0.002s, episode steps: 11, steps per second: 5064, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.471, 1.552], mean_best_reward: --
  2849/100000: episode: 164, duration: 0.002s, episode steps: 10, steps per second: 4699, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.139 [-1.779, 2.761], mean_best_reward: --
  2864/100000: episode: 165, duration: 0.004s, episode steps: 15, steps per second: 3596, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.096 [-2.728, 1.726], mean_best_reward: --
  2874/100000: episode: 166, duration: 0.002s, episode steps: 10, steps per second: 5000, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.114 [-2.654, 1.785], mean_best_reward: --
  2889/100000: episode: 167, duration: 0.003s, episode steps: 15, steps per second: 5319, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.099 [-1.940, 1.011], mean_best_reward: --
  2913/100000: episode: 168, duration: 0.004s, episode steps: 24, steps per second: 5342, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.059 [-1.371, 0.938], mean_best_reward: --
  2923/100000: episode: 169, duration: 0.002s, episode steps: 10, steps per second: 4776, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.113 [-1.011, 1.535], mean_best_reward: --
  2938/100000: episode: 170, duration: 0.003s, episode steps: 15, steps per second: 5200, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.094 [-1.309, 0.773], mean_best_reward: --
  2951/100000: episode: 171, duration: 0.002s, episode steps: 13, steps per second: 5239, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.091 [-1.357, 2.183], mean_best_reward: --
  2968/100000: episode: 172, duration: 0.003s, episode steps: 17, steps per second: 5531, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.055 [-1.723, 1.005], mean_best_reward: --
  2985/100000: episode: 173, duration: 0.003s, episode steps: 17, steps per second: 5523, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.091 [-2.435, 1.430], mean_best_reward: --
  2998/100000: episode: 174, duration: 0.002s, episode steps: 13, steps per second: 5206, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.098 [-1.785, 1.018], mean_best_reward: --
  3011/100000: episode: 175, duration: 0.003s, episode steps: 13, steps per second: 4655, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.105 [-1.616, 0.969], mean_best_reward: --
  3021/100000: episode: 176, duration: 0.002s, episode steps: 10, steps per second: 4961, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-3.111, 1.992], mean_best_reward: --
  3083/100000: episode: 177, duration: 0.010s, episode steps: 62, steps per second: 6207, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.049 [-1.779, 1.306], mean_best_reward: --
  3094/100000: episode: 178, duration: 0.004s, episode steps: 11, steps per second: 2880, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.104 [-1.010, 1.736], mean_best_reward: --
  3109/100000: episode: 179, duration: 0.003s, episode steps: 15, steps per second: 5050, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.101 [-1.814, 1.012], mean_best_reward: --
  3121/100000: episode: 180, duration: 0.002s, episode steps: 12, steps per second: 5132, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.104 [-2.537, 1.547], mean_best_reward: --
  3134/100000: episode: 181, duration: 0.002s, episode steps: 13, steps per second: 5259, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.099 [-2.243, 1.380], mean_best_reward: --
  3148/100000: episode: 182, duration: 0.003s, episode steps: 14, steps per second: 5174, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.094 [-2.169, 1.385], mean_best_reward: --
  3158/100000: episode: 183, duration: 0.002s, episode steps: 10, steps per second: 4963, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.122 [-2.534, 1.598], mean_best_reward: --
  3169/100000: episode: 184, duration: 0.002s, episode steps: 11, steps per second: 4568, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.100 [-1.803, 2.753], mean_best_reward: --
  3182/100000: episode: 185, duration: 0.002s, episode steps: 13, steps per second: 5250, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.077 [0.000, 1.000], mean observation: 0.105 [-2.141, 3.291], mean_best_reward: --
  3191/100000: episode: 186, duration: 0.002s, episode steps: 9, steps per second: 4862, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.798, 1.768], mean_best_reward: --
  3205/100000: episode: 187, duration: 0.003s, episode steps: 14, steps per second: 5349, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.555, 1.105], mean_best_reward: --
  3227/100000: episode: 188, duration: 0.004s, episode steps: 22, steps per second: 5625, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.096 [-2.663, 1.552], mean_best_reward: --
  3242/100000: episode: 189, duration: 0.003s, episode steps: 15, steps per second: 5228, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.110 [-1.940, 1.138], mean_best_reward: --
  3258/100000: episode: 190, duration: 0.003s, episode steps: 16, steps per second: 5381, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.096 [-2.542, 1.573], mean_best_reward: --
  3274/100000: episode: 191, duration: 0.003s, episode steps: 16, steps per second: 5470, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.059 [-1.906, 1.178], mean_best_reward: --
  3284/100000: episode: 192, duration: 0.002s, episode steps: 10, steps per second: 5019, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.112 [-1.016, 1.556], mean_best_reward: --
  3298/100000: episode: 193, duration: 0.003s, episode steps: 14, steps per second: 5372, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.108 [-2.068, 1.222], mean_best_reward: --
  3312/100000: episode: 194, duration: 0.003s, episode steps: 14, steps per second: 5330, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.083 [-2.220, 1.365], mean_best_reward: --
  3346/100000: episode: 195, duration: 0.008s, episode steps: 34, steps per second: 4342, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.007 [-2.747, 2.109], mean_best_reward: --
  3362/100000: episode: 196, duration: 0.003s, episode steps: 16, steps per second: 5387, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.102 [-0.791, 1.203], mean_best_reward: --
  3400/100000: episode: 197, duration: 0.006s, episode steps: 38, steps per second: 5988, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.152 [-0.932, 0.585], mean_best_reward: --
  3429/100000: episode: 198, duration: 0.005s, episode steps: 29, steps per second: 5695, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.027 [-1.034, 0.777], mean_best_reward: --
  3438/100000: episode: 199, duration: 0.002s, episode steps: 9, steps per second: 4797, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.129 [-1.224, 1.994], mean_best_reward: --
  3456/100000: episode: 200, duration: 0.003s, episode steps: 18, steps per second: 5534, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.068 [-0.622, 1.078], mean_best_reward: --
  3470/100000: episode: 201, duration: 0.003s, episode steps: 14, steps per second: 4828, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.122 [-2.591, 1.566], mean_best_reward: 77.500000
  3482/100000: episode: 202, duration: 0.002s, episode steps: 12, steps per second: 5163, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.115 [-1.954, 3.002], mean_best_reward: --
  3493/100000: episode: 203, duration: 0.002s, episode steps: 11, steps per second: 4871, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.109 [-1.411, 2.282], mean_best_reward: --
  3511/100000: episode: 204, duration: 0.003s, episode steps: 18, steps per second: 5549, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.113 [-2.722, 1.568], mean_best_reward: --
  3522/100000: episode: 205, duration: 0.002s, episode steps: 11, steps per second: 5089, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.115 [-1.345, 2.250], mean_best_reward: --
  3541/100000: episode: 206, duration: 0.003s, episode steps: 19, steps per second: 5608, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.072 [-1.435, 0.778], mean_best_reward: --
  3555/100000: episode: 207, duration: 0.003s, episode steps: 14, steps per second: 5317, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.091 [-1.034, 1.837], mean_best_reward: --
  3571/100000: episode: 208, duration: 0.005s, episode steps: 16, steps per second: 3450, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.065 [-1.889, 1.171], mean_best_reward: --
  3586/100000: episode: 209, duration: 0.003s, episode steps: 15, steps per second: 5227, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.096 [-1.516, 2.313], mean_best_reward: --
  3597/100000: episode: 210, duration: 0.002s, episode steps: 11, steps per second: 5122, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.110 [-1.023, 1.807], mean_best_reward: --
  3613/100000: episode: 211, duration: 0.003s, episode steps: 16, steps per second: 5499, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.094 [-1.206, 2.185], mean_best_reward: --
  3625/100000: episode: 212, duration: 0.002s, episode steps: 12, steps per second: 5128, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.135 [-1.931, 3.076], mean_best_reward: --
  3637/100000: episode: 213, duration: 0.003s, episode steps: 12, steps per second: 4695, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.100 [-1.471, 0.770], mean_best_reward: --
  3649/100000: episode: 214, duration: 0.002s, episode steps: 12, steps per second: 5061, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.112 [-1.574, 2.558], mean_best_reward: --
  3661/100000: episode: 215, duration: 0.002s, episode steps: 12, steps per second: 5194, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.118 [-1.204, 2.129], mean_best_reward: --
  3673/100000: episode: 216, duration: 0.002s, episode steps: 12, steps per second: 5218, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.107 [-1.541, 1.011], mean_best_reward: --
  3686/100000: episode: 217, duration: 0.002s, episode steps: 13, steps per second: 5204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.098 [-1.188, 1.915], mean_best_reward: --
  3715/100000: episode: 218, duration: 0.005s, episode steps: 29, steps per second: 5914, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.345 [0.000, 1.000], mean observation: 0.055 [-1.791, 2.969], mean_best_reward: --
  3727/100000: episode: 219, duration: 0.002s, episode steps: 12, steps per second: 4897, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.125 [-2.021, 1.166], mean_best_reward: --
  3746/100000: episode: 220, duration: 0.003s, episode steps: 19, steps per second: 5615, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.070 [-1.201, 0.738], mean_best_reward: --
  3755/100000: episode: 221, duration: 0.002s, episode steps: 9, steps per second: 4892, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.144 [-0.996, 1.786], mean_best_reward: --
  3774/100000: episode: 222, duration: 0.003s, episode steps: 19, steps per second: 5577, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.086 [-1.914, 1.148], mean_best_reward: --
  3787/100000: episode: 223, duration: 0.002s, episode steps: 13, steps per second: 5298, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.119 [-1.880, 1.024], mean_best_reward: --
  3871/100000: episode: 224, duration: 0.015s, episode steps: 84, steps per second: 5743, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.248 [-0.541, 1.610], mean_best_reward: --
  3887/100000: episode: 225, duration: 0.003s, episode steps: 16, steps per second: 5375, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.083 [-0.764, 1.442], mean_best_reward: --
  3897/100000: episode: 226, duration: 0.002s, episode steps: 10, steps per second: 4434, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.115 [-2.517, 1.593], mean_best_reward: --
  3911/100000: episode: 227, duration: 0.003s, episode steps: 14, steps per second: 5350, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.109 [-2.122, 1.177], mean_best_reward: --
  3922/100000: episode: 228, duration: 0.002s, episode steps: 11, steps per second: 5043, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.109 [-1.411, 2.319], mean_best_reward: --
  3936/100000: episode: 229, duration: 0.003s, episode steps: 14, steps per second: 5303, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.100 [-1.494, 0.970], mean_best_reward: --
  3997/100000: episode: 230, duration: 0.010s, episode steps: 61, steps per second: 6070, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.229 [-0.543, 1.322], mean_best_reward: --
  4007/100000: episode: 231, duration: 0.002s, episode steps: 10, steps per second: 4931, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.922, 2.980], mean_best_reward: --
  4018/100000: episode: 232, duration: 0.002s, episode steps: 11, steps per second: 4990, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.121 [-1.780, 2.783], mean_best_reward: --
  4040/100000: episode: 233, duration: 0.004s, episode steps: 22, steps per second: 5723, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.081 [-1.743, 0.959], mean_best_reward: --
  4051/100000: episode: 234, duration: 0.002s, episode steps: 11, steps per second: 5066, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.120 [-1.371, 2.246], mean_best_reward: --
  4063/100000: episode: 235, duration: 0.003s, episode steps: 12, steps per second: 4399, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.112 [-1.743, 1.203], mean_best_reward: --
  4073/100000: episode: 236, duration: 0.002s, episode steps: 10, steps per second: 4657, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.545, 2.494], mean_best_reward: --
  4097/100000: episode: 237, duration: 0.004s, episode steps: 24, steps per second: 5450, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.015 [-1.218, 1.746], mean_best_reward: --
  4107/100000: episode: 238, duration: 0.002s, episode steps: 10, steps per second: 4925, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.113 [-1.604, 2.563], mean_best_reward: --
  4120/100000: episode: 239, duration: 0.002s, episode steps: 13, steps per second: 5206, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.107 [-0.940, 1.683], mean_best_reward: --
  4134/100000: episode: 240, duration: 0.003s, episode steps: 14, steps per second: 4728, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.100 [-1.551, 0.829], mean_best_reward: --
  4148/100000: episode: 241, duration: 0.003s, episode steps: 14, steps per second: 5348, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.074 [-1.593, 1.026], mean_best_reward: --
  4157/100000: episode: 242, duration: 0.002s, episode steps: 9, steps per second: 4888, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.143 [-1.534, 2.436], mean_best_reward: --
  4169/100000: episode: 243, duration: 0.002s, episode steps: 12, steps per second: 5179, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.121 [-1.906, 3.006], mean_best_reward: --
  4179/100000: episode: 244, duration: 0.002s, episode steps: 10, steps per second: 5035, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.125 [-1.406, 2.167], mean_best_reward: --
  4196/100000: episode: 245, duration: 0.003s, episode steps: 17, steps per second: 5524, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.049 [-2.213, 1.415], mean_best_reward: --
  4211/100000: episode: 246, duration: 0.003s, episode steps: 15, steps per second: 5208, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.076 [-0.979, 1.596], mean_best_reward: --
  4224/100000: episode: 247, duration: 0.002s, episode steps: 13, steps per second: 5282, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.126 [-1.360, 2.334], mean_best_reward: --
  4233/100000: episode: 248, duration: 0.002s, episode steps: 9, steps per second: 4904, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.116 [-1.423, 2.313], mean_best_reward: --
  4257/100000: episode: 249, duration: 0.004s, episode steps: 24, steps per second: 5804, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.070 [-1.663, 0.820], mean_best_reward: --
  4289/100000: episode: 250, duration: 0.005s, episode steps: 32, steps per second: 5944, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.860, 1.007], mean_best_reward: --
  4301/100000: episode: 251, duration: 0.004s, episode steps: 12, steps per second: 3199, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.123 [-2.065, 1.171], mean_best_reward: 83.000000
  4321/100000: episode: 252, duration: 0.004s, episode steps: 20, steps per second: 5640, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.177, 0.586], mean_best_reward: --
  4333/100000: episode: 253, duration: 0.002s, episode steps: 12, steps per second: 5196, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.094 [-2.518, 1.581], mean_best_reward: --
  4347/100000: episode: 254, duration: 0.003s, episode steps: 14, steps per second: 5050, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.087 [-3.058, 1.996], mean_best_reward: --
  4359/100000: episode: 255, duration: 0.002s, episode steps: 12, steps per second: 5154, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.140 [-2.583, 1.539], mean_best_reward: --
  4379/100000: episode: 256, duration: 0.004s, episode steps: 20, steps per second: 5184, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.091 [-1.381, 0.777], mean_best_reward: --
  4389/100000: episode: 257, duration: 0.002s, episode steps: 10, steps per second: 4992, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.144 [-2.248, 1.331], mean_best_reward: --
  4399/100000: episode: 258, duration: 0.002s, episode steps: 10, steps per second: 4990, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.113 [-1.143, 1.867], mean_best_reward: --
  4410/100000: episode: 259, duration: 0.002s, episode steps: 11, steps per second: 5075, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.133 [-2.223, 1.341], mean_best_reward: --
  4424/100000: episode: 260, duration: 0.003s, episode steps: 14, steps per second: 5337, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.088 [-0.817, 1.228], mean_best_reward: --
  4445/100000: episode: 261, duration: 0.004s, episode steps: 21, steps per second: 5282, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.042 [-1.849, 1.210], mean_best_reward: --
  4458/100000: episode: 262, duration: 0.003s, episode steps: 13, steps per second: 5053, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.103 [-1.379, 2.267], mean_best_reward: --
  4471/100000: episode: 263, duration: 0.002s, episode steps: 13, steps per second: 5272, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.101 [-2.214, 1.369], mean_best_reward: --
  4480/100000: episode: 264, duration: 0.002s, episode steps: 9, steps per second: 4883, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.164 [-2.505, 1.537], mean_best_reward: --
  4491/100000: episode: 265, duration: 0.002s, episode steps: 11, steps per second: 5003, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.131 [-1.345, 2.263], mean_best_reward: --
  4516/100000: episode: 266, duration: 0.004s, episode steps: 25, steps per second: 5806, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: -0.045 [-2.599, 1.738], mean_best_reward: --
  4573/100000: episode: 267, duration: 0.011s, episode steps: 57, steps per second: 5283, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.068 [-0.465, 1.232], mean_best_reward: --
  4585/100000: episode: 268, duration: 0.002s, episode steps: 12, steps per second: 5126, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.117 [-1.540, 2.533], mean_best_reward: --
  4595/100000: episode: 269, duration: 0.002s, episode steps: 10, steps per second: 4900, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.135 [-2.394, 1.586], mean_best_reward: --
  4608/100000: episode: 270, duration: 0.003s, episode steps: 13, steps per second: 5170, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.083 [-0.803, 1.373], mean_best_reward: --
  4619/100000: episode: 271, duration: 0.002s, episode steps: 11, steps per second: 4987, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.115 [-1.391, 2.224], mean_best_reward: --
  4640/100000: episode: 272, duration: 0.004s, episode steps: 21, steps per second: 5480, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.044 [-1.014, 1.626], mean_best_reward: --
  4650/100000: episode: 273, duration: 0.002s, episode steps: 10, steps per second: 5016, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.139 [-2.045, 1.167], mean_best_reward: --
  4663/100000: episode: 274, duration: 0.002s, episode steps: 13, steps per second: 5299, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.096 [-1.413, 2.386], mean_best_reward: --
  4675/100000: episode: 275, duration: 0.002s, episode steps: 12, steps per second: 5197, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.113 [-2.546, 1.589], mean_best_reward: --
  4685/100000: episode: 276, duration: 0.002s, episode steps: 10, steps per second: 4346, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.611, 1.553], mean_best_reward: --
  4698/100000: episode: 277, duration: 0.003s, episode steps: 13, steps per second: 5117, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.116 [-0.776, 1.453], mean_best_reward: --
  4713/100000: episode: 278, duration: 0.003s, episode steps: 15, steps per second: 5390, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.106 [-1.339, 0.559], mean_best_reward: --
  4734/100000: episode: 279, duration: 0.004s, episode steps: 21, steps per second: 5696, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.046 [-1.407, 2.149], mean_best_reward: --
  4743/100000: episode: 280, duration: 0.002s, episode steps: 9, steps per second: 4899, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.713, 2.804], mean_best_reward: --
  4758/100000: episode: 281, duration: 0.003s, episode steps: 15, steps per second: 4855, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.081 [-0.813, 1.425], mean_best_reward: --
  4774/100000: episode: 282, duration: 0.005s, episode steps: 16, steps per second: 3374, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.114 [-2.084, 1.173], mean_best_reward: --
  4786/100000: episode: 283, duration: 0.002s, episode steps: 12, steps per second: 5122, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.099 [-0.974, 1.624], mean_best_reward: --
  4795/100000: episode: 284, duration: 0.002s, episode steps: 9, steps per second: 4772, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.110 [-2.249, 1.414], mean_best_reward: --
  4805/100000: episode: 285, duration: 0.002s, episode steps: 10, steps per second: 4955, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.133 [-1.783, 2.706], mean_best_reward: --
  4816/100000: episode: 286, duration: 0.002s, episode steps: 11, steps per second: 5094, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.122 [-1.579, 2.484], mean_best_reward: --
  4836/100000: episode: 287, duration: 0.004s, episode steps: 20, steps per second: 5163, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.047 [-1.998, 3.061], mean_best_reward: --
  4861/100000: episode: 288, duration: 0.004s, episode steps: 25, steps per second: 5802, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.066 [-2.241, 1.329], mean_best_reward: --
  4889/100000: episode: 289, duration: 0.005s, episode steps: 28, steps per second: 5733, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.077 [-0.634, 1.537], mean_best_reward: --
  4902/100000: episode: 290, duration: 0.002s, episode steps: 13, steps per second: 5293, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.110 [-1.518, 2.383], mean_best_reward: --
  4917/100000: episode: 291, duration: 0.003s, episode steps: 15, steps per second: 4930, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.108 [-0.795, 1.421], mean_best_reward: --
  4945/100000: episode: 292, duration: 0.005s, episode steps: 28, steps per second: 5800, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: -0.052 [-2.139, 1.360], mean_best_reward: --
  4957/100000: episode: 293, duration: 0.002s, episode steps: 12, steps per second: 5204, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.107 [-1.570, 2.525], mean_best_reward: --
  4971/100000: episode: 294, duration: 0.003s, episode steps: 14, steps per second: 5384, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.107 [-2.642, 1.598], mean_best_reward: --
  4983/100000: episode: 295, duration: 0.002s, episode steps: 12, steps per second: 5048, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.129 [-1.599, 2.563], mean_best_reward: --
  4993/100000: episode: 296, duration: 0.002s, episode steps: 10, steps per second: 4819, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.121 [-2.032, 1.176], mean_best_reward: --
  5005/100000: episode: 297, duration: 0.004s, episode steps: 12, steps per second: 3107, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.084 [-1.378, 2.043], mean_best_reward: --
  5037/100000: episode: 298, duration: 0.005s, episode steps: 32, steps per second: 5880, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.063 [-1.016, 0.418], mean_best_reward: --
  5049/100000: episode: 299, duration: 0.002s, episode steps: 12, steps per second: 5150, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.082 [-1.562, 0.989], mean_best_reward: --
  5133/100000: episode: 300, duration: 0.013s, episode steps: 84, steps per second: 6236, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.054 [-1.178, 2.172], mean_best_reward: --
  5170/100000: episode: 301, duration: 0.006s, episode steps: 37, steps per second: 5705, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.072 [-0.930, 0.632], mean_best_reward: 59.000000
  5208/100000: episode: 302, duration: 0.006s, episode steps: 38, steps per second: 5949, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.121 [-1.224, 0.579], mean_best_reward: --
  5230/100000: episode: 303, duration: 0.004s, episode steps: 22, steps per second: 5689, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.078 [-0.829, 1.663], mean_best_reward: --
  5242/100000: episode: 304, duration: 0.002s, episode steps: 12, steps per second: 5197, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.141 [-2.614, 1.531], mean_best_reward: --
  5256/100000: episode: 305, duration: 0.003s, episode steps: 14, steps per second: 5304, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.079 [-1.718, 1.154], mean_best_reward: --
  5267/100000: episode: 306, duration: 0.003s, episode steps: 11, steps per second: 3997, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.256, 1.331], mean_best_reward: --
  5281/100000: episode: 307, duration: 0.003s, episode steps: 14, steps per second: 4780, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.127 [-1.236, 0.735], mean_best_reward: --
  5295/100000: episode: 308, duration: 0.003s, episode steps: 14, steps per second: 5268, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.100 [-2.457, 1.518], mean_best_reward: --
  5305/100000: episode: 309, duration: 0.002s, episode steps: 10, steps per second: 4951, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.101 [-1.945, 1.183], mean_best_reward: --
  5328/100000: episode: 310, duration: 0.004s, episode steps: 23, steps per second: 5737, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.083 [-1.185, 0.564], mean_best_reward: --
  5357/100000: episode: 311, duration: 0.005s, episode steps: 29, steps per second: 5554, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.031 [-1.266, 0.939], mean_best_reward: --
  5370/100000: episode: 312, duration: 0.002s, episode steps: 13, steps per second: 5277, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.093 [-2.763, 1.779], mean_best_reward: --
  5390/100000: episode: 313, duration: 0.004s, episode steps: 20, steps per second: 5657, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.080 [-1.652, 0.836], mean_best_reward: --
  5403/100000: episode: 314, duration: 0.002s, episode steps: 13, steps per second: 5287, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.099 [-1.273, 0.782], mean_best_reward: --
  5424/100000: episode: 315, duration: 0.004s, episode steps: 21, steps per second: 5244, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.060 [-2.436, 1.539], mean_best_reward: --
  5438/100000: episode: 316, duration: 0.003s, episode steps: 14, steps per second: 5247, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.117 [-2.575, 1.555], mean_best_reward: --
  5460/100000: episode: 317, duration: 0.004s, episode steps: 22, steps per second: 5730, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.114, 0.589], mean_best_reward: --
  5471/100000: episode: 318, duration: 0.002s, episode steps: 11, steps per second: 5122, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.093 [-2.768, 1.810], mean_best_reward: --
  5480/100000: episode: 319, duration: 0.002s, episode steps: 9, steps per second: 4878, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.126 [-2.817, 1.807], mean_best_reward: --
  5492/100000: episode: 320, duration: 0.002s, episode steps: 12, steps per second: 5118, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.118 [-1.878, 1.149], mean_best_reward: --
  5502/100000: episode: 321, duration: 0.003s, episode steps: 10, steps per second: 3898, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.124 [-2.549, 1.590], mean_best_reward: --
  5520/100000: episode: 322, duration: 0.004s, episode steps: 18, steps per second: 4528, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.092 [-2.209, 1.320], mean_best_reward: --
  5535/100000: episode: 323, duration: 0.003s, episode steps: 15, steps per second: 5448, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.888, 1.723], mean_best_reward: --
  5548/100000: episode: 324, duration: 0.002s, episode steps: 13, steps per second: 5300, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.082 [-2.748, 1.806], mean_best_reward: --
  5571/100000: episode: 325, duration: 0.004s, episode steps: 23, steps per second: 5776, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.069 [-1.435, 0.971], mean_best_reward: --
  5583/100000: episode: 326, duration: 0.003s, episode steps: 12, steps per second: 4640, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.090 [-2.445, 1.589], mean_best_reward: --
  5607/100000: episode: 327, duration: 0.004s, episode steps: 24, steps per second: 5778, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.081 [-1.246, 0.748], mean_best_reward: --
  5624/100000: episode: 328, duration: 0.003s, episode steps: 17, steps per second: 5522, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.090 [-1.751, 0.935], mean_best_reward: --
  5649/100000: episode: 329, duration: 0.004s, episode steps: 25, steps per second: 5832, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.280 [0.000, 1.000], mean observation: 0.015 [-2.116, 3.015], mean_best_reward: --
  5668/100000: episode: 330, duration: 0.004s, episode steps: 19, steps per second: 4988, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.084 [-0.943, 1.760], mean_best_reward: --
  5680/100000: episode: 331, duration: 0.002s, episode steps: 12, steps per second: 5171, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-1.023, 1.703], mean_best_reward: --
  5690/100000: episode: 332, duration: 0.002s, episode steps: 10, steps per second: 5019, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.993, 1.904], mean_best_reward: --
  5705/100000: episode: 333, duration: 0.003s, episode steps: 15, steps per second: 5404, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.089 [-1.873, 1.036], mean_best_reward: --
  5725/100000: episode: 334, duration: 0.004s, episode steps: 20, steps per second: 5673, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.094 [-2.656, 1.564], mean_best_reward: --
  5742/100000: episode: 335, duration: 0.004s, episode steps: 17, steps per second: 4823, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.118 [-0.579, 1.251], mean_best_reward: --
  5757/100000: episode: 336, duration: 0.003s, episode steps: 15, steps per second: 4600, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.105 [-1.207, 0.570], mean_best_reward: --
  5767/100000: episode: 337, duration: 0.002s, episode steps: 10, steps per second: 5023, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.120 [-1.968, 1.163], mean_best_reward: --
  5781/100000: episode: 338, duration: 0.003s, episode steps: 14, steps per second: 5300, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.088 [-1.679, 0.992], mean_best_reward: --
  5795/100000: episode: 339, duration: 0.003s, episode steps: 14, steps per second: 5331, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.122 [-1.785, 0.965], mean_best_reward: --
  5812/100000: episode: 340, duration: 0.003s, episode steps: 17, steps per second: 5534, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.090 [-1.327, 0.584], mean_best_reward: --
  5841/100000: episode: 341, duration: 0.005s, episode steps: 29, steps per second: 5680, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.023 [-1.247, 0.953], mean_best_reward: --
  5850/100000: episode: 342, duration: 0.002s, episode steps: 9, steps per second: 4887, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.115 [-1.915, 1.225], mean_best_reward: --
  5885/100000: episode: 343, duration: 0.006s, episode steps: 35, steps per second: 6001, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.686 [0.000, 1.000], mean observation: 0.103 [-2.958, 2.685], mean_best_reward: --
  5899/100000: episode: 344, duration: 0.003s, episode steps: 14, steps per second: 5242, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.092 [-2.606, 1.597], mean_best_reward: --
  5915/100000: episode: 345, duration: 0.003s, episode steps: 16, steps per second: 5210, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.108 [-1.212, 2.246], mean_best_reward: --
  5923/100000: episode: 346, duration: 0.002s, episode steps: 8, steps per second: 4749, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.530, 1.555], mean_best_reward: --
  5938/100000: episode: 347, duration: 0.003s, episode steps: 15, steps per second: 5466, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.086 [-1.741, 1.131], mean_best_reward: --
  5950/100000: episode: 348, duration: 0.002s, episode steps: 12, steps per second: 5218, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.138 [-2.619, 1.535], mean_best_reward: --
  5961/100000: episode: 349, duration: 0.002s, episode steps: 11, steps per second: 5155, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.110 [-1.364, 2.297], mean_best_reward: --
  5975/100000: episode: 350, duration: 0.003s, episode steps: 14, steps per second: 5351, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.069 [-1.213, 1.867], mean_best_reward: --
  5997/100000: episode: 351, duration: 0.005s, episode steps: 22, steps per second: 4721, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.089 [-0.798, 1.602], mean_best_reward: 42.500000
  6013/100000: episode: 352, duration: 0.003s, episode steps: 16, steps per second: 4876, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.076 [-0.830, 1.318], mean_best_reward: --
  6028/100000: episode: 353, duration: 0.003s, episode steps: 15, steps per second: 5417, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.080 [-2.169, 1.321], mean_best_reward: --
  6041/100000: episode: 354, duration: 0.002s, episode steps: 13, steps per second: 5338, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.133 [-2.407, 1.329], mean_best_reward: --
  6056/100000: episode: 355, duration: 0.003s, episode steps: 15, steps per second: 5301, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.078 [-2.679, 1.711], mean_best_reward: --
  6103/100000: episode: 356, duration: 0.008s, episode steps: 47, steps per second: 6102, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.072 [-0.550, 1.057], mean_best_reward: --
  6114/100000: episode: 357, duration: 0.002s, episode steps: 11, steps per second: 5063, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.124 [-1.891, 1.196], mean_best_reward: --
  6126/100000: episode: 358, duration: 0.002s, episode steps: 12, steps per second: 5212, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.115 [-1.786, 0.967], mean_best_reward: --
  6142/100000: episode: 359, duration: 0.003s, episode steps: 16, steps per second: 5473, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.091 [-0.795, 1.535], mean_best_reward: --
  6170/100000: episode: 360, duration: 0.005s, episode steps: 28, steps per second: 5686, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: 0.024 [-1.178, 1.997], mean_best_reward: --
  6197/100000: episode: 361, duration: 0.005s, episode steps: 27, steps per second: 5857, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.070 [-1.173, 0.735], mean_best_reward: --
  6206/100000: episode: 362, duration: 0.002s, episode steps: 9, steps per second: 4912, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.142 [-2.187, 1.338], mean_best_reward: --
  6219/100000: episode: 363, duration: 0.002s, episode steps: 13, steps per second: 5298, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.100 [-2.251, 1.360], mean_best_reward: --
  6229/100000: episode: 364, duration: 0.002s, episode steps: 10, steps per second: 4182, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.142 [-1.163, 2.036], mean_best_reward: --
  6240/100000: episode: 365, duration: 0.003s, episode steps: 11, steps per second: 3281, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.578, 2.486], mean_best_reward: --
  6256/100000: episode: 366, duration: 0.003s, episode steps: 16, steps per second: 5495, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.115 [-2.105, 1.145], mean_best_reward: --
  6278/100000: episode: 367, duration: 0.004s, episode steps: 22, steps per second: 5724, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.096 [-0.967, 1.883], mean_best_reward: --
  6292/100000: episode: 368, duration: 0.003s, episode steps: 14, steps per second: 5347, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.095 [-0.801, 1.574], mean_best_reward: --
  6305/100000: episode: 369, duration: 0.003s, episode steps: 13, steps per second: 4917, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.104 [-1.220, 0.813], mean_best_reward: --
  6333/100000: episode: 370, duration: 0.005s, episode steps: 28, steps per second: 5773, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: -0.046 [-2.049, 1.190], mean_best_reward: --
  6343/100000: episode: 371, duration: 0.002s, episode steps: 10, steps per second: 5036, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.108 [-0.986, 1.631], mean_best_reward: --
  6364/100000: episode: 372, duration: 0.004s, episode steps: 21, steps per second: 5724, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.064 [-1.236, 0.613], mean_best_reward: --
  6375/100000: episode: 373, duration: 0.002s, episode steps: 11, steps per second: 5031, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.100 [-1.851, 1.212], mean_best_reward: --
  6392/100000: episode: 374, duration: 0.003s, episode steps: 17, steps per second: 5124, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.103 [-0.552, 1.033], mean_best_reward: --
  6403/100000: episode: 375, duration: 0.002s, episode steps: 11, steps per second: 4838, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.128 [-2.807, 1.746], mean_best_reward: --
  6417/100000: episode: 376, duration: 0.003s, episode steps: 14, steps per second: 5282, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.095 [-2.431, 1.527], mean_best_reward: --
  6433/100000: episode: 377, duration: 0.003s, episode steps: 16, steps per second: 5515, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.061 [-1.937, 1.181], mean_best_reward: --
  6442/100000: episode: 378, duration: 0.002s, episode steps: 9, steps per second: 4911, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.127 [-2.317, 1.379], mean_best_reward: --
  6478/100000: episode: 379, duration: 0.007s, episode steps: 36, steps per second: 5393, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.057 [-0.905, 0.359], mean_best_reward: --
  6488/100000: episode: 380, duration: 0.003s, episode steps: 10, steps per second: 3656, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.149 [-1.341, 2.114], mean_best_reward: --
  6510/100000: episode: 381, duration: 0.004s, episode steps: 22, steps per second: 5719, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.070 [-2.466, 1.519], mean_best_reward: --
  6524/100000: episode: 382, duration: 0.003s, episode steps: 14, steps per second: 5368, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.638, 1.116], mean_best_reward: --
  6557/100000: episode: 383, duration: 0.006s, episode steps: 33, steps per second: 5910, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.048 [-0.804, 1.556], mean_best_reward: --
  6567/100000: episode: 384, duration: 0.002s, episode steps: 10, steps per second: 4994, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.130 [-1.229, 2.067], mean_best_reward: --
  6593/100000: episode: 385, duration: 0.004s, episode steps: 26, steps per second: 5844, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-0.986, 0.627], mean_best_reward: --
  6612/100000: episode: 386, duration: 0.003s, episode steps: 19, steps per second: 5649, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.071 [-2.379, 1.534], mean_best_reward: --
  6623/100000: episode: 387, duration: 0.002s, episode steps: 11, steps per second: 5148, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.121 [-1.965, 1.171], mean_best_reward: --
  6633/100000: episode: 388, duration: 0.002s, episode steps: 10, steps per second: 4561, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.111 [-1.955, 1.196], mean_best_reward: --
  6656/100000: episode: 389, duration: 0.004s, episode steps: 23, steps per second: 5753, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.058 [-1.769, 1.000], mean_best_reward: --
  6672/100000: episode: 390, duration: 0.003s, episode steps: 16, steps per second: 5523, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.064 [-2.113, 1.404], mean_best_reward: --
  6685/100000: episode: 391, duration: 0.002s, episode steps: 13, steps per second: 5319, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.120 [-2.235, 1.347], mean_best_reward: --
  6703/100000: episode: 392, duration: 0.003s, episode steps: 18, steps per second: 5544, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.085 [-2.032, 1.189], mean_best_reward: --
  6718/100000: episode: 393, duration: 0.003s, episode steps: 15, steps per second: 5167, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.105 [-1.977, 1.145], mean_best_reward: --
  6727/100000: episode: 394, duration: 0.003s, episode steps: 9, steps per second: 3049, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.752, 2.818], mean_best_reward: --
  6745/100000: episode: 395, duration: 0.003s, episode steps: 18, steps per second: 5491, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.703, 1.172], mean_best_reward: --
  6761/100000: episode: 396, duration: 0.003s, episode steps: 16, steps per second: 5463, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.098 [-1.573, 2.577], mean_best_reward: --
  6787/100000: episode: 397, duration: 0.005s, episode steps: 26, steps per second: 5762, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.032 [-1.568, 1.125], mean_best_reward: --
  6797/100000: episode: 398, duration: 0.002s, episode steps: 10, steps per second: 4862, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.133 [-2.594, 1.601], mean_best_reward: --
  6818/100000: episode: 399, duration: 0.004s, episode steps: 21, steps per second: 5692, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.045 [-1.517, 2.318], mean_best_reward: --
  6835/100000: episode: 400, duration: 0.003s, episode steps: 17, steps per second: 5567, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.042 [-1.804, 1.186], mean_best_reward: --
  6850/100000: episode: 401, duration: 0.003s, episode steps: 15, steps per second: 4934, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.103 [-1.755, 0.939], mean_best_reward: 72.500000
  6882/100000: episode: 402, duration: 0.005s, episode steps: 32, steps per second: 5908, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-1.025, 0.598], mean_best_reward: --
  6898/100000: episode: 403, duration: 0.003s, episode steps: 16, steps per second: 5482, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.084 [-1.408, 0.778], mean_best_reward: --
  6926/100000: episode: 404, duration: 0.005s, episode steps: 28, steps per second: 5920, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.572, 0.950], mean_best_reward: --
  6949/100000: episode: 405, duration: 0.004s, episode steps: 23, steps per second: 5792, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.018 [-1.014, 1.391], mean_best_reward: --
  6994/100000: episode: 406, duration: 0.008s, episode steps: 45, steps per second: 5591, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.422 [0.000, 1.000], mean observation: 0.059 [-1.411, 2.610], mean_best_reward: --
  7042/100000: episode: 407, duration: 0.008s, episode steps: 48, steps per second: 6166, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.066 [-0.926, 0.601], mean_best_reward: --
  7076/100000: episode: 408, duration: 0.006s, episode steps: 34, steps per second: 5967, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.004 [-1.663, 1.213], mean_best_reward: --
  7109/100000: episode: 409, duration: 0.005s, episode steps: 33, steps per second: 6020, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.041 [-0.813, 1.080], mean_best_reward: --
  7120/100000: episode: 410, duration: 0.002s, episode steps: 11, steps per second: 5156, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.127 [-1.365, 2.139], mean_best_reward: --
  7134/100000: episode: 411, duration: 0.003s, episode steps: 14, steps per second: 5375, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.091 [-2.013, 1.231], mean_best_reward: --
  7152/100000: episode: 412, duration: 0.004s, episode steps: 18, steps per second: 4683, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.063 [-2.597, 1.605], mean_best_reward: --
  7178/100000: episode: 413, duration: 0.004s, episode steps: 26, steps per second: 5840, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.476, 1.102], mean_best_reward: --
  7196/100000: episode: 414, duration: 0.003s, episode steps: 18, steps per second: 5561, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.044 [-1.024, 1.566], mean_best_reward: --
  7213/100000: episode: 415, duration: 0.003s, episode steps: 17, steps per second: 5554, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.064 [-0.826, 1.212], mean_best_reward: --
  7236/100000: episode: 416, duration: 0.004s, episode steps: 23, steps per second: 5274, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.101 [-1.060, 0.421], mean_best_reward: --
  7252/100000: episode: 417, duration: 0.004s, episode steps: 16, steps per second: 4306, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.097 [-1.520, 2.587], mean_best_reward: --
  7263/100000: episode: 418, duration: 0.002s, episode steps: 11, steps per second: 5114, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.135 [-1.904, 1.131], mean_best_reward: --
  7303/100000: episode: 419, duration: 0.007s, episode steps: 40, steps per second: 6034, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.069 [-1.936, 1.004], mean_best_reward: --
  7326/100000: episode: 420, duration: 0.004s, episode steps: 23, steps per second: 5245, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.085 [-0.983, 1.958], mean_best_reward: --
  7339/100000: episode: 421, duration: 0.002s, episode steps: 13, steps per second: 5243, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.128 [-2.535, 1.538], mean_best_reward: --
  7349/100000: episode: 422, duration: 0.002s, episode steps: 10, steps per second: 5096, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.153 [-2.604, 1.563], mean_best_reward: --
  7380/100000: episode: 423, duration: 0.005s, episode steps: 31, steps per second: 5970, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.019 [-1.024, 1.783], mean_best_reward: --
  7391/100000: episode: 424, duration: 0.002s, episode steps: 11, steps per second: 5176, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.140 [-2.872, 1.747], mean_best_reward: --
  7405/100000: episode: 425, duration: 0.003s, episode steps: 14, steps per second: 4754, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.076 [-1.204, 2.043], mean_best_reward: --
  7425/100000: episode: 426, duration: 0.003s, episode steps: 20, steps per second: 5716, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.083 [-0.821, 1.524], mean_best_reward: --
  7529/100000: episode: 427, duration: 0.018s, episode steps: 104, steps per second: 5858, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.037 [-1.888, 1.958], mean_best_reward: --
  7548/100000: episode: 428, duration: 0.003s, episode steps: 19, steps per second: 5485, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.051 [-2.664, 1.733], mean_best_reward: --
  7571/100000: episode: 429, duration: 0.004s, episode steps: 23, steps per second: 5696, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.092 [-1.429, 0.614], mean_best_reward: --
  7599/100000: episode: 430, duration: 0.005s, episode steps: 28, steps per second: 5644, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-1.019, 1.350], mean_best_reward: --
  7615/100000: episode: 431, duration: 0.003s, episode steps: 16, steps per second: 5463, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.101 [-1.568, 0.799], mean_best_reward: --
  7632/100000: episode: 432, duration: 0.003s, episode steps: 17, steps per second: 5544, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.084 [-2.418, 1.548], mean_best_reward: --
  7648/100000: episode: 433, duration: 0.003s, episode steps: 16, steps per second: 5488, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.099 [-1.839, 1.126], mean_best_reward: --
  7700/100000: episode: 434, duration: 0.009s, episode steps: 52, steps per second: 6114, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: 0.169 [-2.416, 2.635], mean_best_reward: --
  7710/100000: episode: 435, duration: 0.002s, episode steps: 10, steps per second: 4991, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.139 [-2.555, 1.606], mean_best_reward: --
  7725/100000: episode: 436, duration: 0.003s, episode steps: 15, steps per second: 5410, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.092 [-1.392, 0.820], mean_best_reward: --
  7741/100000: episode: 437, duration: 0.003s, episode steps: 16, steps per second: 5476, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.094 [-1.199, 0.551], mean_best_reward: --
  7780/100000: episode: 438, duration: 0.007s, episode steps: 39, steps per second: 5445, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.025 [-0.802, 1.087], mean_best_reward: --
  7792/100000: episode: 439, duration: 0.002s, episode steps: 12, steps per second: 5061, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.102 [-1.799, 1.158], mean_best_reward: --
  7801/100000: episode: 440, duration: 0.002s, episode steps: 9, steps per second: 4832, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.129 [-2.151, 1.392], mean_best_reward: --
  7831/100000: episode: 441, duration: 0.005s, episode steps: 30, steps per second: 5686, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.561, 1.158], mean_best_reward: --
  7856/100000: episode: 442, duration: 0.005s, episode steps: 25, steps per second: 5472, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.070 [-1.110, 0.603], mean_best_reward: --
  7906/100000: episode: 443, duration: 0.008s, episode steps: 50, steps per second: 6159, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.111 [-0.938, 0.541], mean_best_reward: --
  7926/100000: episode: 444, duration: 0.004s, episode steps: 20, steps per second: 5657, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.056 [-2.960, 1.955], mean_best_reward: --
  7938/100000: episode: 445, duration: 0.002s, episode steps: 12, steps per second: 5053, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.100 [-2.130, 1.377], mean_best_reward: --
  7989/100000: episode: 446, duration: 0.008s, episode steps: 51, steps per second: 6126, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.046 [-1.187, 0.583], mean_best_reward: --
  7997/100000: episode: 447, duration: 0.002s, episode steps: 8, steps per second: 4726, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.131 [-1.214, 1.988], mean_best_reward: --
  8008/100000: episode: 448, duration: 0.002s, episode steps: 11, steps per second: 5092, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.132 [-0.988, 1.672], mean_best_reward: --
  8032/100000: episode: 449, duration: 0.004s, episode steps: 24, steps per second: 5565, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.082 [-1.576, 2.671], mean_best_reward: --
  8051/100000: episode: 450, duration: 0.004s, episode steps: 19, steps per second: 4706, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.091 [-2.078, 1.185], mean_best_reward: --
  8066/100000: episode: 451, duration: 0.003s, episode steps: 15, steps per second: 4900, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.111 [-0.755, 1.206], mean_best_reward: 36.500000
  8088/100000: episode: 452, duration: 0.004s, episode steps: 22, steps per second: 5735, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.043 [-0.815, 1.437], mean_best_reward: --
  8105/100000: episode: 453, duration: 0.003s, episode steps: 17, steps per second: 5159, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.064 [-1.729, 2.682], mean_best_reward: --
  8124/100000: episode: 454, duration: 0.003s, episode steps: 19, steps per second: 5644, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.075 [-1.291, 0.643], mean_best_reward: --
  8155/100000: episode: 455, duration: 0.005s, episode steps: 31, steps per second: 5956, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.081 [-0.609, 0.893], mean_best_reward: --
  8171/100000: episode: 456, duration: 0.003s, episode steps: 16, steps per second: 5445, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.087 [-1.961, 1.174], mean_best_reward: --
  8183/100000: episode: 457, duration: 0.002s, episode steps: 12, steps per second: 5108, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.099 [-2.950, 1.939], mean_best_reward: --
  8205/100000: episode: 458, duration: 0.004s, episode steps: 22, steps per second: 5693, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.064 [-1.210, 2.141], mean_best_reward: --
  8225/100000: episode: 459, duration: 0.004s, episode steps: 20, steps per second: 5690, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.072 [-1.187, 2.015], mean_best_reward: --
  8244/100000: episode: 460, duration: 0.003s, episode steps: 19, steps per second: 5571, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.084 [-1.135, 1.910], mean_best_reward: --
  8256/100000: episode: 461, duration: 0.002s, episode steps: 12, steps per second: 5228, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.135 [-3.091, 1.923], mean_best_reward: --
  8266/100000: episode: 462, duration: 0.002s, episode steps: 10, steps per second: 4869, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.155 [-1.533, 2.589], mean_best_reward: --
  8282/100000: episode: 463, duration: 0.004s, episode steps: 16, steps per second: 4294, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.089 [-1.192, 2.035], mean_best_reward: --
  8298/100000: episode: 464, duration: 0.003s, episode steps: 16, steps per second: 5359, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.094 [-1.729, 0.952], mean_best_reward: --
  8311/100000: episode: 465, duration: 0.002s, episode steps: 13, steps per second: 5314, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.117 [-1.616, 2.573], mean_best_reward: --
  8324/100000: episode: 466, duration: 0.002s, episode steps: 13, steps per second: 5274, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.109 [-1.782, 0.989], mean_best_reward: --
  8335/100000: episode: 467, duration: 0.002s, episode steps: 11, steps per second: 5121, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.118 [-2.846, 1.764], mean_best_reward: --
  8364/100000: episode: 468, duration: 0.005s, episode steps: 29, steps per second: 5845, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.109 [-1.245, 0.402], mean_best_reward: --
  8374/100000: episode: 469, duration: 0.002s, episode steps: 10, steps per second: 5023, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.119 [-2.549, 1.602], mean_best_reward: --
  8387/100000: episode: 470, duration: 0.002s, episode steps: 13, steps per second: 5327, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.127 [-2.352, 1.332], mean_best_reward: --
  8398/100000: episode: 471, duration: 0.002s, episode steps: 11, steps per second: 5160, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.097 [-1.803, 2.749], mean_best_reward: --
  8408/100000: episode: 472, duration: 0.002s, episode steps: 10, steps per second: 5056, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.131 [-2.599, 1.610], mean_best_reward: --
  8423/100000: episode: 473, duration: 0.003s, episode steps: 15, steps per second: 5410, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.790, 1.273], mean_best_reward: --
  8446/100000: episode: 474, duration: 0.004s, episode steps: 23, steps per second: 5710, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.055 [-1.433, 1.001], mean_best_reward: --
  8461/100000: episode: 475, duration: 0.003s, episode steps: 15, steps per second: 5454, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.085 [-1.744, 2.660], mean_best_reward: --
  8476/100000: episode: 476, duration: 0.003s, episode steps: 15, steps per second: 5369, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.095 [-1.755, 0.940], mean_best_reward: --
  8488/100000: episode: 477, duration: 0.002s, episode steps: 12, steps per second: 5002, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.115 [-1.160, 2.060], mean_best_reward: --
  8516/100000: episode: 478, duration: 0.005s, episode steps: 28, steps per second: 5689, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.811, 1.127], mean_best_reward: --
  8535/100000: episode: 479, duration: 0.004s, episode steps: 19, steps per second: 5028, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.080 [-1.390, 2.334], mean_best_reward: --
  8577/100000: episode: 480, duration: 0.007s, episode steps: 42, steps per second: 6094, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.137 [-2.924, 2.717], mean_best_reward: --
  8618/100000: episode: 481, duration: 0.007s, episode steps: 41, steps per second: 6032, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.040 [-0.463, 1.271], mean_best_reward: --
  8629/100000: episode: 482, duration: 0.002s, episode steps: 11, steps per second: 5126, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.127 [-1.158, 1.957], mean_best_reward: --
  8647/100000: episode: 483, duration: 0.003s, episode steps: 18, steps per second: 5608, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.083 [-0.653, 1.288], mean_best_reward: --
  8662/100000: episode: 484, duration: 0.003s, episode steps: 15, steps per second: 5443, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.096 [-0.963, 1.431], mean_best_reward: --
  8687/100000: episode: 485, duration: 0.005s, episode steps: 25, steps per second: 5475, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.036 [-0.633, 0.903], mean_best_reward: --
  8700/100000: episode: 486, duration: 0.002s, episode steps: 13, steps per second: 5206, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.103 [-1.381, 2.207], mean_best_reward: --
  8719/100000: episode: 487, duration: 0.003s, episode steps: 19, steps per second: 5641, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.093 [-1.336, 2.329], mean_best_reward: --
  8729/100000: episode: 488, duration: 0.002s, episode steps: 10, steps per second: 5049, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.135 [-2.051, 1.155], mean_best_reward: --
  8744/100000: episode: 489, duration: 0.003s, episode steps: 15, steps per second: 5446, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.128 [-0.991, 1.963], mean_best_reward: --
  8757/100000: episode: 490, duration: 0.002s, episode steps: 13, steps per second: 5208, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.092 [-2.725, 1.756], mean_best_reward: --
  8826/100000: episode: 491, duration: 0.012s, episode steps: 69, steps per second: 5890, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: -0.074 [-2.218, 1.373], mean_best_reward: --
  8837/100000: episode: 492, duration: 0.002s, episode steps: 11, steps per second: 4901, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.129 [-0.941, 1.606], mean_best_reward: --
  8849/100000: episode: 493, duration: 0.002s, episode steps: 12, steps per second: 4888, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.092 [-1.224, 1.914], mean_best_reward: --
  8864/100000: episode: 494, duration: 0.003s, episode steps: 15, steps per second: 4804, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.064 [-1.007, 1.583], mean_best_reward: --
  8907/100000: episode: 495, duration: 0.007s, episode steps: 43, steps per second: 6123, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.395 [0.000, 1.000], mean observation: 0.025 [-1.756, 2.364], mean_best_reward: --
  8919/100000: episode: 496, duration: 0.002s, episode steps: 12, steps per second: 5092, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.325, 2.026], mean_best_reward: --
  8929/100000: episode: 497, duration: 0.002s, episode steps: 10, steps per second: 5026, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.134 [-1.137, 2.039], mean_best_reward: --
  8939/100000: episode: 498, duration: 0.002s, episode steps: 10, steps per second: 4241, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.122 [-1.405, 2.145], mean_best_reward: --
  8952/100000: episode: 499, duration: 0.003s, episode steps: 13, steps per second: 5183, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.088 [-1.407, 2.268], mean_best_reward: --
  8962/100000: episode: 500, duration: 0.002s, episode steps: 10, steps per second: 5029, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.144 [-1.542, 2.614], mean_best_reward: --
  8975/100000: episode: 501, duration: 0.003s, episode steps: 13, steps per second: 4735, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.125 [-2.051, 1.143], mean_best_reward: 54.000000
  9000/100000: episode: 502, duration: 0.004s, episode steps: 25, steps per second: 5824, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.088 [-0.579, 1.528], mean_best_reward: --
  9015/100000: episode: 503, duration: 0.003s, episode steps: 15, steps per second: 5389, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.108 [-0.792, 1.485], mean_best_reward: --
  9026/100000: episode: 504, duration: 0.003s, episode steps: 11, steps per second: 4386, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.110 [-1.406, 0.820], mean_best_reward: --
  9041/100000: episode: 505, duration: 0.003s, episode steps: 15, steps per second: 4356, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.090 [-2.776, 1.753], mean_best_reward: --
  9069/100000: episode: 506, duration: 0.005s, episode steps: 28, steps per second: 5908, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.083 [-0.755, 1.273], mean_best_reward: --
  9098/100000: episode: 507, duration: 0.005s, episode steps: 29, steps per second: 5721, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.082 [-1.410, 0.638], mean_best_reward: --
  9128/100000: episode: 508, duration: 0.005s, episode steps: 30, steps per second: 5881, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.091 [-1.444, 0.469], mean_best_reward: --
  9137/100000: episode: 509, duration: 0.002s, episode steps: 9, steps per second: 4893, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.810, 1.744], mean_best_reward: --
  9148/100000: episode: 510, duration: 0.002s, episode steps: 11, steps per second: 5158, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.144 [-2.503, 1.550], mean_best_reward: --
  9170/100000: episode: 511, duration: 0.004s, episode steps: 22, steps per second: 5753, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.056 [-1.721, 0.990], mean_best_reward: --
  9191/100000: episode: 512, duration: 0.004s, episode steps: 21, steps per second: 5233, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.063 [-1.076, 0.570], mean_best_reward: --
  9210/100000: episode: 513, duration: 0.003s, episode steps: 19, steps per second: 5647, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.091 [-1.166, 0.806], mean_best_reward: --
  9221/100000: episode: 514, duration: 0.002s, episode steps: 11, steps per second: 5154, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.135 [-2.357, 1.340], mean_best_reward: --
  9237/100000: episode: 515, duration: 0.003s, episode steps: 16, steps per second: 5550, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.118 [-1.609, 0.759], mean_best_reward: --
  9254/100000: episode: 516, duration: 0.003s, episode steps: 17, steps per second: 5571, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.089 [-1.243, 0.739], mean_best_reward: --
  9271/100000: episode: 517, duration: 0.004s, episode steps: 17, steps per second: 4543, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.060 [-1.741, 2.743], mean_best_reward: --
  9284/100000: episode: 518, duration: 0.003s, episode steps: 13, steps per second: 4735, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.070 [-0.824, 1.322], mean_best_reward: --
  9312/100000: episode: 519, duration: 0.005s, episode steps: 28, steps per second: 5897, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.613, 0.918], mean_best_reward: --
  9324/100000: episode: 520, duration: 0.002s, episode steps: 12, steps per second: 5205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.110 [-1.524, 2.463], mean_best_reward: --
  9337/100000: episode: 521, duration: 0.003s, episode steps: 13, steps per second: 4762, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.096 [-1.732, 1.133], mean_best_reward: --
  9356/100000: episode: 522, duration: 0.004s, episode steps: 19, steps per second: 5133, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.074 [-2.477, 1.586], mean_best_reward: --
  9367/100000: episode: 523, duration: 0.002s, episode steps: 11, steps per second: 5126, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.106 [-1.397, 2.321], mean_best_reward: --
  9377/100000: episode: 524, duration: 0.002s, episode steps: 10, steps per second: 5065, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.541, 2.566], mean_best_reward: --
  9389/100000: episode: 525, duration: 0.002s, episode steps: 12, steps per second: 5252, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.129 [-2.263, 1.326], mean_best_reward: --
  9441/100000: episode: 526, duration: 0.009s, episode steps: 52, steps per second: 5940, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.092 [-1.001, 0.466], mean_best_reward: --
  9466/100000: episode: 527, duration: 0.004s, episode steps: 25, steps per second: 5792, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: -0.002 [-2.562, 1.779], mean_best_reward: --
  9624/100000: episode: 528, duration: 0.025s, episode steps: 158, steps per second: 6220, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-1.855, 1.838], mean_best_reward: --
  9636/100000: episode: 529, duration: 0.003s, episode steps: 12, steps per second: 4768, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.094 [-0.788, 1.405], mean_best_reward: --
  9660/100000: episode: 530, duration: 0.004s, episode steps: 24, steps per second: 5801, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.065 [-1.177, 0.567], mean_best_reward: --
  9683/100000: episode: 531, duration: 0.004s, episode steps: 23, steps per second: 5789, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.050 [-1.160, 0.630], mean_best_reward: --
  9696/100000: episode: 532, duration: 0.003s, episode steps: 13, steps per second: 5062, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.104 [-2.743, 1.741], mean_best_reward: --
  9706/100000: episode: 533, duration: 0.002s, episode steps: 10, steps per second: 5052, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.132 [-0.983, 1.630], mean_best_reward: --
  9716/100000: episode: 534, duration: 0.002s, episode steps: 10, steps per second: 4532, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.158 [-2.285, 1.368], mean_best_reward: --
  9728/100000: episode: 535, duration: 0.002s, episode steps: 12, steps per second: 4914, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.116 [-1.678, 0.989], mean_best_reward: --
  9743/100000: episode: 536, duration: 0.003s, episode steps: 15, steps per second: 5446, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.100 [-0.743, 1.252], mean_best_reward: --
  9757/100000: episode: 537, duration: 0.003s, episode steps: 14, steps per second: 5403, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.096 [-1.744, 1.125], mean_best_reward: --
  9775/100000: episode: 538, duration: 0.003s, episode steps: 18, steps per second: 5412, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.054 [-1.396, 2.068], mean_best_reward: --
  9791/100000: episode: 539, duration: 0.003s, episode steps: 16, steps per second: 4765, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.090 [-1.009, 1.758], mean_best_reward: --
  9803/100000: episode: 540, duration: 0.003s, episode steps: 12, steps per second: 4308, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.138 [-1.496, 0.751], mean_best_reward: --
  9819/100000: episode: 541, duration: 0.003s, episode steps: 16, steps per second: 5479, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.090 [-1.564, 2.610], mean_best_reward: --
  9842/100000: episode: 542, duration: 0.004s, episode steps: 23, steps per second: 5779, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.103 [-1.943, 0.965], mean_best_reward: --
  9863/100000: episode: 543, duration: 0.004s, episode steps: 21, steps per second: 5620, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.057 [-1.138, 0.823], mean_best_reward: --
  9879/100000: episode: 544, duration: 0.003s, episode steps: 16, steps per second: 4930, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.068 [-1.664, 1.025], mean_best_reward: --
  9895/100000: episode: 545, duration: 0.003s, episode steps: 16, steps per second: 5520, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.120 [-0.781, 1.246], mean_best_reward: --
  9912/100000: episode: 546, duration: 0.003s, episode steps: 17, steps per second: 5299, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.176 [0.000, 1.000], mean observation: 0.047 [-2.121, 3.141], mean_best_reward: --
  9936/100000: episode: 547, duration: 0.004s, episode steps: 24, steps per second: 5793, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.075 [-1.795, 0.846], mean_best_reward: --
  9948/100000: episode: 548, duration: 0.002s, episode steps: 12, steps per second: 5267, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.123 [-1.706, 0.949], mean_best_reward: --
  9957/100000: episode: 549, duration: 0.002s, episode steps: 9, steps per second: 4406, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.822, 1.799], mean_best_reward: --
 10035/100000: episode: 550, duration: 0.012s, episode steps: 78, steps per second: 6326, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-0.778, 1.188], mean_best_reward: --
 10058/100000: episode: 551, duration: 0.006s, episode steps: 23, steps per second: 3972, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.105 [-0.537, 1.229], mean_best_reward: 59.500000
 10070/100000: episode: 552, duration: 0.002s, episode steps: 12, steps per second: 5168, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.108 [-1.348, 0.817], mean_best_reward: --
 10081/100000: episode: 553, duration: 0.002s, episode steps: 11, steps per second: 5145, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.107 [-1.538, 0.962], mean_best_reward: --
 10093/100000: episode: 554, duration: 0.002s, episode steps: 12, steps per second: 5200, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.141 [-1.138, 2.063], mean_best_reward: --
 10105/100000: episode: 555, duration: 0.002s, episode steps: 12, steps per second: 4967, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.097 [-1.991, 1.178], mean_best_reward: --
 10127/100000: episode: 556, duration: 0.004s, episode steps: 22, steps per second: 5266, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.011 [-1.225, 1.856], mean_best_reward: --
 10142/100000: episode: 557, duration: 0.003s, episode steps: 15, steps per second: 5435, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.867 [0.000, 1.000], mean observation: -0.076 [-3.235, 2.118], mean_best_reward: --
 10152/100000: episode: 558, duration: 0.002s, episode steps: 10, steps per second: 5056, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.119 [-1.965, 1.208], mean_best_reward: --
 10197/100000: episode: 559, duration: 0.007s, episode steps: 45, steps per second: 6148, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.080 [-0.727, 1.254], mean_best_reward: --
 10209/100000: episode: 560, duration: 0.003s, episode steps: 12, steps per second: 4526, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.113 [-2.037, 1.211], mean_best_reward: --
 10223/100000: episode: 561, duration: 0.003s, episode steps: 14, steps per second: 5352, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.097 [-2.079, 1.228], mean_best_reward: --
 10266/100000: episode: 562, duration: 0.007s, episode steps: 43, steps per second: 6022, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.016 [-1.541, 2.159], mean_best_reward: --
 10280/100000: episode: 563, duration: 0.003s, episode steps: 14, steps per second: 5405, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.097 [-3.014, 1.970], mean_best_reward: --
 10290/100000: episode: 564, duration: 0.002s, episode steps: 10, steps per second: 4249, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.117 [-2.264, 1.404], mean_best_reward: --
 10311/100000: episode: 565, duration: 0.004s, episode steps: 21, steps per second: 5102, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.076 [-0.807, 1.503], mean_best_reward: --
 10323/100000: episode: 566, duration: 0.002s, episode steps: 12, steps per second: 5148, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.127 [-1.334, 0.736], mean_best_reward: --
 10347/100000: episode: 567, duration: 0.004s, episode steps: 24, steps per second: 5783, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.005 [-2.322, 1.587], mean_best_reward: --
 10364/100000: episode: 568, duration: 0.003s, episode steps: 17, steps per second: 5537, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.079 [-1.994, 1.203], mean_best_reward: --
 10402/100000: episode: 569, duration: 0.007s, episode steps: 38, steps per second: 5691, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.080, 0.737], mean_best_reward: --
 10412/100000: episode: 570, duration: 0.002s, episode steps: 10, steps per second: 5012, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.126 [-2.536, 1.577], mean_best_reward: --
 10425/100000: episode: 571, duration: 0.002s, episode steps: 13, steps per second: 5308, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.108 [-2.854, 1.782], mean_best_reward: --
 10437/100000: episode: 572, duration: 0.002s, episode steps: 12, steps per second: 5223, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.100 [-1.328, 1.987], mean_best_reward: --
 10448/100000: episode: 573, duration: 0.002s, episode steps: 11, steps per second: 5137, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.138 [-2.016, 1.158], mean_best_reward: --
 10464/100000: episode: 574, duration: 0.003s, episode steps: 16, steps per second: 4917, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.098 [-1.335, 0.772], mean_best_reward: --
 10479/100000: episode: 575, duration: 0.003s, episode steps: 15, steps per second: 5435, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.074 [-2.802, 1.806], mean_best_reward: --
 10488/100000: episode: 576, duration: 0.002s, episode steps: 9, steps per second: 4762, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.837, 1.789], mean_best_reward: --
 10497/100000: episode: 577, duration: 0.002s, episode steps: 9, steps per second: 4894, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.160 [-2.172, 1.325], mean_best_reward: --
 10510/100000: episode: 578, duration: 0.002s, episode steps: 13, steps per second: 5318, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.116 [-1.370, 2.336], mean_best_reward: --
 10519/100000: episode: 579, duration: 0.002s, episode steps: 9, steps per second: 4920, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.121 [-2.213, 1.396], mean_best_reward: --
 10565/100000: episode: 580, duration: 0.008s, episode steps: 46, steps per second: 5592, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.171 [-0.987, 0.707], mean_best_reward: --
 10578/100000: episode: 581, duration: 0.002s, episode steps: 13, steps per second: 5295, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.113 [-1.805, 1.018], mean_best_reward: --
 10589/100000: episode: 582, duration: 0.002s, episode steps: 11, steps per second: 5135, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.100 [-1.418, 2.306], mean_best_reward: --
 10616/100000: episode: 583, duration: 0.005s, episode steps: 27, steps per second: 5866, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.370 [0.000, 1.000], mean observation: 0.083 [-1.332, 2.371], mean_best_reward: --
 10629/100000: episode: 584, duration: 0.003s, episode steps: 13, steps per second: 5007, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.109 [-0.961, 1.453], mean_best_reward: --
 10640/100000: episode: 585, duration: 0.002s, episode steps: 11, steps per second: 5143, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.126 [-1.013, 1.551], mean_best_reward: --
 10651/100000: episode: 586, duration: 0.002s, episode steps: 11, steps per second: 5141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.362, 1.369], mean_best_reward: --
 10662/100000: episode: 587, duration: 0.002s, episode steps: 11, steps per second: 5132, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.122 [-2.836, 1.765], mean_best_reward: --
 10675/100000: episode: 588, duration: 0.003s, episode steps: 13, steps per second: 5192, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.101 [-2.785, 1.780], mean_best_reward: --
 10692/100000: episode: 589, duration: 0.003s, episode steps: 17, steps per second: 5543, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.093 [-0.761, 1.456], mean_best_reward: --
 10716/100000: episode: 590, duration: 0.004s, episode steps: 24, steps per second: 5391, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.089 [-0.743, 1.333], mean_best_reward: --
 10755/100000: episode: 591, duration: 0.006s, episode steps: 39, steps per second: 6075, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: -0.004 [-1.767, 2.592], mean_best_reward: --
 10808/100000: episode: 592, duration: 0.010s, episode steps: 53, steps per second: 5567, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.057 [-0.920, 0.349], mean_best_reward: --
 10817/100000: episode: 593, duration: 0.002s, episode steps: 9, steps per second: 4859, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.280, 1.408], mean_best_reward: --
 10832/100000: episode: 594, duration: 0.003s, episode steps: 15, steps per second: 5414, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.101 [-1.818, 1.021], mean_best_reward: --
 10870/100000: episode: 595, duration: 0.006s, episode steps: 38, steps per second: 6056, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.605 [0.000, 1.000], mean observation: -0.023 [-2.544, 1.790], mean_best_reward: --
 10888/100000: episode: 596, duration: 0.004s, episode steps: 18, steps per second: 5071, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.062 [-2.324, 1.579], mean_best_reward: --
 10906/100000: episode: 597, duration: 0.003s, episode steps: 18, steps per second: 5469, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.050 [-1.632, 1.153], mean_best_reward: --
 10955/100000: episode: 598, duration: 0.008s, episode steps: 49, steps per second: 6193, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.001 [-1.130, 0.738], mean_best_reward: --
 10965/100000: episode: 599, duration: 0.002s, episode steps: 10, steps per second: 4346, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.124 [-2.471, 1.604], mean_best_reward: --
 10977/100000: episode: 600, duration: 0.002s, episode steps: 12, steps per second: 5118, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.111 [-1.738, 1.007], mean_best_reward: --
 10986/100000: episode: 601, duration: 0.002s, episode steps: 9, steps per second: 4239, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.178 [-2.859, 1.726], mean_best_reward: 58.500000
 11020/100000: episode: 602, duration: 0.006s, episode steps: 34, steps per second: 5998, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.018 [-2.041, 1.383], mean_best_reward: --
 11031/100000: episode: 603, duration: 0.002s, episode steps: 11, steps per second: 5114, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.122 [-1.159, 2.047], mean_best_reward: --
 11046/100000: episode: 604, duration: 0.003s, episode steps: 15, steps per second: 4832, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.087 [-2.333, 1.372], mean_best_reward: --
 11059/100000: episode: 605, duration: 0.003s, episode steps: 13, steps per second: 4108, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.101 [-1.004, 1.708], mean_best_reward: --
 11108/100000: episode: 606, duration: 0.008s, episode steps: 49, steps per second: 6185, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.040 [-1.179, 1.371], mean_best_reward: --
 11123/100000: episode: 607, duration: 0.003s, episode steps: 15, steps per second: 5470, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.066 [-0.842, 1.446], mean_best_reward: --
 11140/100000: episode: 608, duration: 0.003s, episode steps: 17, steps per second: 5002, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.092 [-1.391, 2.318], mean_best_reward: --
 11177/100000: episode: 609, duration: 0.006s, episode steps: 37, steps per second: 6025, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.093 [-0.573, 1.089], mean_best_reward: --
 11187/100000: episode: 610, duration: 0.002s, episode steps: 10, steps per second: 5011, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.571, 1.557], mean_best_reward: --
 11230/100000: episode: 611, duration: 0.007s, episode steps: 43, steps per second: 5762, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.037 [-1.002, 1.193], mean_best_reward: --
 11240/100000: episode: 612, duration: 0.002s, episode steps: 10, steps per second: 4989, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.165 [-1.721, 2.775], mean_best_reward: --
 11261/100000: episode: 613, duration: 0.004s, episode steps: 21, steps per second: 5702, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.106 [-0.621, 0.909], mean_best_reward: --
 11281/100000: episode: 614, duration: 0.004s, episode steps: 20, steps per second: 5692, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.049 [-3.096, 2.119], mean_best_reward: --
 11325/100000: episode: 615, duration: 0.008s, episode steps: 44, steps per second: 5375, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: 0.116 [-2.380, 2.262], mean_best_reward: --
 11347/100000: episode: 616, duration: 0.004s, episode steps: 22, steps per second: 5709, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.047 [-1.936, 1.219], mean_best_reward: --
 11362/100000: episode: 617, duration: 0.003s, episode steps: 15, steps per second: 5408, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.076 [-0.993, 1.514], mean_best_reward: --
 11375/100000: episode: 618, duration: 0.002s, episode steps: 13, steps per second: 5281, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.105 [-0.968, 1.731], mean_best_reward: --
 11386/100000: episode: 619, duration: 0.002s, episode steps: 11, steps per second: 4965, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.122 [-0.817, 1.430], mean_best_reward: --
 11396/100000: episode: 620, duration: 0.002s, episode steps: 10, steps per second: 4878, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.129 [-1.583, 0.984], mean_best_reward: --
 11412/100000: episode: 621, duration: 0.003s, episode steps: 16, steps per second: 5468, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.234, 0.747], mean_best_reward: --
 11428/100000: episode: 622, duration: 0.003s, episode steps: 16, steps per second: 5506, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.092 [-0.750, 1.493], mean_best_reward: --
 11451/100000: episode: 623, duration: 0.004s, episode steps: 23, steps per second: 5784, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.739 [0.000, 1.000], mean observation: -0.060 [-3.231, 2.111], mean_best_reward: --
 11467/100000: episode: 624, duration: 0.003s, episode steps: 16, steps per second: 5471, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.082 [-0.807, 1.267], mean_best_reward: --
 11478/100000: episode: 625, duration: 0.002s, episode steps: 11, steps per second: 4550, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.111 [-1.394, 2.238], mean_best_reward: --
 11492/100000: episode: 626, duration: 0.003s, episode steps: 14, steps per second: 5367, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.085 [-1.597, 2.536], mean_best_reward: --
 11507/100000: episode: 627, duration: 0.003s, episode steps: 15, steps per second: 5364, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.095 [-1.246, 0.800], mean_best_reward: --
 11522/100000: episode: 628, duration: 0.003s, episode steps: 15, steps per second: 5388, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.085 [-0.972, 1.749], mean_best_reward: --
 11538/100000: episode: 629, duration: 0.003s, episode steps: 16, steps per second: 5310, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.101 [-0.548, 1.129], mean_best_reward: --
 11555/100000: episode: 630, duration: 0.003s, episode steps: 17, steps per second: 4911, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.085 [-1.401, 0.829], mean_best_reward: --
 11565/100000: episode: 631, duration: 0.003s, episode steps: 10, steps per second: 3864, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.157 [-1.334, 2.130], mean_best_reward: --
 11581/100000: episode: 632, duration: 0.003s, episode steps: 16, steps per second: 5488, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.100 [-0.612, 1.124], mean_best_reward: --
 11596/100000: episode: 633, duration: 0.003s, episode steps: 15, steps per second: 5439, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.095 [-1.517, 2.482], mean_best_reward: --
 11631/100000: episode: 634, duration: 0.006s, episode steps: 35, steps per second: 5813, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.095 [-1.765, 1.768], mean_best_reward: --
 11643/100000: episode: 635, duration: 0.002s, episode steps: 12, steps per second: 5109, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.121 [-1.386, 2.194], mean_best_reward: --
 11653/100000: episode: 636, duration: 0.002s, episode steps: 10, steps per second: 5046, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.144 [-2.624, 1.591], mean_best_reward: --
 11688/100000: episode: 637, duration: 0.006s, episode steps: 35, steps per second: 5964, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.113 [-0.796, 0.523], mean_best_reward: --
 11702/100000: episode: 638, duration: 0.003s, episode steps: 14, steps per second: 5357, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.116 [-1.342, 0.750], mean_best_reward: --
 11723/100000: episode: 639, duration: 0.004s, episode steps: 21, steps per second: 5650, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.047 [-0.969, 1.458], mean_best_reward: --
 11741/100000: episode: 640, duration: 0.003s, episode steps: 18, steps per second: 5591, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.596, 1.030], mean_best_reward: --
 11750/100000: episode: 641, duration: 0.002s, episode steps: 9, steps per second: 4607, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.160 [-2.504, 1.556], mean_best_reward: --
 11762/100000: episode: 642, duration: 0.002s, episode steps: 12, steps per second: 5216, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.072 [-1.862, 1.203], mean_best_reward: --
 11778/100000: episode: 643, duration: 0.003s, episode steps: 16, steps per second: 5492, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.076 [-1.550, 0.835], mean_best_reward: --
 11811/100000: episode: 644, duration: 0.006s, episode steps: 33, steps per second: 5795, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.056 [-0.607, 1.005], mean_best_reward: --
 11846/100000: episode: 645, duration: 0.006s, episode steps: 35, steps per second: 5520, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.177 [-0.994, 0.619], mean_best_reward: --
 11858/100000: episode: 646, duration: 0.002s, episode steps: 12, steps per second: 5179, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-2.421, 1.563], mean_best_reward: --
 11869/100000: episode: 647, duration: 0.002s, episode steps: 11, steps per second: 5123, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.131 [-2.484, 1.585], mean_best_reward: --
 11889/100000: episode: 648, duration: 0.004s, episode steps: 20, steps per second: 5601, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.070 [-1.508, 0.756], mean_best_reward: --
 11956/100000: episode: 649, duration: 0.011s, episode steps: 67, steps per second: 6287, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.036 [-0.624, 1.115], mean_best_reward: --
 11976/100000: episode: 650, duration: 0.004s, episode steps: 20, steps per second: 5678, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.058 [-0.986, 1.548], mean_best_reward: --
 11990/100000: episode: 651, duration: 0.003s, episode steps: 14, steps per second: 4709, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.096 [-0.812, 1.515], mean_best_reward: 56.500000
 12006/100000: episode: 652, duration: 0.003s, episode steps: 16, steps per second: 5490, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.087 [-2.010, 1.320], mean_best_reward: --
 12038/100000: episode: 653, duration: 0.005s, episode steps: 32, steps per second: 5961, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.115 [-1.056, 0.593], mean_best_reward: --
 12112/100000: episode: 654, duration: 0.012s, episode steps: 74, steps per second: 6062, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.146 [-0.571, 1.115], mean_best_reward: --
 12128/100000: episode: 655, duration: 0.003s, episode steps: 16, steps per second: 5464, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.110 [-0.754, 1.587], mean_best_reward: --
 12187/100000: episode: 656, duration: 0.009s, episode steps: 59, steps per second: 6233, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.172 [-0.770, 1.113], mean_best_reward: --
 12209/100000: episode: 657, duration: 0.004s, episode steps: 22, steps per second: 5753, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.748, 1.138], mean_best_reward: --
 12220/100000: episode: 658, duration: 0.002s, episode steps: 11, steps per second: 4922, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.116 [-1.586, 2.478], mean_best_reward: --
 12236/100000: episode: 659, duration: 0.003s, episode steps: 16, steps per second: 5408, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.106 [-1.582, 0.789], mean_best_reward: --
 12245/100000: episode: 660, duration: 0.002s, episode steps: 9, steps per second: 4912, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.159 [-1.346, 2.293], mean_best_reward: --
 12259/100000: episode: 661, duration: 0.003s, episode steps: 14, steps per second: 4930, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.108 [-2.522, 1.526], mean_best_reward: --
 12269/100000: episode: 662, duration: 0.002s, episode steps: 10, steps per second: 5023, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.134 [-2.561, 1.601], mean_best_reward: --
 12278/100000: episode: 663, duration: 0.002s, episode steps: 9, steps per second: 4923, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.146 [-1.935, 1.138], mean_best_reward: --
 12297/100000: episode: 664, duration: 0.003s, episode steps: 19, steps per second: 5656, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.076 [-2.213, 1.346], mean_best_reward: --
 12324/100000: episode: 665, duration: 0.005s, episode steps: 27, steps per second: 5882, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.053 [-1.436, 0.974], mean_best_reward: --
 12337/100000: episode: 666, duration: 0.003s, episode steps: 13, steps per second: 5073, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.113 [-0.813, 1.350], mean_best_reward: --
 12348/100000: episode: 667, duration: 0.003s, episode steps: 11, steps per second: 4056, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.120 [-2.417, 1.557], mean_best_reward: --
 12359/100000: episode: 668, duration: 0.002s, episode steps: 11, steps per second: 5111, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.118 [-2.227, 1.376], mean_best_reward: --
 12496/100000: episode: 669, duration: 0.022s, episode steps: 137, steps per second: 6343, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.131 [-1.380, 1.388], mean_best_reward: --
 12515/100000: episode: 670, duration: 0.003s, episode steps: 19, steps per second: 5581, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.044 [-1.712, 1.189], mean_best_reward: --
 12528/100000: episode: 671, duration: 0.003s, episode steps: 13, steps per second: 5183, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.095 [-2.091, 1.358], mean_best_reward: --
 12547/100000: episode: 672, duration: 0.003s, episode steps: 19, steps per second: 5618, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.094 [-0.761, 1.406], mean_best_reward: --
 12561/100000: episode: 673, duration: 0.003s, episode steps: 14, steps per second: 5370, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.083 [-1.693, 1.020], mean_best_reward: --
 12570/100000: episode: 674, duration: 0.002s, episode steps: 9, steps per second: 4912, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.123 [-2.815, 1.807], mean_best_reward: --
 12581/100000: episode: 675, duration: 0.002s, episode steps: 11, steps per second: 5139, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.110 [-1.379, 2.215], mean_best_reward: --
 12593/100000: episode: 676, duration: 0.002s, episode steps: 12, steps per second: 5229, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.120 [-2.478, 1.519], mean_best_reward: --
 12606/100000: episode: 677, duration: 0.003s, episode steps: 13, steps per second: 4305, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.117 [-0.584, 1.119], mean_best_reward: --
 12618/100000: episode: 678, duration: 0.003s, episode steps: 12, steps per second: 4328, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.134 [-1.752, 2.732], mean_best_reward: --
 12638/100000: episode: 679, duration: 0.004s, episode steps: 20, steps per second: 5422, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.806, 1.145], mean_best_reward: --
 12648/100000: episode: 680, duration: 0.002s, episode steps: 10, steps per second: 5022, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.101 [-2.965, 2.000], mean_best_reward: --
 12661/100000: episode: 681, duration: 0.002s, episode steps: 13, steps per second: 5295, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.096 [-1.798, 1.034], mean_best_reward: --
 12673/100000: episode: 682, duration: 0.002s, episode steps: 12, steps per second: 4898, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.109 [-1.465, 0.747], mean_best_reward: --
 12684/100000: episode: 683, duration: 0.002s, episode steps: 11, steps per second: 4952, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.109 [-1.229, 2.013], mean_best_reward: --
 12699/100000: episode: 684, duration: 0.003s, episode steps: 15, steps per second: 5447, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.087 [-1.951, 1.217], mean_best_reward: --
 12709/100000: episode: 685, duration: 0.002s, episode steps: 10, steps per second: 5037, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.142 [-1.906, 1.152], mean_best_reward: --
 12732/100000: episode: 686, duration: 0.004s, episode steps: 23, steps per second: 5709, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.107 [-0.634, 1.640], mean_best_reward: --
 12743/100000: episode: 687, duration: 0.002s, episode steps: 11, steps per second: 5138, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.132 [-2.796, 1.768], mean_best_reward: --
 12782/100000: episode: 688, duration: 0.007s, episode steps: 39, steps per second: 5975, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.153 [-1.134, 0.828], mean_best_reward: --
 12795/100000: episode: 689, duration: 0.002s, episode steps: 13, steps per second: 5305, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.127 [-1.965, 1.138], mean_best_reward: --
 12806/100000: episode: 690, duration: 0.002s, episode steps: 11, steps per second: 5170, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.119 [-2.320, 1.584], mean_best_reward: --
 12817/100000: episode: 691, duration: 0.002s, episode steps: 11, steps per second: 5145, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.126 [-1.402, 2.233], mean_best_reward: --
 12829/100000: episode: 692, duration: 0.002s, episode steps: 12, steps per second: 5243, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.103 [-1.612, 0.996], mean_best_reward: --
 12838/100000: episode: 693, duration: 0.002s, episode steps: 9, steps per second: 4530, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.131 [-1.392, 2.262], mean_best_reward: --
 12860/100000: episode: 694, duration: 0.004s, episode steps: 22, steps per second: 5216, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.044 [-1.941, 1.202], mean_best_reward: --
 12873/100000: episode: 695, duration: 0.002s, episode steps: 13, steps per second: 5299, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.130 [-0.956, 1.886], mean_best_reward: --
 12903/100000: episode: 696, duration: 0.005s, episode steps: 30, steps per second: 5860, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.123, 0.544], mean_best_reward: --
 12914/100000: episode: 697, duration: 0.002s, episode steps: 11, steps per second: 4703, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.114 [-1.574, 0.981], mean_best_reward: --
 12931/100000: episode: 698, duration: 0.003s, episode steps: 17, steps per second: 5432, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.078 [-1.779, 2.784], mean_best_reward: --
 12954/100000: episode: 699, duration: 0.004s, episode steps: 23, steps per second: 5782, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.076 [-1.133, 0.788], mean_best_reward: --
 12992/100000: episode: 700, duration: 0.006s, episode steps: 38, steps per second: 6065, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.098 [-0.632, 1.015], mean_best_reward: --
 13003/100000: episode: 701, duration: 0.002s, episode steps: 11, steps per second: 4508, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.120 [-2.713, 1.715], mean_best_reward: 48.000000
 13017/100000: episode: 702, duration: 0.003s, episode steps: 14, steps per second: 4888, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.070 [-1.613, 2.574], mean_best_reward: --
 13030/100000: episode: 703, duration: 0.002s, episode steps: 13, steps per second: 5285, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.089 [-2.273, 1.365], mean_best_reward: --
 13059/100000: episode: 704, duration: 0.005s, episode steps: 29, steps per second: 5893, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.655 [0.000, 1.000], mean observation: -0.017 [-2.644, 1.738], mean_best_reward: --
 13070/100000: episode: 705, duration: 0.002s, episode steps: 11, steps per second: 5147, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.131 [-0.803, 1.510], mean_best_reward: --
 13090/100000: episode: 706, duration: 0.004s, episode steps: 20, steps per second: 5444, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.600, 1.236], mean_best_reward: --
 13108/100000: episode: 707, duration: 0.004s, episode steps: 18, steps per second: 4864, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.084 [-1.908, 3.028], mean_best_reward: --
 13120/100000: episode: 708, duration: 0.002s, episode steps: 12, steps per second: 4919, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.109 [-2.545, 1.539], mean_best_reward: --
 13138/100000: episode: 709, duration: 0.003s, episode steps: 18, steps per second: 5459, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.081 [-2.494, 1.530], mean_best_reward: --
 13150/100000: episode: 710, duration: 0.002s, episode steps: 12, steps per second: 4946, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.099 [-1.151, 1.936], mean_best_reward: --
 13165/100000: episode: 711, duration: 0.003s, episode steps: 15, steps per second: 5153, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.087 [-1.421, 2.405], mean_best_reward: --
 13195/100000: episode: 712, duration: 0.005s, episode steps: 30, steps per second: 5663, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.426, 1.025], mean_best_reward: --
 13217/100000: episode: 713, duration: 0.004s, episode steps: 22, steps per second: 5758, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.086 [-0.780, 1.491], mean_best_reward: --
 13237/100000: episode: 714, duration: 0.004s, episode steps: 20, steps per second: 5698, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.052 [-1.196, 1.884], mean_best_reward: --
 13252/100000: episode: 715, duration: 0.003s, episode steps: 15, steps per second: 5249, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.082 [-2.306, 1.364], mean_best_reward: --
 13283/100000: episode: 716, duration: 0.005s, episode steps: 31, steps per second: 5817, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: 0.003 [-1.557, 1.313], mean_best_reward: --
 13298/100000: episode: 717, duration: 0.003s, episode steps: 15, steps per second: 5287, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.114 [-1.761, 2.840], mean_best_reward: --
 13331/100000: episode: 718, duration: 0.005s, episode steps: 33, steps per second: 6025, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.084 [-1.327, 0.623], mean_best_reward: --
 13347/100000: episode: 719, duration: 0.003s, episode steps: 16, steps per second: 5075, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.064 [-1.714, 1.158], mean_best_reward: --
 13362/100000: episode: 720, duration: 0.003s, episode steps: 15, steps per second: 4406, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.102 [-0.817, 1.524], mean_best_reward: --
 13395/100000: episode: 721, duration: 0.005s, episode steps: 33, steps per second: 6035, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: -0.041 [-1.533, 1.733], mean_best_reward: --
 13405/100000: episode: 722, duration: 0.002s, episode steps: 10, steps per second: 5018, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.140 [-1.991, 1.155], mean_best_reward: --
 13414/100000: episode: 723, duration: 0.002s, episode steps: 9, steps per second: 4912, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.137 [-1.209, 1.961], mean_best_reward: --
 13429/100000: episode: 724, duration: 0.003s, episode steps: 15, steps per second: 5234, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.081 [-1.191, 1.927], mean_best_reward: --
 13441/100000: episode: 725, duration: 0.002s, episode steps: 12, steps per second: 5227, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.108 [-1.539, 2.372], mean_best_reward: --
 13453/100000: episode: 726, duration: 0.002s, episode steps: 12, steps per second: 5250, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.134 [-1.915, 3.090], mean_best_reward: --
 13465/100000: episode: 727, duration: 0.002s, episode steps: 12, steps per second: 5231, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.112 [-1.590, 2.660], mean_best_reward: --
 13481/100000: episode: 728, duration: 0.003s, episode steps: 16, steps per second: 5512, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.065 [-1.867, 1.135], mean_best_reward: --
 13501/100000: episode: 729, duration: 0.004s, episode steps: 20, steps per second: 5547, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.089 [-2.109, 1.182], mean_best_reward: --
 13540/100000: episode: 730, duration: 0.006s, episode steps: 39, steps per second: 6034, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.112 [-1.054, 0.628], mean_best_reward: --
 13558/100000: episode: 731, duration: 0.003s, episode steps: 18, steps per second: 5318, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.592, 1.134], mean_best_reward: --
 13579/100000: episode: 732, duration: 0.004s, episode steps: 21, steps per second: 5653, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.099 [-1.988, 0.999], mean_best_reward: --
 13595/100000: episode: 733, duration: 0.003s, episode steps: 16, steps per second: 4918, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.080 [-0.978, 1.593], mean_best_reward: --
 13608/100000: episode: 734, duration: 0.003s, episode steps: 13, steps per second: 5029, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.099 [-2.047, 1.189], mean_best_reward: --
 13621/100000: episode: 735, duration: 0.003s, episode steps: 13, steps per second: 4561, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.126 [-0.950, 1.822], mean_best_reward: --
 13634/100000: episode: 736, duration: 0.002s, episode steps: 13, steps per second: 5224, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.106 [-1.855, 0.989], mean_best_reward: --
 13650/100000: episode: 737, duration: 0.003s, episode steps: 16, steps per second: 5466, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.058 [-2.022, 1.385], mean_best_reward: --
 13673/100000: episode: 738, duration: 0.004s, episode steps: 23, steps per second: 5453, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.064 [-0.951, 1.752], mean_best_reward: --
 13689/100000: episode: 739, duration: 0.003s, episode steps: 16, steps per second: 5370, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.078 [-1.556, 0.826], mean_best_reward: --
 13701/100000: episode: 740, duration: 0.002s, episode steps: 12, steps per second: 5205, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.125 [-0.934, 1.529], mean_best_reward: --
 13720/100000: episode: 741, duration: 0.003s, episode steps: 19, steps per second: 5610, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.072 [-0.779, 1.377], mean_best_reward: --
 13732/100000: episode: 742, duration: 0.002s, episode steps: 12, steps per second: 5222, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.103 [-2.535, 1.585], mean_best_reward: --
 13744/100000: episode: 743, duration: 0.002s, episode steps: 12, steps per second: 5211, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.143 [-1.134, 2.081], mean_best_reward: --
 13755/100000: episode: 744, duration: 0.002s, episode steps: 11, steps per second: 4463, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.136 [-1.742, 0.947], mean_best_reward: --
 13774/100000: episode: 745, duration: 0.003s, episode steps: 19, steps per second: 5616, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.075 [-0.807, 1.516], mean_best_reward: --
 13788/100000: episode: 746, duration: 0.003s, episode steps: 14, steps per second: 5362, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.111 [-1.249, 0.737], mean_best_reward: --
 13798/100000: episode: 747, duration: 0.002s, episode steps: 10, steps per second: 5005, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-1.156, 1.948], mean_best_reward: --
 13806/100000: episode: 748, duration: 0.002s, episode steps: 8, steps per second: 4780, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.129 [-2.184, 1.413], mean_best_reward: --
 13818/100000: episode: 749, duration: 0.002s, episode steps: 12, steps per second: 5227, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.119 [-1.189, 1.974], mean_best_reward: --
 13832/100000: episode: 750, duration: 0.003s, episode steps: 14, steps per second: 4814, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.107 [-2.121, 1.150], mean_best_reward: --
 13847/100000: episode: 751, duration: 0.004s, episode steps: 15, steps per second: 4022, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.100 [-0.613, 0.970], mean_best_reward: 75.500000
 13860/100000: episode: 752, duration: 0.003s, episode steps: 13, steps per second: 5118, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.109 [-1.172, 1.942], mean_best_reward: --
 13876/100000: episode: 753, duration: 0.003s, episode steps: 16, steps per second: 5475, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.083 [-1.274, 0.766], mean_best_reward: --
 13923/100000: episode: 754, duration: 0.008s, episode steps: 47, steps per second: 5860, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.013 [-1.766, 1.196], mean_best_reward: --
 13956/100000: episode: 755, duration: 0.006s, episode steps: 33, steps per second: 5927, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.027 [-1.438, 0.785], mean_best_reward: --
 13982/100000: episode: 756, duration: 0.004s, episode steps: 26, steps per second: 5842, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.069 [-1.711, 0.847], mean_best_reward: --
 14034/100000: episode: 757, duration: 0.009s, episode steps: 52, steps per second: 5947, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.090 [-1.040, 0.661], mean_best_reward: --
 14049/100000: episode: 758, duration: 0.003s, episode steps: 15, steps per second: 5374, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.079 [-1.399, 2.116], mean_best_reward: --
 14064/100000: episode: 759, duration: 0.003s, episode steps: 15, steps per second: 5307, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.106 [-1.212, 0.744], mean_best_reward: --
 14082/100000: episode: 760, duration: 0.003s, episode steps: 18, steps per second: 5387, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.069 [-1.327, 1.961], mean_best_reward: --
 14099/100000: episode: 761, duration: 0.003s, episode steps: 17, steps per second: 5301, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.062 [-1.200, 1.967], mean_best_reward: --
 14114/100000: episode: 762, duration: 0.003s, episode steps: 15, steps per second: 4496, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.112 [-0.933, 1.511], mean_best_reward: --
 14149/100000: episode: 763, duration: 0.006s, episode steps: 35, steps per second: 6008, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.008 [-1.138, 1.330], mean_best_reward: --
 14168/100000: episode: 764, duration: 0.003s, episode steps: 19, steps per second: 5616, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.072 [-1.000, 1.444], mean_best_reward: --
 14193/100000: episode: 765, duration: 0.005s, episode steps: 25, steps per second: 5531, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.113 [-1.302, 0.779], mean_best_reward: --
 14209/100000: episode: 766, duration: 0.003s, episode steps: 16, steps per second: 5496, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.059 [-1.878, 1.183], mean_best_reward: --
 14258/100000: episode: 767, duration: 0.008s, episode steps: 49, steps per second: 6195, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.408 [0.000, 1.000], mean observation: 0.050 [-1.732, 2.789], mean_best_reward: --
 14268/100000: episode: 768, duration: 0.002s, episode steps: 10, steps per second: 4759, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.149 [-2.216, 1.333], mean_best_reward: --
 14285/100000: episode: 769, duration: 0.003s, episode steps: 17, steps per second: 5520, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.079 [-1.308, 0.770], mean_best_reward: --
 14301/100000: episode: 770, duration: 0.003s, episode steps: 16, steps per second: 5522, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.074 [-1.231, 2.077], mean_best_reward: --
 14313/100000: episode: 771, duration: 0.002s, episode steps: 12, steps per second: 5253, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.117 [-1.558, 2.529], mean_best_reward: --
 14329/100000: episode: 772, duration: 0.003s, episode steps: 16, steps per second: 5536, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.114 [-1.308, 0.750], mean_best_reward: --
 14340/100000: episode: 773, duration: 0.002s, episode steps: 11, steps per second: 5139, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.101 [-1.406, 2.282], mean_best_reward: --
 14365/100000: episode: 774, duration: 0.006s, episode steps: 25, steps per second: 4300, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.086 [-0.537, 1.010], mean_best_reward: --
 14388/100000: episode: 775, duration: 0.004s, episode steps: 23, steps per second: 5533, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.021 [-1.961, 2.671], mean_best_reward: --
 14467/100000: episode: 776, duration: 0.013s, episode steps: 79, steps per second: 6154, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.359 [-2.013, 0.694], mean_best_reward: --
 14481/100000: episode: 777, duration: 0.003s, episode steps: 14, steps per second: 5325, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.088 [-2.488, 1.581], mean_best_reward: --
 14562/100000: episode: 778, duration: 0.013s, episode steps: 81, steps per second: 6329, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.048 [-0.958, 0.802], mean_best_reward: --
 14574/100000: episode: 779, duration: 0.002s, episode steps: 12, steps per second: 4941, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-1.009, 1.784], mean_best_reward: --
 14595/100000: episode: 780, duration: 0.004s, episode steps: 21, steps per second: 5725, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.096 [-0.587, 0.980], mean_best_reward: --
 14612/100000: episode: 781, duration: 0.003s, episode steps: 17, steps per second: 5540, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.075 [-1.389, 0.831], mean_best_reward: --
 14622/100000: episode: 782, duration: 0.002s, episode steps: 10, steps per second: 4328, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.984, 3.051], mean_best_reward: --
 14636/100000: episode: 783, duration: 0.003s, episode steps: 14, steps per second: 4526, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.071 [-1.495, 1.026], mean_best_reward: --
 14658/100000: episode: 784, duration: 0.004s, episode steps: 22, steps per second: 5604, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.057 [-0.811, 1.524], mean_best_reward: --
 14673/100000: episode: 785, duration: 0.003s, episode steps: 15, steps per second: 5413, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.050 [-1.369, 1.991], mean_best_reward: --
 14788/100000: episode: 786, duration: 0.018s, episode steps: 115, steps per second: 6437, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.222 [-1.291, 1.740], mean_best_reward: --
 14800/100000: episode: 787, duration: 0.003s, episode steps: 12, steps per second: 4686, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-1.165, 1.889], mean_best_reward: --
 14809/100000: episode: 788, duration: 0.002s, episode steps: 9, steps per second: 4784, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.416, 2.329], mean_best_reward: --
 14829/100000: episode: 789, duration: 0.004s, episode steps: 20, steps per second: 5700, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.078 [-1.495, 0.798], mean_best_reward: --
 14868/100000: episode: 790, duration: 0.006s, episode steps: 39, steps per second: 6094, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.095 [-1.726, 0.607], mean_best_reward: --
 14963/100000: episode: 791, duration: 0.015s, episode steps: 95, steps per second: 6174, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.039 [-1.339, 0.811], mean_best_reward: --
 14982/100000: episode: 792, duration: 0.003s, episode steps: 19, steps per second: 5587, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.063 [-0.981, 1.722], mean_best_reward: --
 15003/100000: episode: 793, duration: 0.004s, episode steps: 21, steps per second: 5416, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.090 [-1.117, 0.599], mean_best_reward: --
 15020/100000: episode: 794, duration: 0.003s, episode steps: 17, steps per second: 5536, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.084 [-1.147, 1.862], mean_best_reward: --
 15038/100000: episode: 795, duration: 0.003s, episode steps: 18, steps per second: 5607, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.084 [-1.597, 2.587], mean_best_reward: --
 15070/100000: episode: 796, duration: 0.005s, episode steps: 32, steps per second: 6009, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.034 [-1.168, 0.612], mean_best_reward: --
 15091/100000: episode: 797, duration: 0.004s, episode steps: 21, steps per second: 5528, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.066 [-0.640, 1.351], mean_best_reward: --
 15103/100000: episode: 798, duration: 0.002s, episode steps: 12, steps per second: 5185, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.119 [-1.353, 2.119], mean_best_reward: --
 15122/100000: episode: 799, duration: 0.003s, episode steps: 19, steps per second: 5650, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.066 [-1.380, 0.834], mean_best_reward: --
 15135/100000: episode: 800, duration: 0.002s, episode steps: 13, steps per second: 5230, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.109 [-1.897, 1.143], mean_best_reward: --
 15146/100000: episode: 801, duration: 0.002s, episode steps: 11, steps per second: 4543, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.129 [-1.776, 0.960], mean_best_reward: 48.500000
 15157/100000: episode: 802, duration: 0.002s, episode steps: 11, steps per second: 4403, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.454, 1.599], mean_best_reward: --
 15207/100000: episode: 803, duration: 0.008s, episode steps: 50, steps per second: 5931, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-1.103, 0.579], mean_best_reward: --
 15218/100000: episode: 804, duration: 0.002s, episode steps: 11, steps per second: 4835, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.120 [-2.714, 1.764], mean_best_reward: --
 15244/100000: episode: 805, duration: 0.005s, episode steps: 26, steps per second: 5558, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.077 [-1.271, 0.609], mean_best_reward: --
 15268/100000: episode: 806, duration: 0.004s, episode steps: 24, steps per second: 5816, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.053 [-2.485, 1.530], mean_best_reward: --
 15329/100000: episode: 807, duration: 0.010s, episode steps: 61, steps per second: 6259, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.164 [-1.158, 2.008], mean_best_reward: --
 15337/100000: episode: 808, duration: 0.002s, episode steps: 8, steps per second: 4472, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [-1.570, 2.589], mean_best_reward: --
 15348/100000: episode: 809, duration: 0.002s, episode steps: 11, steps per second: 5084, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.083 [-2.017, 1.412], mean_best_reward: --
 15359/100000: episode: 810, duration: 0.002s, episode steps: 11, steps per second: 5127, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.096 [-2.743, 1.796], mean_best_reward: --
 15377/100000: episode: 811, duration: 0.003s, episode steps: 18, steps per second: 5667, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.053 [-1.708, 1.134], mean_best_reward: --
 15392/100000: episode: 812, duration: 0.003s, episode steps: 15, steps per second: 5453, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.094 [-1.625, 0.949], mean_best_reward: --
 15404/100000: episode: 813, duration: 0.002s, episode steps: 12, steps per second: 5265, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.086 [-2.407, 1.604], mean_best_reward: --
 15429/100000: episode: 814, duration: 0.005s, episode steps: 25, steps per second: 5340, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: -0.036 [-2.669, 1.747], mean_best_reward: --
 15445/100000: episode: 815, duration: 0.003s, episode steps: 16, steps per second: 4627, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.084 [-2.576, 1.612], mean_best_reward: --
 15459/100000: episode: 816, duration: 0.003s, episode steps: 14, steps per second: 5332, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.087 [-2.510, 1.610], mean_best_reward: --
 15471/100000: episode: 817, duration: 0.002s, episode steps: 12, steps per second: 5233, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.108 [-1.724, 0.987], mean_best_reward: --
 15482/100000: episode: 818, duration: 0.002s, episode steps: 11, steps per second: 5120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.149 [-1.716, 2.832], mean_best_reward: --
 15510/100000: episode: 819, duration: 0.005s, episode steps: 28, steps per second: 5852, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.126 [-0.366, 1.051], mean_best_reward: --
 15541/100000: episode: 820, duration: 0.005s, episode steps: 31, steps per second: 5972, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.710 [0.000, 1.000], mean observation: 0.048 [-3.076, 2.506], mean_best_reward: --
 15567/100000: episode: 821, duration: 0.004s, episode steps: 26, steps per second: 5860, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-0.986, 0.638], mean_best_reward: --
 15576/100000: episode: 822, duration: 0.002s, episode steps: 9, steps per second: 4786, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.128 [-2.224, 1.334], mean_best_reward: --
 15608/100000: episode: 823, duration: 0.006s, episode steps: 32, steps per second: 5765, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.052 [-1.197, 0.600], mean_best_reward: --
 15621/100000: episode: 824, duration: 0.002s, episode steps: 13, steps per second: 5267, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.110 [-1.420, 0.838], mean_best_reward: --
 15634/100000: episode: 825, duration: 0.002s, episode steps: 13, steps per second: 5308, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.094 [-1.804, 2.734], mean_best_reward: --
 15689/100000: episode: 826, duration: 0.009s, episode steps: 55, steps per second: 5880, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.021 [-0.797, 0.630], mean_best_reward: --
 15721/100000: episode: 827, duration: 0.006s, episode steps: 32, steps per second: 5561, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.196, 0.364], mean_best_reward: --
 15740/100000: episode: 828, duration: 0.003s, episode steps: 19, steps per second: 5440, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.111 [-1.192, 0.747], mean_best_reward: --
 15754/100000: episode: 829, duration: 0.003s, episode steps: 14, steps per second: 5057, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.062 [-2.376, 1.598], mean_best_reward: --
 15779/100000: episode: 830, duration: 0.004s, episode steps: 25, steps per second: 5689, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.036 [-1.739, 0.959], mean_best_reward: --
 15819/100000: episode: 831, duration: 0.007s, episode steps: 40, steps per second: 5778, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: 0.083 [-2.358, 2.292], mean_best_reward: --
 15830/100000: episode: 832, duration: 0.002s, episode steps: 11, steps per second: 5099, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-1.011, 1.810], mean_best_reward: --
 15898/100000: episode: 833, duration: 0.011s, episode steps: 68, steps per second: 6222, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: -0.037 [-2.266, 2.482], mean_best_reward: --
 15911/100000: episode: 834, duration: 0.003s, episode steps: 13, steps per second: 4962, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.107 [-1.028, 1.802], mean_best_reward: --
 16017/100000: episode: 835, duration: 0.017s, episode steps: 106, steps per second: 6193, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.019 [-1.149, 1.708], mean_best_reward: --
 16027/100000: episode: 836, duration: 0.002s, episode steps: 10, steps per second: 4893, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.105 [-2.141, 1.375], mean_best_reward: --
 16048/100000: episode: 837, duration: 0.004s, episode steps: 21, steps per second: 5366, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.066 [-1.418, 0.770], mean_best_reward: --
 16059/100000: episode: 838, duration: 0.002s, episode steps: 11, steps per second: 5108, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.119 [-1.723, 2.790], mean_best_reward: --
 16070/100000: episode: 839, duration: 0.002s, episode steps: 11, steps per second: 5131, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.124 [-1.355, 2.218], mean_best_reward: --
 16080/100000: episode: 840, duration: 0.002s, episode steps: 10, steps per second: 5056, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.115 [-1.197, 1.791], mean_best_reward: --
 16096/100000: episode: 841, duration: 0.003s, episode steps: 16, steps per second: 5522, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.042 [-2.926, 1.997], mean_best_reward: --
 16108/100000: episode: 842, duration: 0.002s, episode steps: 12, steps per second: 5164, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.111 [-2.458, 1.523], mean_best_reward: --
 16125/100000: episode: 843, duration: 0.003s, episode steps: 17, steps per second: 5148, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.065 [-1.980, 1.358], mean_best_reward: --
 16136/100000: episode: 844, duration: 0.002s, episode steps: 11, steps per second: 5137, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.139 [-2.333, 1.338], mean_best_reward: --
 16171/100000: episode: 845, duration: 0.006s, episode steps: 35, steps per second: 6030, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.119 [-1.285, 0.556], mean_best_reward: --
 16182/100000: episode: 846, duration: 0.002s, episode steps: 11, steps per second: 5013, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.123 [-1.837, 1.006], mean_best_reward: --
 16202/100000: episode: 847, duration: 0.004s, episode steps: 20, steps per second: 5621, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.053 [-1.643, 1.010], mean_best_reward: --
 16215/100000: episode: 848, duration: 0.003s, episode steps: 13, steps per second: 5112, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.095 [-2.757, 1.787], mean_best_reward: --
 16229/100000: episode: 849, duration: 0.003s, episode steps: 14, steps per second: 4635, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.105 [-1.559, 0.818], mean_best_reward: --
 16243/100000: episode: 850, duration: 0.003s, episode steps: 14, steps per second: 5279, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.101 [-0.816, 1.558], mean_best_reward: --
 16252/100000: episode: 851, duration: 0.002s, episode steps: 9, steps per second: 4125, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.123 [-1.726, 0.964], mean_best_reward: 68.500000
 16284/100000: episode: 852, duration: 0.005s, episode steps: 32, steps per second: 5842, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.041 [-1.323, 0.767], mean_best_reward: --
 16305/100000: episode: 853, duration: 0.004s, episode steps: 21, steps per second: 5699, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.059 [-1.860, 0.992], mean_best_reward: --
 16320/100000: episode: 854, duration: 0.003s, episode steps: 15, steps per second: 5465, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.083 [-1.391, 2.261], mean_best_reward: --
 16333/100000: episode: 855, duration: 0.003s, episode steps: 13, steps per second: 5144, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.081 [-0.993, 1.419], mean_best_reward: --
 16345/100000: episode: 856, duration: 0.002s, episode steps: 12, steps per second: 5234, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.101 [-2.046, 1.230], mean_best_reward: --
 16362/100000: episode: 857, duration: 0.003s, episode steps: 17, steps per second: 5515, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.105 [-0.592, 1.067], mean_best_reward: --
 16375/100000: episode: 858, duration: 0.002s, episode steps: 13, steps per second: 5240, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.086 [-1.391, 0.811], mean_best_reward: --
 16397/100000: episode: 859, duration: 0.004s, episode steps: 22, steps per second: 5730, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.449, 0.896], mean_best_reward: --
 16408/100000: episode: 860, duration: 0.002s, episode steps: 11, steps per second: 5149, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.148 [-1.733, 2.801], mean_best_reward: --
 16421/100000: episode: 861, duration: 0.002s, episode steps: 13, steps per second: 5318, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.120 [-1.923, 2.951], mean_best_reward: --
 16434/100000: episode: 862, duration: 0.002s, episode steps: 13, steps per second: 5352, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.125 [-1.713, 2.770], mean_best_reward: --
 16453/100000: episode: 863, duration: 0.003s, episode steps: 19, steps per second: 5539, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.064 [-1.231, 2.086], mean_best_reward: --
 16468/100000: episode: 864, duration: 0.003s, episode steps: 15, steps per second: 4595, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.083 [-1.363, 2.195], mean_best_reward: --
 16484/100000: episode: 865, duration: 0.003s, episode steps: 16, steps per second: 5456, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.070 [-2.531, 1.596], mean_best_reward: --
 16504/100000: episode: 866, duration: 0.004s, episode steps: 20, steps per second: 5561, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.005 [-2.699, 1.963], mean_best_reward: --
 16515/100000: episode: 867, duration: 0.002s, episode steps: 11, steps per second: 4819, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.096 [-2.770, 1.796], mean_best_reward: --
 16525/100000: episode: 868, duration: 0.002s, episode steps: 10, steps per second: 4072, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.222, 1.364], mean_best_reward: --
 16537/100000: episode: 869, duration: 0.002s, episode steps: 12, steps per second: 5025, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.103 [-1.227, 2.003], mean_best_reward: --
 16554/100000: episode: 870, duration: 0.003s, episode steps: 17, steps per second: 5412, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.091 [-1.993, 1.180], mean_best_reward: --
 16575/100000: episode: 871, duration: 0.004s, episode steps: 21, steps per second: 5701, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.112 [-0.585, 1.446], mean_best_reward: --
 16603/100000: episode: 872, duration: 0.005s, episode steps: 28, steps per second: 5632, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.026 [-0.996, 1.613], mean_best_reward: --
 16614/100000: episode: 873, duration: 0.002s, episode steps: 11, steps per second: 5003, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.114 [-2.193, 1.359], mean_best_reward: --
 16626/100000: episode: 874, duration: 0.002s, episode steps: 12, steps per second: 5228, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.109 [-1.539, 2.465], mean_best_reward: --
 16641/100000: episode: 875, duration: 0.003s, episode steps: 15, steps per second: 5442, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.080 [-1.474, 0.828], mean_best_reward: --
 16653/100000: episode: 876, duration: 0.002s, episode steps: 12, steps per second: 5235, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.124 [-1.395, 2.214], mean_best_reward: --
 16679/100000: episode: 877, duration: 0.004s, episode steps: 26, steps per second: 5865, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.047 [-1.576, 0.958], mean_best_reward: --
 16690/100000: episode: 878, duration: 0.002s, episode steps: 11, steps per second: 4749, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.129 [-2.863, 1.802], mean_best_reward: --
 16700/100000: episode: 879, duration: 0.002s, episode steps: 10, steps per second: 4736, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.155 [-2.604, 1.571], mean_best_reward: --
 16718/100000: episode: 880, duration: 0.004s, episode steps: 18, steps per second: 4802, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.064 [-2.619, 1.609], mean_best_reward: --
 16728/100000: episode: 881, duration: 0.002s, episode steps: 10, steps per second: 4831, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.567, 2.530], mean_best_reward: --
 16746/100000: episode: 882, duration: 0.003s, episode steps: 18, steps per second: 5534, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.085 [-1.595, 0.784], mean_best_reward: --
 16756/100000: episode: 883, duration: 0.002s, episode steps: 10, steps per second: 4972, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.129 [-2.583, 1.553], mean_best_reward: --
 16767/100000: episode: 884, duration: 0.002s, episode steps: 11, steps per second: 5060, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.132 [-1.358, 2.324], mean_best_reward: --
 16781/100000: episode: 885, duration: 0.003s, episode steps: 14, steps per second: 5385, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.084 [-1.565, 2.353], mean_best_reward: --
 16800/100000: episode: 886, duration: 0.003s, episode steps: 19, steps per second: 5667, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.263 [0.000, 1.000], mean observation: 0.079 [-1.747, 2.896], mean_best_reward: --
 16811/100000: episode: 887, duration: 0.002s, episode steps: 11, steps per second: 5169, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.111 [-1.257, 0.742], mean_best_reward: --
 16822/100000: episode: 888, duration: 0.002s, episode steps: 11, steps per second: 5177, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.137 [-1.715, 2.706], mean_best_reward: --
 16832/100000: episode: 889, duration: 0.002s, episode steps: 10, steps per second: 5042, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.144 [-1.567, 2.576], mean_best_reward: --
 16842/100000: episode: 890, duration: 0.002s, episode steps: 10, steps per second: 4983, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.979, 3.137], mean_best_reward: --
 16858/100000: episode: 891, duration: 0.003s, episode steps: 16, steps per second: 5526, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.097 [-0.960, 1.724], mean_best_reward: --
 16870/100000: episode: 892, duration: 0.002s, episode steps: 12, steps per second: 5253, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.096 [-2.932, 1.952], mean_best_reward: --
 16888/100000: episode: 893, duration: 0.003s, episode steps: 18, steps per second: 5628, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.095 [-0.964, 1.782], mean_best_reward: --
 16899/100000: episode: 894, duration: 0.002s, episode steps: 11, steps per second: 5176, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.124 [-0.799, 1.477], mean_best_reward: --
 16923/100000: episode: 895, duration: 0.005s, episode steps: 24, steps per second: 5281, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.125 [-0.955, 0.538], mean_best_reward: --
 16940/100000: episode: 896, duration: 0.004s, episode steps: 17, steps per second: 4479, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.081 [-1.009, 1.849], mean_best_reward: --
 16958/100000: episode: 897, duration: 0.004s, episode steps: 18, steps per second: 5034, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.097 [-1.310, 0.561], mean_best_reward: --
 16979/100000: episode: 898, duration: 0.004s, episode steps: 21, steps per second: 5639, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.076 [-1.362, 0.621], mean_best_reward: --
 16993/100000: episode: 899, duration: 0.003s, episode steps: 14, steps per second: 5304, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.089 [-1.555, 2.506], mean_best_reward: --
 17001/100000: episode: 900, duration: 0.002s, episode steps: 8, steps per second: 4364, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.610, 2.569], mean_best_reward: --
 17029/100000: episode: 901, duration: 0.005s, episode steps: 28, steps per second: 5551, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.008 [-2.307, 1.586], mean_best_reward: 76.000000
 17039/100000: episode: 902, duration: 0.002s, episode steps: 10, steps per second: 5000, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-3.107, 1.985], mean_best_reward: --
 17065/100000: episode: 903, duration: 0.004s, episode steps: 26, steps per second: 5847, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.070 [-0.552, 1.131], mean_best_reward: --
 17134/100000: episode: 904, duration: 0.011s, episode steps: 69, steps per second: 6227, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.243 [-1.179, 1.874], mean_best_reward: --
 17152/100000: episode: 905, duration: 0.003s, episode steps: 18, steps per second: 5563, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.088 [-1.371, 0.762], mean_best_reward: --
 17197/100000: episode: 906, duration: 0.008s, episode steps: 45, steps per second: 5941, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.077 [-0.333, 0.814], mean_best_reward: --
 17216/100000: episode: 907, duration: 0.004s, episode steps: 19, steps per second: 5165, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.069 [-1.181, 1.957], mean_best_reward: --
 17273/100000: episode: 908, duration: 0.009s, episode steps: 57, steps per second: 6170, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.012 [-0.750, 1.111], mean_best_reward: --
 17288/100000: episode: 909, duration: 0.003s, episode steps: 15, steps per second: 5311, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.090 [-2.290, 1.385], mean_best_reward: --
 17314/100000: episode: 910, duration: 0.004s, episode steps: 26, steps per second: 5796, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.023 [-1.009, 1.490], mean_best_reward: --
 17336/100000: episode: 911, duration: 0.004s, episode steps: 22, steps per second: 5720, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.781, 1.093], mean_best_reward: --
 17380/100000: episode: 912, duration: 0.008s, episode steps: 44, steps per second: 5810, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.045 [-1.474, 0.631], mean_best_reward: --
 17400/100000: episode: 913, duration: 0.004s, episode steps: 20, steps per second: 5625, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.067 [-2.222, 1.342], mean_best_reward: --
 17451/100000: episode: 914, duration: 0.008s, episode steps: 51, steps per second: 6173, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.009 [-1.189, 0.748], mean_best_reward: --
 17555/100000: episode: 915, duration: 0.017s, episode steps: 104, steps per second: 6017, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.495 [-2.980, 0.880], mean_best_reward: --
 17571/100000: episode: 916, duration: 0.003s, episode steps: 16, steps per second: 5331, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.104 [-0.630, 1.317], mean_best_reward: --
 17632/100000: episode: 917, duration: 0.010s, episode steps: 61, steps per second: 6249, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.028 [-0.940, 0.550], mean_best_reward: --
 17711/100000: episode: 918, duration: 0.013s, episode steps: 79, steps per second: 6146, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.081 [-0.605, 1.682], mean_best_reward: --
 17754/100000: episode: 919, duration: 0.007s, episode steps: 43, steps per second: 5778, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.056 [-0.635, 0.927], mean_best_reward: --
 17767/100000: episode: 920, duration: 0.003s, episode steps: 13, steps per second: 4969, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.101 [-1.770, 2.772], mean_best_reward: --
 17781/100000: episode: 921, duration: 0.003s, episode steps: 14, steps per second: 4823, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.109 [-2.642, 1.540], mean_best_reward: --
 17823/100000: episode: 922, duration: 0.007s, episode steps: 42, steps per second: 6066, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.034 [-1.153, 2.093], mean_best_reward: --
 17836/100000: episode: 923, duration: 0.003s, episode steps: 13, steps per second: 4640, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.130 [-1.369, 2.375], mean_best_reward: --
 17888/100000: episode: 924, duration: 0.008s, episode steps: 52, steps per second: 6163, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.043 [-0.902, 0.604], mean_best_reward: --
 17909/100000: episode: 925, duration: 0.004s, episode steps: 21, steps per second: 5692, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.077 [-0.588, 1.123], mean_best_reward: --
 17921/100000: episode: 926, duration: 0.003s, episode steps: 12, steps per second: 4707, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.100 [-1.005, 1.672], mean_best_reward: --
 17936/100000: episode: 927, duration: 0.003s, episode steps: 15, steps per second: 5308, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.128 [-0.552, 1.095], mean_best_reward: --
 17947/100000: episode: 928, duration: 0.002s, episode steps: 11, steps per second: 5115, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.151 [-2.377, 1.341], mean_best_reward: --
 18009/100000: episode: 929, duration: 0.010s, episode steps: 62, steps per second: 6251, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.203 [-0.426, 1.052], mean_best_reward: --
 18023/100000: episode: 930, duration: 0.003s, episode steps: 14, steps per second: 4951, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.095 [-2.064, 1.211], mean_best_reward: --
 18078/100000: episode: 931, duration: 0.009s, episode steps: 55, steps per second: 5863, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.021 [-0.839, 1.553], mean_best_reward: --
 18090/100000: episode: 932, duration: 0.002s, episode steps: 12, steps per second: 5116, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.108 [-1.651, 1.021], mean_best_reward: --
 18152/100000: episode: 933, duration: 0.010s, episode steps: 62, steps per second: 6178, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.438, 0.613], mean_best_reward: --
 18239/100000: episode: 934, duration: 0.014s, episode steps: 87, steps per second: 6285, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.257 [-1.839, 1.174], mean_best_reward: --
 18280/100000: episode: 935, duration: 0.007s, episode steps: 41, steps per second: 6030, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.152 [-1.213, 0.368], mean_best_reward: --
 18349/100000: episode: 936, duration: 0.012s, episode steps: 69, steps per second: 5880, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.102 [-1.544, 1.546], mean_best_reward: --
 18383/100000: episode: 937, duration: 0.006s, episode steps: 34, steps per second: 5971, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.051 [-0.963, 0.580], mean_best_reward: --
 18393/100000: episode: 938, duration: 0.002s, episode steps: 10, steps per second: 4234, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.386, 1.598], mean_best_reward: --
 18405/100000: episode: 939, duration: 0.002s, episode steps: 12, steps per second: 5126, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.123 [-1.988, 1.167], mean_best_reward: --
 18415/100000: episode: 940, duration: 0.002s, episode steps: 10, steps per second: 5004, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.117 [-2.726, 1.805], mean_best_reward: --
 18429/100000: episode: 941, duration: 0.003s, episode steps: 14, steps per second: 5381, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.097 [-1.955, 1.142], mean_best_reward: --
 18443/100000: episode: 942, duration: 0.003s, episode steps: 14, steps per second: 5363, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.092 [-2.984, 1.928], mean_best_reward: --
 18454/100000: episode: 943, duration: 0.002s, episode steps: 11, steps per second: 5118, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.129 [-1.393, 2.318], mean_best_reward: --
 18463/100000: episode: 944, duration: 0.002s, episode steps: 9, steps per second: 4769, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.128 [-1.879, 1.194], mean_best_reward: --
 18496/100000: episode: 945, duration: 0.006s, episode steps: 33, steps per second: 5836, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.150 [-0.380, 0.879], mean_best_reward: --
 18511/100000: episode: 946, duration: 0.003s, episode steps: 15, steps per second: 5394, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.090 [-1.398, 2.288], mean_best_reward: --
 18523/100000: episode: 947, duration: 0.002s, episode steps: 12, steps per second: 5212, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.101 [-2.015, 1.172], mean_best_reward: --
 18538/100000: episode: 948, duration: 0.003s, episode steps: 15, steps per second: 5418, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.089 [-0.818, 1.257], mean_best_reward: --
 18568/100000: episode: 949, duration: 0.005s, episode steps: 30, steps per second: 5535, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.064 [-0.584, 1.293], mean_best_reward: --
 18645/100000: episode: 950, duration: 0.013s, episode steps: 77, steps per second: 5974, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.057 [-2.267, 2.195], mean_best_reward: --
 18659/100000: episode: 951, duration: 0.003s, episode steps: 14, steps per second: 4294, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.101 [-1.331, 2.183], mean_best_reward: 52.000000
 18679/100000: episode: 952, duration: 0.004s, episode steps: 20, steps per second: 5604, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.088 [-1.246, 0.742], mean_best_reward: --
 18689/100000: episode: 953, duration: 0.002s, episode steps: 10, steps per second: 5028, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.109 [-1.670, 1.028], mean_best_reward: --
 18783/100000: episode: 954, duration: 0.015s, episode steps: 94, steps per second: 6311, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.021 [-2.099, 2.036], mean_best_reward: --
 18800/100000: episode: 955, duration: 0.003s, episode steps: 17, steps per second: 5355, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.056 [-1.541, 2.359], mean_best_reward: --
 18864/100000: episode: 956, duration: 0.012s, episode steps: 64, steps per second: 5462, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.216 [-1.468, 0.761], mean_best_reward: --
 18883/100000: episode: 957, duration: 0.004s, episode steps: 19, steps per second: 5337, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.108 [-0.973, 0.391], mean_best_reward: --
 18902/100000: episode: 958, duration: 0.003s, episode steps: 19, steps per second: 5567, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.108 [-1.254, 0.745], mean_best_reward: --
 18912/100000: episode: 959, duration: 0.002s, episode steps: 10, steps per second: 4853, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.557, 2.574], mean_best_reward: --
 18923/100000: episode: 960, duration: 0.002s, episode steps: 11, steps per second: 4997, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.113 [-2.442, 1.546], mean_best_reward: --
 18935/100000: episode: 961, duration: 0.002s, episode steps: 12, steps per second: 5213, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.122 [-1.738, 0.966], mean_best_reward: --
 18953/100000: episode: 962, duration: 0.003s, episode steps: 18, steps per second: 5602, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.095 [-0.830, 1.712], mean_best_reward: --
 18967/100000: episode: 963, duration: 0.003s, episode steps: 14, steps per second: 5347, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.095 [-1.196, 2.033], mean_best_reward: --
 18992/100000: episode: 964, duration: 0.004s, episode steps: 25, steps per second: 5729, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.105 [-1.507, 0.703], mean_best_reward: --
 19006/100000: episode: 965, duration: 0.003s, episode steps: 14, steps per second: 5236, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.115 [-0.947, 1.692], mean_best_reward: --
 19023/100000: episode: 966, duration: 0.003s, episode steps: 17, steps per second: 5512, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.075 [-0.962, 1.447], mean_best_reward: --
 19037/100000: episode: 967, duration: 0.003s, episode steps: 14, steps per second: 5351, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.098 [-1.921, 2.972], mean_best_reward: --
 19047/100000: episode: 968, duration: 0.002s, episode steps: 10, steps per second: 5041, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.574, 2.581], mean_best_reward: --
 19059/100000: episode: 969, duration: 0.002s, episode steps: 12, steps per second: 5115, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.082 [-0.836, 1.464], mean_best_reward: --
 19074/100000: episode: 970, duration: 0.003s, episode steps: 15, steps per second: 5372, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.123 [-0.562, 1.198], mean_best_reward: --
 19084/100000: episode: 971, duration: 0.002s, episode steps: 10, steps per second: 4762, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.143 [-1.764, 2.739], mean_best_reward: --
 19095/100000: episode: 972, duration: 0.002s, episode steps: 11, steps per second: 4874, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.105 [-2.929, 1.985], mean_best_reward: --
 19106/100000: episode: 973, duration: 0.003s, episode steps: 11, steps per second: 4238, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.110 [-1.414, 2.270], mean_best_reward: --
 19116/100000: episode: 974, duration: 0.002s, episode steps: 10, steps per second: 4920, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.521, 2.515], mean_best_reward: --
 19129/100000: episode: 975, duration: 0.002s, episode steps: 13, steps per second: 5276, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.081 [-1.756, 2.678], mean_best_reward: --
 19138/100000: episode: 976, duration: 0.002s, episode steps: 9, steps per second: 4399, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.147 [-1.148, 1.956], mean_best_reward: --
 19153/100000: episode: 977, duration: 0.003s, episode steps: 15, steps per second: 5197, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.109 [-1.788, 0.958], mean_best_reward: --
 19170/100000: episode: 978, duration: 0.003s, episode steps: 17, steps per second: 5498, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.098 [-1.539, 0.756], mean_best_reward: --
 19183/100000: episode: 979, duration: 0.003s, episode steps: 13, steps per second: 5152, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.093 [-1.745, 2.715], mean_best_reward: --
 19203/100000: episode: 980, duration: 0.004s, episode steps: 20, steps per second: 5591, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.071 [-1.243, 0.748], mean_best_reward: --
 19216/100000: episode: 981, duration: 0.002s, episode steps: 13, steps per second: 5276, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.100 [-0.786, 1.340], mean_best_reward: --
 19234/100000: episode: 982, duration: 0.004s, episode steps: 18, steps per second: 5133, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.126 [-0.565, 1.380], mean_best_reward: --
 19245/100000: episode: 983, duration: 0.002s, episode steps: 11, steps per second: 5109, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.110 [-1.687, 0.953], mean_best_reward: --
 19258/100000: episode: 984, duration: 0.002s, episode steps: 13, steps per second: 5259, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.082 [-1.015, 1.651], mean_best_reward: --
 19276/100000: episode: 985, duration: 0.003s, episode steps: 18, steps per second: 5476, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.072 [-1.917, 1.156], mean_best_reward: --
 19296/100000: episode: 986, duration: 0.004s, episode steps: 20, steps per second: 5654, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.079 [-1.488, 0.754], mean_best_reward: --
 19306/100000: episode: 987, duration: 0.002s, episode steps: 10, steps per second: 4300, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.135 [-1.881, 1.138], mean_best_reward: --
 19315/100000: episode: 988, duration: 0.002s, episode steps: 9, steps per second: 4501, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.144 [-0.997, 1.748], mean_best_reward: --
 19348/100000: episode: 989, duration: 0.006s, episode steps: 33, steps per second: 5672, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.086 [-1.006, 0.446], mean_best_reward: --
 19359/100000: episode: 990, duration: 0.002s, episode steps: 11, steps per second: 5090, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.107 [-1.794, 2.785], mean_best_reward: --
 19379/100000: episode: 991, duration: 0.004s, episode steps: 20, steps per second: 5504, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.060 [-0.937, 1.445], mean_best_reward: --
 19390/100000: episode: 992, duration: 0.002s, episode steps: 11, steps per second: 4929, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.735, 0.942], mean_best_reward: --
 19435/100000: episode: 993, duration: 0.007s, episode steps: 45, steps per second: 6172, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.081 [-0.516, 0.782], mean_best_reward: --
 19446/100000: episode: 994, duration: 0.002s, episode steps: 11, steps per second: 5163, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.105 [-1.887, 1.186], mean_best_reward: --
 19460/100000: episode: 995, duration: 0.003s, episode steps: 14, steps per second: 5068, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.124 [-2.115, 1.178], mean_best_reward: --
 19486/100000: episode: 996, duration: 0.005s, episode steps: 26, steps per second: 5731, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.580, 1.005], mean_best_reward: --
 19507/100000: episode: 997, duration: 0.004s, episode steps: 21, steps per second: 5690, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.050 [-1.754, 2.756], mean_best_reward: --
 19533/100000: episode: 998, duration: 0.004s, episode steps: 26, steps per second: 5828, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.083 [-0.752, 0.442], mean_best_reward: --
 19553/100000: episode: 999, duration: 0.004s, episode steps: 20, steps per second: 5648, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.084 [-2.132, 1.151], mean_best_reward: --
 19570/100000: episode: 1000, duration: 0.003s, episode steps: 17, steps per second: 5411, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.076 [-1.725, 1.181], mean_best_reward: --
 19608/100000: episode: 1001, duration: 0.007s, episode steps: 38, steps per second: 5379, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.092 [-1.069, 0.443], mean_best_reward: 100.000000
 19632/100000: episode: 1002, duration: 0.004s, episode steps: 24, steps per second: 5723, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-0.967, 0.581], mean_best_reward: --
 19642/100000: episode: 1003, duration: 0.002s, episode steps: 10, steps per second: 4884, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.129 [-1.854, 1.194], mean_best_reward: --
 19653/100000: episode: 1004, duration: 0.002s, episode steps: 11, steps per second: 5066, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.107 [-1.600, 2.496], mean_best_reward: --
 19713/100000: episode: 1005, duration: 0.010s, episode steps: 60, steps per second: 6172, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.079 [-0.535, 0.982], mean_best_reward: --
 19731/100000: episode: 1006, duration: 0.003s, episode steps: 18, steps per second: 5455, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.073 [-1.182, 0.805], mean_best_reward: --
 19758/100000: episode: 1007, duration: 0.005s, episode steps: 27, steps per second: 5816, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.134 [-0.579, 0.979], mean_best_reward: --
 19819/100000: episode: 1008, duration: 0.010s, episode steps: 61, steps per second: 6210, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.074 [-0.571, 1.214], mean_best_reward: --
 19828/100000: episode: 1009, duration: 0.002s, episode steps: 9, steps per second: 4272, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.167 [-2.309, 1.359], mean_best_reward: --
 19845/100000: episode: 1010, duration: 0.003s, episode steps: 17, steps per second: 5217, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.084 [-1.717, 2.789], mean_best_reward: --
 19860/100000: episode: 1011, duration: 0.003s, episode steps: 15, steps per second: 4689, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.113 [-1.252, 0.593], mean_best_reward: --
 19880/100000: episode: 1012, duration: 0.004s, episode steps: 20, steps per second: 5680, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.060 [-1.771, 2.629], mean_best_reward: --
 19902/100000: episode: 1013, duration: 0.004s, episode steps: 22, steps per second: 5733, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.080 [-1.306, 0.574], mean_best_reward: --
 19923/100000: episode: 1014, duration: 0.004s, episode steps: 21, steps per second: 5617, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.059 [-1.361, 2.258], mean_best_reward: --
 19943/100000: episode: 1015, duration: 0.004s, episode steps: 20, steps per second: 5564, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-0.600, 0.938], mean_best_reward: --
 19954/100000: episode: 1016, duration: 0.002s, episode steps: 11, steps per second: 5118, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.126 [-1.761, 2.786], mean_best_reward: --
 19974/100000: episode: 1017, duration: 0.004s, episode steps: 20, steps per second: 5659, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.073 [-0.775, 1.523], mean_best_reward: --
 19988/100000: episode: 1018, duration: 0.003s, episode steps: 14, steps per second: 5229, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.128 [-2.272, 1.325], mean_best_reward: --
 20005/100000: episode: 1019, duration: 0.003s, episode steps: 17, steps per second: 5342, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.077 [-0.802, 1.168], mean_best_reward: --
 20019/100000: episode: 1020, duration: 0.003s, episode steps: 14, steps per second: 5391, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.100 [-1.601, 0.797], mean_best_reward: --
 20044/100000: episode: 1021, duration: 0.004s, episode steps: 25, steps per second: 5820, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.067 [-1.748, 0.940], mean_best_reward: --
 20054/100000: episode: 1022, duration: 0.002s, episode steps: 10, steps per second: 4727, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.116 [-1.900, 1.183], mean_best_reward: --
 20068/100000: episode: 1023, duration: 0.003s, episode steps: 14, steps per second: 5370, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.095 [-2.443, 1.544], mean_best_reward: --
 20078/100000: episode: 1024, duration: 0.002s, episode steps: 10, steps per second: 4329, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.116 [-1.584, 2.514], mean_best_reward: --
 20124/100000: episode: 1025, duration: 0.008s, episode steps: 46, steps per second: 5939, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.131 [-1.461, 1.537], mean_best_reward: --
 20137/100000: episode: 1026, duration: 0.002s, episode steps: 13, steps per second: 5268, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.113 [-2.299, 1.415], mean_best_reward: --
 20149/100000: episode: 1027, duration: 0.002s, episode steps: 12, steps per second: 5189, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.715, 0.982], mean_best_reward: --
 20174/100000: episode: 1028, duration: 0.005s, episode steps: 25, steps per second: 5365, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.080 [-1.050, 0.596], mean_best_reward: --
 20198/100000: episode: 1029, duration: 0.004s, episode steps: 24, steps per second: 5812, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.091 [-0.757, 1.690], mean_best_reward: --
 20219/100000: episode: 1030, duration: 0.004s, episode steps: 21, steps per second: 5703, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.115 [-1.042, 0.363], mean_best_reward: --
 20233/100000: episode: 1031, duration: 0.003s, episode steps: 14, steps per second: 5365, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.095 [-2.605, 1.555], mean_best_reward: --
 20245/100000: episode: 1032, duration: 0.003s, episode steps: 12, steps per second: 4478, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.120 [-1.606, 2.544], mean_best_reward: --
 20261/100000: episode: 1033, duration: 0.003s, episode steps: 16, steps per second: 5448, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.093 [-2.278, 1.361], mean_best_reward: --
 20276/100000: episode: 1034, duration: 0.003s, episode steps: 15, steps per second: 5448, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.102 [-1.428, 0.809], mean_best_reward: --
 20310/100000: episode: 1035, duration: 0.006s, episode steps: 34, steps per second: 5976, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.382 [0.000, 1.000], mean observation: -0.011 [-1.570, 2.091], mean_best_reward: --
 20337/100000: episode: 1036, duration: 0.005s, episode steps: 27, steps per second: 5398, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.083 [-1.242, 0.533], mean_best_reward: --
 20349/100000: episode: 1037, duration: 0.002s, episode steps: 12, steps per second: 5025, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.091 [-1.299, 0.798], mean_best_reward: --
 20358/100000: episode: 1038, duration: 0.002s, episode steps: 9, steps per second: 4017, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-2.758, 1.776], mean_best_reward: --
 20368/100000: episode: 1039, duration: 0.002s, episode steps: 10, steps per second: 5019, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.116 [-1.026, 1.547], mean_best_reward: --
 20387/100000: episode: 1040, duration: 0.003s, episode steps: 19, steps per second: 5547, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.073 [-0.614, 1.339], mean_best_reward: --
 20398/100000: episode: 1041, duration: 0.002s, episode steps: 11, steps per second: 5128, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.101 [-2.786, 1.784], mean_best_reward: --
 20411/100000: episode: 1042, duration: 0.003s, episode steps: 13, steps per second: 4616, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.118 [-1.134, 1.903], mean_best_reward: --
 20483/100000: episode: 1043, duration: 0.012s, episode steps: 72, steps per second: 6239, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.086 [-2.334, 2.428], mean_best_reward: --
 20495/100000: episode: 1044, duration: 0.003s, episode steps: 12, steps per second: 4562, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-1.016, 1.657], mean_best_reward: --
 20513/100000: episode: 1045, duration: 0.003s, episode steps: 18, steps per second: 5456, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.073 [-1.529, 2.422], mean_best_reward: --
 20526/100000: episode: 1046, duration: 0.002s, episode steps: 13, steps per second: 5303, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.096 [-1.098, 0.610], mean_best_reward: --
 20536/100000: episode: 1047, duration: 0.002s, episode steps: 10, steps per second: 5036, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.147 [-2.538, 1.518], mean_best_reward: --
 20555/100000: episode: 1048, duration: 0.003s, episode steps: 19, steps per second: 5633, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.103 [-0.553, 1.078], mean_best_reward: --
 20567/100000: episode: 1049, duration: 0.002s, episode steps: 12, steps per second: 5129, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.095 [-1.416, 2.177], mean_best_reward: --
 20580/100000: episode: 1050, duration: 0.003s, episode steps: 13, steps per second: 4679, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.095 [-2.817, 1.790], mean_best_reward: --
 20595/100000: episode: 1051, duration: 0.003s, episode steps: 15, steps per second: 4502, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.097 [-0.990, 1.503], mean_best_reward: 54.500000
 20613/100000: episode: 1052, duration: 0.004s, episode steps: 18, steps per second: 5037, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.049 [-1.371, 2.055], mean_best_reward: --
 20624/100000: episode: 1053, duration: 0.002s, episode steps: 11, steps per second: 5149, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.119 [-1.380, 2.322], mean_best_reward: --
 20646/100000: episode: 1054, duration: 0.004s, episode steps: 22, steps per second: 5686, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.170, 0.809], mean_best_reward: --
 20661/100000: episode: 1055, duration: 0.003s, episode steps: 15, steps per second: 4813, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.124 [-1.326, 2.345], mean_best_reward: --
 20672/100000: episode: 1056, duration: 0.002s, episode steps: 11, steps per second: 5138, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.134 [-1.179, 1.993], mean_best_reward: --
 20687/100000: episode: 1057, duration: 0.003s, episode steps: 15, steps per second: 5450, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.091 [-1.142, 1.944], mean_best_reward: --
 20697/100000: episode: 1058, duration: 0.002s, episode steps: 10, steps per second: 5012, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.133 [-2.740, 1.748], mean_best_reward: --
 20725/100000: episode: 1059, duration: 0.005s, episode steps: 28, steps per second: 5884, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.026 [-2.281, 3.208], mean_best_reward: --
 20743/100000: episode: 1060, duration: 0.004s, episode steps: 18, steps per second: 5019, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.096 [-2.112, 1.163], mean_best_reward: --
 20760/100000: episode: 1061, duration: 0.003s, episode steps: 17, steps per second: 5504, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.063 [-1.184, 1.924], mean_best_reward: --
 20777/100000: episode: 1062, duration: 0.003s, episode steps: 17, steps per second: 5530, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.094 [-1.406, 2.370], mean_best_reward: --
 20791/100000: episode: 1063, duration: 0.003s, episode steps: 14, steps per second: 5379, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.102 [-1.550, 2.517], mean_best_reward: --
 20848/100000: episode: 1064, duration: 0.010s, episode steps: 57, steps per second: 5658, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.199 [-1.437, 0.832], mean_best_reward: --
 20872/100000: episode: 1065, duration: 0.004s, episode steps: 24, steps per second: 5670, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.113 [-0.770, 1.250], mean_best_reward: --
 20884/100000: episode: 1066, duration: 0.002s, episode steps: 12, steps per second: 5207, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.097 [-1.607, 2.520], mean_best_reward: --
 20897/100000: episode: 1067, duration: 0.002s, episode steps: 13, steps per second: 5286, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.075 [-1.569, 2.364], mean_best_reward: --
 20916/100000: episode: 1068, duration: 0.004s, episode steps: 19, steps per second: 5109, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.049 [-1.412, 2.246], mean_best_reward: --
 20929/100000: episode: 1069, duration: 0.002s, episode steps: 13, steps per second: 5303, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.135 [-1.305, 0.752], mean_best_reward: --
 20955/100000: episode: 1070, duration: 0.004s, episode steps: 26, steps per second: 5842, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.077 [-2.152, 1.148], mean_best_reward: --
 20969/100000: episode: 1071, duration: 0.003s, episode steps: 14, steps per second: 5380, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.125 [-1.531, 2.620], mean_best_reward: --
 20983/100000: episode: 1072, duration: 0.003s, episode steps: 14, steps per second: 5367, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.094 [-1.833, 1.158], mean_best_reward: --
 20991/100000: episode: 1073, duration: 0.002s, episode steps: 8, steps per second: 4538, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.528, 2.519], mean_best_reward: --
 21162/100000: episode: 1074, duration: 0.027s, episode steps: 171, steps per second: 6383, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.373 [-2.436, 1.197], mean_best_reward: --
 21185/100000: episode: 1075, duration: 0.004s, episode steps: 23, steps per second: 5642, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.050 [-2.348, 1.424], mean_best_reward: --
 21291/100000: episode: 1076, duration: 0.017s, episode steps: 106, steps per second: 6242, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.044 [-1.307, 0.919], mean_best_reward: --
 21307/100000: episode: 1077, duration: 0.003s, episode steps: 16, steps per second: 5433, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.068 [-1.030, 1.623], mean_best_reward: --
 21317/100000: episode: 1078, duration: 0.002s, episode steps: 10, steps per second: 4934, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.909, 2.998], mean_best_reward: --
 21346/100000: episode: 1079, duration: 0.005s, episode steps: 29, steps per second: 5745, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.048 [-1.622, 1.002], mean_best_reward: --
 21357/100000: episode: 1080, duration: 0.002s, episode steps: 11, steps per second: 4952, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-1.377, 2.133], mean_best_reward: --
 21376/100000: episode: 1081, duration: 0.004s, episode steps: 19, steps per second: 5086, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.105 [-0.584, 1.495], mean_best_reward: --
 21387/100000: episode: 1082, duration: 0.002s, episode steps: 11, steps per second: 4860, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.088 [-1.421, 2.219], mean_best_reward: --
 21397/100000: episode: 1083, duration: 0.002s, episode steps: 10, steps per second: 4328, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.130 [-1.405, 2.196], mean_best_reward: --
 21411/100000: episode: 1084, duration: 0.003s, episode steps: 14, steps per second: 5341, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.096 [-1.962, 3.099], mean_best_reward: --
 21443/100000: episode: 1085, duration: 0.005s, episode steps: 32, steps per second: 5954, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.585, 1.441], mean_best_reward: --
 21458/100000: episode: 1086, duration: 0.003s, episode steps: 15, steps per second: 4828, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.109 [-1.520, 2.551], mean_best_reward: --
 21487/100000: episode: 1087, duration: 0.005s, episode steps: 29, steps per second: 5918, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.133 [-0.801, 0.353], mean_best_reward: --
 21498/100000: episode: 1088, duration: 0.002s, episode steps: 11, steps per second: 5141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.116 [-1.515, 2.335], mean_best_reward: --
 21514/100000: episode: 1089, duration: 0.003s, episode steps: 16, steps per second: 5474, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.067 [-1.923, 2.948], mean_best_reward: --
 21532/100000: episode: 1090, duration: 0.003s, episode steps: 18, steps per second: 5568, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.064 [-1.940, 2.947], mean_best_reward: --
 21551/100000: episode: 1091, duration: 0.004s, episode steps: 19, steps per second: 5239, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.051 [-0.837, 1.364], mean_best_reward: --
 21561/100000: episode: 1092, duration: 0.002s, episode steps: 10, steps per second: 5023, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.543, 2.550], mean_best_reward: --
 21572/100000: episode: 1093, duration: 0.002s, episode steps: 11, steps per second: 5134, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.093 [-1.013, 1.711], mean_best_reward: --
 21594/100000: episode: 1094, duration: 0.004s, episode steps: 22, steps per second: 5740, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.042 [-1.343, 1.917], mean_best_reward: --
 21671/100000: episode: 1095, duration: 0.013s, episode steps: 77, steps per second: 5995, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.279 [-1.497, 0.564], mean_best_reward: --
 21684/100000: episode: 1096, duration: 0.002s, episode steps: 13, steps per second: 5263, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.111 [-1.418, 2.301], mean_best_reward: --
 21740/100000: episode: 1097, duration: 0.009s, episode steps: 56, steps per second: 6058, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: 0.029 [-2.262, 2.870], mean_best_reward: --
 21749/100000: episode: 1098, duration: 0.002s, episode steps: 9, steps per second: 4742, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.719, 2.843], mean_best_reward: --
 21760/100000: episode: 1099, duration: 0.002s, episode steps: 11, steps per second: 4849, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-1.144, 1.959], mean_best_reward: --
 21778/100000: episode: 1100, duration: 0.003s, episode steps: 18, steps per second: 5604, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.062 [-1.179, 1.792], mean_best_reward: --
 21795/100000: episode: 1101, duration: 0.003s, episode steps: 17, steps per second: 5036, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.103 [-1.354, 2.363], mean_best_reward: 62.000000
 21812/100000: episode: 1102, duration: 0.003s, episode steps: 17, steps per second: 5311, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.099 [-0.763, 1.540], mean_best_reward: --
 21835/100000: episode: 1103, duration: 0.004s, episode steps: 23, steps per second: 5747, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.109 [-0.548, 1.173], mean_best_reward: --
 21859/100000: episode: 1104, duration: 0.004s, episode steps: 24, steps per second: 5784, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.087 [-0.618, 1.437], mean_best_reward: --
 21920/100000: episode: 1105, duration: 0.011s, episode steps: 61, steps per second: 5780, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.111 [-1.212, 0.441], mean_best_reward: --
 21933/100000: episode: 1106, duration: 0.002s, episode steps: 13, steps per second: 5263, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.105 [-1.375, 2.232], mean_best_reward: --
 21965/100000: episode: 1107, duration: 0.005s, episode steps: 32, steps per second: 5952, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-0.995, 0.557], mean_best_reward: --
 21983/100000: episode: 1108, duration: 0.003s, episode steps: 18, steps per second: 5187, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.064 [-1.027, 1.660], mean_best_reward: --
 21993/100000: episode: 1109, duration: 0.002s, episode steps: 10, steps per second: 4905, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.129 [-2.174, 1.344], mean_best_reward: --
 22112/100000: episode: 1110, duration: 0.019s, episode steps: 119, steps per second: 6327, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.109 [-1.367, 0.657], mean_best_reward: --
 22138/100000: episode: 1111, duration: 0.004s, episode steps: 26, steps per second: 5801, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.553, 1.046], mean_best_reward: --
 22152/100000: episode: 1112, duration: 0.003s, episode steps: 14, steps per second: 5355, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.087 [-1.744, 1.199], mean_best_reward: --
 22161/100000: episode: 1113, duration: 0.002s, episode steps: 9, steps per second: 4390, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.136 [-1.163, 1.909], mean_best_reward: --
 22178/100000: episode: 1114, duration: 0.003s, episode steps: 17, steps per second: 5282, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.091 [-0.757, 1.485], mean_best_reward: --
 22186/100000: episode: 1115, duration: 0.002s, episode steps: 8, steps per second: 3846, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.562, 1.577], mean_best_reward: --
 22204/100000: episode: 1116, duration: 0.003s, episode steps: 18, steps per second: 5494, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.073 [-1.193, 2.050], mean_best_reward: --
 22217/100000: episode: 1117, duration: 0.003s, episode steps: 13, steps per second: 5078, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.099 [-1.587, 2.429], mean_best_reward: --
 22302/100000: episode: 1118, duration: 0.014s, episode steps: 85, steps per second: 6193, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.069 [-1.223, 0.795], mean_best_reward: --
 22320/100000: episode: 1119, duration: 0.003s, episode steps: 18, steps per second: 5556, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.074 [-1.526, 2.530], mean_best_reward: --
 22344/100000: episode: 1120, duration: 0.004s, episode steps: 24, steps per second: 5681, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.083 [-1.811, 0.980], mean_best_reward: --
 22367/100000: episode: 1121, duration: 0.004s, episode steps: 23, steps per second: 5774, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.062 [-1.018, 1.867], mean_best_reward: --
 22388/100000: episode: 1122, duration: 0.004s, episode steps: 21, steps per second: 5458, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.081 [-0.590, 1.303], mean_best_reward: --
 22440/100000: episode: 1123, duration: 0.009s, episode steps: 52, steps per second: 5976, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.170 [-1.141, 0.749], mean_best_reward: --
 22464/100000: episode: 1124, duration: 0.005s, episode steps: 24, steps per second: 5282, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.409, 1.131], mean_best_reward: --
 22476/100000: episode: 1125, duration: 0.002s, episode steps: 12, steps per second: 5179, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.125 [-0.935, 1.624], mean_best_reward: --
 22486/100000: episode: 1126, duration: 0.002s, episode steps: 10, steps per second: 4608, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.116 [-1.161, 1.847], mean_best_reward: --
 22497/100000: episode: 1127, duration: 0.002s, episode steps: 11, steps per second: 5108, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.105 [-1.213, 1.864], mean_best_reward: --
 22519/100000: episode: 1128, duration: 0.004s, episode steps: 22, steps per second: 5253, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.038 [-0.642, 1.225], mean_best_reward: --
 22562/100000: episode: 1129, duration: 0.007s, episode steps: 43, steps per second: 6126, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.035 [-1.050, 0.603], mean_best_reward: --
 22582/100000: episode: 1130, duration: 0.004s, episode steps: 20, steps per second: 5702, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.803, 1.157], mean_best_reward: --
 22598/100000: episode: 1131, duration: 0.003s, episode steps: 16, steps per second: 5000, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.098 [-1.544, 2.594], mean_best_reward: --
 22625/100000: episode: 1132, duration: 0.005s, episode steps: 27, steps per second: 5802, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.630 [0.000, 1.000], mean observation: -0.031 [-2.239, 1.373], mean_best_reward: --
 22646/100000: episode: 1133, duration: 0.004s, episode steps: 21, steps per second: 5720, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.075 [-1.142, 2.026], mean_best_reward: --
 22662/100000: episode: 1134, duration: 0.003s, episode steps: 16, steps per second: 5498, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.641, 1.185], mean_best_reward: --
 22682/100000: episode: 1135, duration: 0.004s, episode steps: 20, steps per second: 5692, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.553, 0.937], mean_best_reward: --
 22703/100000: episode: 1136, duration: 0.004s, episode steps: 21, steps per second: 5085, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.088 [-0.406, 0.938], mean_best_reward: --
 22746/100000: episode: 1137, duration: 0.007s, episode steps: 43, steps per second: 5905, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: -0.062 [-1.449, 0.758], mean_best_reward: --
 22761/100000: episode: 1138, duration: 0.003s, episode steps: 15, steps per second: 5335, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.085 [-1.654, 0.982], mean_best_reward: --
 22785/100000: episode: 1139, duration: 0.004s, episode steps: 24, steps per second: 5460, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.050 [-1.973, 1.201], mean_best_reward: --
 22804/100000: episode: 1140, duration: 0.003s, episode steps: 19, steps per second: 5581, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.068 [-0.633, 1.223], mean_best_reward: --
 22826/100000: episode: 1141, duration: 0.004s, episode steps: 22, steps per second: 5721, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.391, 1.003], mean_best_reward: --
 22844/100000: episode: 1142, duration: 0.003s, episode steps: 18, steps per second: 5590, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.100 [-1.683, 0.785], mean_best_reward: --
 22864/100000: episode: 1143, duration: 0.004s, episode steps: 20, steps per second: 5493, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.065 [-1.022, 1.764], mean_best_reward: --
 22891/100000: episode: 1144, duration: 0.005s, episode steps: 27, steps per second: 5829, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.061 [-1.277, 0.818], mean_best_reward: --
 22927/100000: episode: 1145, duration: 0.006s, episode steps: 36, steps per second: 5876, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.041 [-0.763, 1.304], mean_best_reward: --
 22959/100000: episode: 1146, duration: 0.006s, episode steps: 32, steps per second: 5725, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.080 [-1.632, 0.753], mean_best_reward: --
 22974/100000: episode: 1147, duration: 0.003s, episode steps: 15, steps per second: 4398, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.112 [-1.224, 0.554], mean_best_reward: --
 23012/100000: episode: 1148, duration: 0.006s, episode steps: 38, steps per second: 5999, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.036 [-1.395, 1.879], mean_best_reward: --
 23031/100000: episode: 1149, duration: 0.003s, episode steps: 19, steps per second: 5619, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.071 [-2.413, 1.392], mean_best_reward: --
 23049/100000: episode: 1150, duration: 0.003s, episode steps: 18, steps per second: 5471, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.099 [-1.312, 0.602], mean_best_reward: --
 23065/100000: episode: 1151, duration: 0.003s, episode steps: 16, steps per second: 4958, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.105 [-1.252, 0.610], mean_best_reward: 105.000000
 23076/100000: episode: 1152, duration: 0.002s, episode steps: 11, steps per second: 5105, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.099 [-1.414, 2.303], mean_best_reward: --
 23093/100000: episode: 1153, duration: 0.003s, episode steps: 17, steps per second: 5532, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.060 [-1.391, 0.829], mean_best_reward: --
 23104/100000: episode: 1154, duration: 0.002s, episode steps: 11, steps per second: 5100, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.126 [-1.426, 2.355], mean_best_reward: --
 23115/100000: episode: 1155, duration: 0.002s, episode steps: 11, steps per second: 4973, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.118 [-1.391, 2.336], mean_best_reward: --
 23140/100000: episode: 1156, duration: 0.004s, episode steps: 25, steps per second: 5761, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.280 [0.000, 1.000], mean observation: 0.051 [-2.155, 3.195], mean_best_reward: --
 23162/100000: episode: 1157, duration: 0.004s, episode steps: 22, steps per second: 5730, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.033 [-1.387, 2.077], mean_best_reward: --
 23174/100000: episode: 1158, duration: 0.002s, episode steps: 12, steps per second: 5199, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.119 [-1.775, 2.750], mean_best_reward: --
 23185/100000: episode: 1159, duration: 0.002s, episode steps: 11, steps per second: 5122, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.123 [-1.378, 2.286], mean_best_reward: --
 23198/100000: episode: 1160, duration: 0.003s, episode steps: 13, steps per second: 4889, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.092 [-1.214, 1.917], mean_best_reward: --
 23231/100000: episode: 1161, duration: 0.006s, episode steps: 33, steps per second: 5569, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.123 [-1.837, 0.697], mean_best_reward: --
 23246/100000: episode: 1162, duration: 0.003s, episode steps: 15, steps per second: 5400, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.090 [-1.808, 2.861], mean_best_reward: --
 23259/100000: episode: 1163, duration: 0.003s, episode steps: 13, steps per second: 5144, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.104 [-2.292, 1.396], mean_best_reward: --
 23273/100000: episode: 1164, duration: 0.003s, episode steps: 14, steps per second: 4585, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.111 [-2.127, 1.182], mean_best_reward: --
 23298/100000: episode: 1165, duration: 0.005s, episode steps: 25, steps per second: 5491, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.081 [-1.482, 0.621], mean_best_reward: --
 23308/100000: episode: 1166, duration: 0.002s, episode steps: 10, steps per second: 4874, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.949, 3.128], mean_best_reward: --
 23324/100000: episode: 1167, duration: 0.003s, episode steps: 16, steps per second: 5474, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.121 [-1.125, 0.404], mean_best_reward: --
 23394/100000: episode: 1168, duration: 0.012s, episode steps: 70, steps per second: 6055, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.031 [-1.290, 1.187], mean_best_reward: --
 23418/100000: episode: 1169, duration: 0.004s, episode steps: 24, steps per second: 5736, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.101 [-1.215, 0.547], mean_best_reward: --
 23429/100000: episode: 1170, duration: 0.002s, episode steps: 11, steps per second: 5129, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.130 [-1.638, 0.941], mean_best_reward: --
 23444/100000: episode: 1171, duration: 0.003s, episode steps: 15, steps per second: 5408, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.072 [-1.862, 1.174], mean_best_reward: --
 23462/100000: episode: 1172, duration: 0.004s, episode steps: 18, steps per second: 5065, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.075 [-1.147, 2.007], mean_best_reward: --
 23474/100000: episode: 1173, duration: 0.002s, episode steps: 12, steps per second: 4932, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.121 [-1.980, 1.160], mean_best_reward: --
 23487/100000: episode: 1174, duration: 0.003s, episode steps: 13, steps per second: 4579, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.075 [-1.203, 1.692], mean_best_reward: --
 23513/100000: episode: 1175, duration: 0.004s, episode steps: 26, steps per second: 5855, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.071 [-0.752, 1.228], mean_best_reward: --
 23523/100000: episode: 1176, duration: 0.002s, episode steps: 10, steps per second: 4702, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-3.105, 1.921], mean_best_reward: --
 23542/100000: episode: 1177, duration: 0.004s, episode steps: 19, steps per second: 5096, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.071 [-1.341, 0.770], mean_best_reward: --
 23553/100000: episode: 1178, duration: 0.002s, episode steps: 11, steps per second: 5075, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.103 [-1.772, 2.791], mean_best_reward: --
 23566/100000: episode: 1179, duration: 0.002s, episode steps: 13, steps per second: 5286, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.082 [-2.732, 1.778], mean_best_reward: --
 23586/100000: episode: 1180, duration: 0.004s, episode steps: 20, steps per second: 5513, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.055 [-1.560, 2.445], mean_best_reward: --
 23598/100000: episode: 1181, duration: 0.002s, episode steps: 12, steps per second: 5018, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.110 [-1.145, 1.951], mean_best_reward: --
 23640/100000: episode: 1182, duration: 0.007s, episode steps: 42, steps per second: 5810, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.081 [-0.971, 0.367], mean_best_reward: --
 23680/100000: episode: 1183, duration: 0.007s, episode steps: 40, steps per second: 6092, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.157 [-0.933, 0.557], mean_best_reward: --
 23700/100000: episode: 1184, duration: 0.004s, episode steps: 20, steps per second: 5637, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.083 [-1.207, 2.237], mean_best_reward: --
 23765/100000: episode: 1185, duration: 0.011s, episode steps: 65, steps per second: 5838, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.087 [-1.570, 1.164], mean_best_reward: --
 23777/100000: episode: 1186, duration: 0.002s, episode steps: 12, steps per second: 5077, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.095 [-1.232, 0.807], mean_best_reward: --
 23800/100000: episode: 1187, duration: 0.004s, episode steps: 23, steps per second: 5292, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.094 [-1.064, 0.613], mean_best_reward: --
 23818/100000: episode: 1188, duration: 0.003s, episode steps: 18, steps per second: 5530, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.085 [-2.598, 1.549], mean_best_reward: --
 23827/100000: episode: 1189, duration: 0.002s, episode steps: 9, steps per second: 4860, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.777, 2.819], mean_best_reward: --
 23848/100000: episode: 1190, duration: 0.004s, episode steps: 21, steps per second: 5685, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.062 [-1.401, 2.290], mean_best_reward: --
 23877/100000: episode: 1191, duration: 0.005s, episode steps: 29, steps per second: 5904, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.109 [-0.986, 0.557], mean_best_reward: --
 23888/100000: episode: 1192, duration: 0.002s, episode steps: 11, steps per second: 4866, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.112 [-1.943, 1.160], mean_best_reward: --
 23899/100000: episode: 1193, duration: 0.002s, episode steps: 11, steps per second: 5079, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.118 [-1.139, 1.861], mean_best_reward: --
 23915/100000: episode: 1194, duration: 0.003s, episode steps: 16, steps per second: 5464, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.074 [-1.142, 1.754], mean_best_reward: --
 23934/100000: episode: 1195, duration: 0.003s, episode steps: 19, steps per second: 5548, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.119 [-0.964, 0.588], mean_best_reward: --
 23947/100000: episode: 1196, duration: 0.002s, episode steps: 13, steps per second: 5295, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.109 [-1.160, 1.927], mean_best_reward: --
 23967/100000: episode: 1197, duration: 0.004s, episode steps: 20, steps per second: 5555, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.092 [-0.425, 1.251], mean_best_reward: --
 23985/100000: episode: 1198, duration: 0.003s, episode steps: 18, steps per second: 5451, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.033 [-1.789, 2.459], mean_best_reward: --
 24041/100000: episode: 1199, duration: 0.009s, episode steps: 56, steps per second: 5964, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.055 [-1.214, 0.639], mean_best_reward: --
 24053/100000: episode: 1200, duration: 0.002s, episode steps: 12, steps per second: 5081, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.117 [-1.774, 2.723], mean_best_reward: --
 24065/100000: episode: 1201, duration: 0.003s, episode steps: 12, steps per second: 4544, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.122 [-0.956, 1.654], mean_best_reward: 92.500000
 24083/100000: episode: 1202, duration: 0.003s, episode steps: 18, steps per second: 5394, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.082 [-2.158, 1.212], mean_best_reward: --
 24143/100000: episode: 1203, duration: 0.010s, episode steps: 60, steps per second: 6185, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.128 [-0.467, 1.152], mean_best_reward: --
 24219/100000: episode: 1204, duration: 0.012s, episode steps: 76, steps per second: 6280, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-0.759, 1.029], mean_best_reward: --
 24281/100000: episode: 1205, duration: 0.010s, episode steps: 62, steps per second: 5927, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.233 [-0.793, 1.449], mean_best_reward: --
 24302/100000: episode: 1206, duration: 0.004s, episode steps: 21, steps per second: 5410, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.066 [-0.968, 1.694], mean_best_reward: --
 24360/100000: episode: 1207, duration: 0.010s, episode steps: 58, steps per second: 5797, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: -0.229 [-2.610, 2.057], mean_best_reward: --
 24376/100000: episode: 1208, duration: 0.003s, episode steps: 16, steps per second: 5477, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.102 [-1.348, 2.215], mean_best_reward: --
 24396/100000: episode: 1209, duration: 0.004s, episode steps: 20, steps per second: 5693, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.051 [-0.829, 1.399], mean_best_reward: --
 24408/100000: episode: 1210, duration: 0.002s, episode steps: 12, steps per second: 5194, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.105 [-1.380, 2.099], mean_best_reward: --
 24425/100000: episode: 1211, duration: 0.003s, episode steps: 17, steps per second: 4997, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.106 [-0.947, 1.788], mean_best_reward: --
 24528/100000: episode: 1212, duration: 0.017s, episode steps: 103, steps per second: 6236, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.220 [-1.516, 0.765], mean_best_reward: --
 24561/100000: episode: 1213, duration: 0.006s, episode steps: 33, steps per second: 5419, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.108 [-1.234, 0.274], mean_best_reward: --
 24575/100000: episode: 1214, duration: 0.003s, episode steps: 14, steps per second: 5369, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.110 [-0.776, 1.297], mean_best_reward: --
 24602/100000: episode: 1215, duration: 0.005s, episode steps: 27, steps per second: 5449, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.008 [-1.198, 1.754], mean_best_reward: --
 24649/100000: episode: 1216, duration: 0.008s, episode steps: 47, steps per second: 6096, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.150 [-0.476, 0.847], mean_best_reward: --
 24666/100000: episode: 1217, duration: 0.003s, episode steps: 17, steps per second: 5527, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.069 [-1.570, 0.986], mean_best_reward: --
 24711/100000: episode: 1218, duration: 0.008s, episode steps: 45, steps per second: 5929, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.112 [-0.771, 0.773], mean_best_reward: --
 24726/100000: episode: 1219, duration: 0.003s, episode steps: 15, steps per second: 5437, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.044 [-1.781, 2.517], mean_best_reward: --
 24742/100000: episode: 1220, duration: 0.003s, episode steps: 16, steps per second: 5494, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.094 [-0.980, 1.405], mean_best_reward: --
 24762/100000: episode: 1221, duration: 0.004s, episode steps: 20, steps per second: 5652, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.018, 0.544], mean_best_reward: --
 24780/100000: episode: 1222, duration: 0.003s, episode steps: 18, steps per second: 5202, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.090 [-1.156, 1.985], mean_best_reward: --
 24851/100000: episode: 1223, duration: 0.012s, episode steps: 71, steps per second: 6049, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.037 [-0.844, 0.591], mean_best_reward: --
 24943/100000: episode: 1224, duration: 0.015s, episode steps: 92, steps per second: 6187, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.041 [-1.293, 1.254], mean_best_reward: --
 24961/100000: episode: 1225, duration: 0.003s, episode steps: 18, steps per second: 5577, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.072 [-0.795, 1.258], mean_best_reward: --
 24972/100000: episode: 1226, duration: 0.002s, episode steps: 11, steps per second: 4518, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-1.181, 1.768], mean_best_reward: --
 24993/100000: episode: 1227, duration: 0.004s, episode steps: 21, steps per second: 5632, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.071 [-0.831, 1.432], mean_best_reward: --
 25050/100000: episode: 1228, duration: 0.009s, episode steps: 57, steps per second: 6217, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.016 [-0.897, 0.973], mean_best_reward: --
 25105/100000: episode: 1229, duration: 0.010s, episode steps: 55, steps per second: 5789, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.181 [-1.070, 0.630], mean_best_reward: --
 25142/100000: episode: 1230, duration: 0.007s, episode steps: 37, steps per second: 5590, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.070 [-0.560, 1.150], mean_best_reward: --
 25153/100000: episode: 1231, duration: 0.003s, episode steps: 11, steps per second: 4181, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.106 [-0.938, 1.562], mean_best_reward: --
 25173/100000: episode: 1232, duration: 0.004s, episode steps: 20, steps per second: 4714, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.056 [-1.180, 0.799], mean_best_reward: --
 25186/100000: episode: 1233, duration: 0.003s, episode steps: 13, steps per second: 4002, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.080 [-0.799, 1.346], mean_best_reward: --
 25244/100000: episode: 1234, duration: 0.009s, episode steps: 58, steps per second: 6133, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.128 [-0.768, 1.228], mean_best_reward: --
 25272/100000: episode: 1235, duration: 0.005s, episode steps: 28, steps per second: 5870, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.085 [-1.211, 0.757], mean_best_reward: --
 25290/100000: episode: 1236, duration: 0.003s, episode steps: 18, steps per second: 5551, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-1.049, 0.561], mean_best_reward: --
 25315/100000: episode: 1237, duration: 0.004s, episode steps: 25, steps per second: 5796, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.130 [-0.937, 0.617], mean_best_reward: --
 25421/100000: episode: 1238, duration: 0.017s, episode steps: 106, steps per second: 6128, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.557 [0.000, 1.000], mean observation: 0.454 [-1.097, 2.743], mean_best_reward: --
 25456/100000: episode: 1239, duration: 0.006s, episode steps: 35, steps per second: 5940, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.002 [-1.316, 1.597], mean_best_reward: --
 25492/100000: episode: 1240, duration: 0.006s, episode steps: 36, steps per second: 6031, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.056 [-0.582, 0.962], mean_best_reward: --
 25534/100000: episode: 1241, duration: 0.007s, episode steps: 42, steps per second: 5831, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.137 [-0.391, 1.678], mean_best_reward: --
 25556/100000: episode: 1242, duration: 0.004s, episode steps: 22, steps per second: 5771, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.081 [-0.633, 1.373], mean_best_reward: --
 25608/100000: episode: 1243, duration: 0.008s, episode steps: 52, steps per second: 6185, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.091 [-1.095, 0.651], mean_best_reward: --
 25736/100000: episode: 1244, duration: 0.021s, episode steps: 128, steps per second: 6138, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.181 [-1.017, 1.715], mean_best_reward: --
 25754/100000: episode: 1245, duration: 0.003s, episode steps: 18, steps per second: 5591, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.099 [-1.371, 2.276], mean_best_reward: --
 25866/100000: episode: 1246, duration: 0.018s, episode steps: 112, steps per second: 6276, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.170 [-1.661, 0.912], mean_best_reward: --
 25988/100000: episode: 1247, duration: 0.020s, episode steps: 122, steps per second: 6215, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.475 [-0.876, 2.117], mean_best_reward: --
 26006/100000: episode: 1248, duration: 0.003s, episode steps: 18, steps per second: 5256, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.047 [-1.176, 1.846], mean_best_reward: --
 26017/100000: episode: 1249, duration: 0.002s, episode steps: 11, steps per second: 5002, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.103 [-1.013, 1.645], mean_best_reward: --
 26092/100000: episode: 1250, duration: 0.012s, episode steps: 75, steps per second: 6316, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.037 [-0.818, 0.727], mean_best_reward: --
 26127/100000: episode: 1251, duration: 0.006s, episode steps: 35, steps per second: 5406, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.140 [-1.518, 0.397], mean_best_reward: 87.500000
 26143/100000: episode: 1252, duration: 0.003s, episode steps: 16, steps per second: 5505, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.082 [-1.331, 2.145], mean_best_reward: --
 26158/100000: episode: 1253, duration: 0.003s, episode steps: 15, steps per second: 5472, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.073 [-1.392, 2.090], mean_best_reward: --
 26183/100000: episode: 1254, duration: 0.004s, episode steps: 25, steps per second: 5788, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.072 [-1.377, 0.585], mean_best_reward: --
 26234/100000: episode: 1255, duration: 0.009s, episode steps: 51, steps per second: 5968, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.057 [-1.381, 1.603], mean_best_reward: --
 26246/100000: episode: 1256, duration: 0.003s, episode steps: 12, steps per second: 4189, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.094 [-2.492, 1.609], mean_best_reward: --
 26270/100000: episode: 1257, duration: 0.004s, episode steps: 24, steps per second: 5778, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.542, 0.910], mean_best_reward: --
 26288/100000: episode: 1258, duration: 0.003s, episode steps: 18, steps per second: 5517, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.067 [-1.216, 1.950], mean_best_reward: --
 26304/100000: episode: 1259, duration: 0.003s, episode steps: 16, steps per second: 5497, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.093 [-1.408, 0.757], mean_best_reward: --
 26328/100000: episode: 1260, duration: 0.004s, episode steps: 24, steps per second: 5819, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.059 [-0.774, 1.298], mean_best_reward: --
 26351/100000: episode: 1261, duration: 0.004s, episode steps: 23, steps per second: 5813, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.065 [-0.631, 1.036], mean_best_reward: --
 26364/100000: episode: 1262, duration: 0.003s, episode steps: 13, steps per second: 4910, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.107 [-0.800, 1.457], mean_best_reward: --
 26392/100000: episode: 1263, duration: 0.005s, episode steps: 28, steps per second: 5869, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.010 [-1.587, 2.389], mean_best_reward: --
 26415/100000: episode: 1264, duration: 0.004s, episode steps: 23, steps per second: 5810, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.104 [-1.326, 0.608], mean_best_reward: --
 26443/100000: episode: 1265, duration: 0.005s, episode steps: 28, steps per second: 5939, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.128 [-1.157, 0.732], mean_best_reward: --
 26461/100000: episode: 1266, duration: 0.004s, episode steps: 18, steps per second: 5114, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.091, 0.602], mean_best_reward: --
 26488/100000: episode: 1267, duration: 0.005s, episode steps: 27, steps per second: 5634, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.704 [0.000, 1.000], mean observation: 0.002 [-3.109, 2.171], mean_best_reward: --
 26505/100000: episode: 1268, duration: 0.003s, episode steps: 17, steps per second: 5125, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.086 [-2.377, 1.405], mean_best_reward: --
 26516/100000: episode: 1269, duration: 0.002s, episode steps: 11, steps per second: 5133, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.098 [-0.823, 1.294], mean_best_reward: --
 26545/100000: episode: 1270, duration: 0.005s, episode steps: 29, steps per second: 5430, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.059 [-0.757, 1.399], mean_best_reward: --
 26593/100000: episode: 1271, duration: 0.008s, episode steps: 48, steps per second: 6137, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.044 [-0.957, 1.045], mean_best_reward: --
 26605/100000: episode: 1272, duration: 0.002s, episode steps: 12, steps per second: 5212, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.084 [-1.587, 2.489], mean_best_reward: --
 26619/100000: episode: 1273, duration: 0.003s, episode steps: 14, steps per second: 4895, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.100 [-0.791, 1.334], mean_best_reward: --
 26635/100000: episode: 1274, duration: 0.003s, episode steps: 16, steps per second: 5081, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.104 [-1.750, 0.968], mean_best_reward: --
 26653/100000: episode: 1275, duration: 0.003s, episode steps: 18, steps per second: 5602, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.049 [-1.942, 1.217], mean_best_reward: --
 26677/100000: episode: 1276, duration: 0.004s, episode steps: 24, steps per second: 5750, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-1.353, 0.780], mean_best_reward: --
 26691/100000: episode: 1277, duration: 0.003s, episode steps: 14, steps per second: 5403, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.086 [-0.974, 1.575], mean_best_reward: --
 26715/100000: episode: 1278, duration: 0.004s, episode steps: 24, steps per second: 5728, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.028 [-1.577, 2.348], mean_best_reward: --
 26741/100000: episode: 1279, duration: 0.004s, episode steps: 26, steps per second: 5804, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.062 [-0.592, 1.340], mean_best_reward: --
 26759/100000: episode: 1280, duration: 0.004s, episode steps: 18, steps per second: 5114, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.076 [-1.186, 2.080], mean_best_reward: --
 26775/100000: episode: 1281, duration: 0.003s, episode steps: 16, steps per second: 5512, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.097 [-0.956, 1.493], mean_best_reward: --
 26801/100000: episode: 1282, duration: 0.005s, episode steps: 26, steps per second: 5477, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.067 [-0.628, 1.415], mean_best_reward: --
 26810/100000: episode: 1283, duration: 0.002s, episode steps: 9, steps per second: 4940, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.112 [-2.119, 1.401], mean_best_reward: --
 26829/100000: episode: 1284, duration: 0.003s, episode steps: 19, steps per second: 5673, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.056 [-1.009, 1.425], mean_best_reward: --
 26853/100000: episode: 1285, duration: 0.004s, episode steps: 24, steps per second: 5797, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.397, 1.011], mean_best_reward: --
 26862/100000: episode: 1286, duration: 0.002s, episode steps: 9, steps per second: 4952, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.170 [-2.297, 1.345], mean_best_reward: --
 26916/100000: episode: 1287, duration: 0.009s, episode steps: 54, steps per second: 6077, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-1.445, 1.162], mean_best_reward: --
 26987/100000: episode: 1288, duration: 0.011s, episode steps: 71, steps per second: 6288, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.180 [-1.006, 1.339], mean_best_reward: --
 27000/100000: episode: 1289, duration: 0.002s, episode steps: 13, steps per second: 5271, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.088 [-1.416, 2.212], mean_best_reward: --
 27016/100000: episode: 1290, duration: 0.003s, episode steps: 16, steps per second: 4876, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.105 [-0.987, 1.840], mean_best_reward: --
 27040/100000: episode: 1291, duration: 0.004s, episode steps: 24, steps per second: 5807, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.022 [-1.587, 2.259], mean_best_reward: --
 27059/100000: episode: 1292, duration: 0.004s, episode steps: 19, steps per second: 5294, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.075 [-1.295, 0.824], mean_best_reward: --
 27075/100000: episode: 1293, duration: 0.003s, episode steps: 16, steps per second: 5285, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.080 [-1.009, 1.747], mean_best_reward: --
 27086/100000: episode: 1294, duration: 0.002s, episode steps: 11, steps per second: 5114, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.087 [-0.840, 1.427], mean_best_reward: --
 27098/100000: episode: 1295, duration: 0.002s, episode steps: 12, steps per second: 5141, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.115 [-2.529, 1.523], mean_best_reward: --
 27137/100000: episode: 1296, duration: 0.006s, episode steps: 39, steps per second: 6072, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: 0.083 [-2.055, 1.928], mean_best_reward: --
 27150/100000: episode: 1297, duration: 0.003s, episode steps: 13, steps per second: 4918, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.101 [-1.841, 1.146], mean_best_reward: --
 27161/100000: episode: 1298, duration: 0.002s, episode steps: 11, steps per second: 5045, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.123 [-0.944, 1.631], mean_best_reward: --
 27176/100000: episode: 1299, duration: 0.003s, episode steps: 15, steps per second: 5411, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.087 [-1.845, 1.028], mean_best_reward: --
 27195/100000: episode: 1300, duration: 0.003s, episode steps: 19, steps per second: 5616, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.083 [-1.426, 0.787], mean_best_reward: --
 27214/100000: episode: 1301, duration: 0.004s, episode steps: 19, steps per second: 5194, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.073 [-0.787, 1.246], mean_best_reward: 123.500000
 27229/100000: episode: 1302, duration: 0.003s, episode steps: 15, steps per second: 5399, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.112 [-1.218, 0.779], mean_best_reward: --
 27244/100000: episode: 1303, duration: 0.003s, episode steps: 15, steps per second: 5342, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.099 [-0.987, 1.490], mean_best_reward: --
 27271/100000: episode: 1304, duration: 0.005s, episode steps: 27, steps per second: 5393, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.296 [0.000, 1.000], mean observation: -0.012 [-2.117, 2.801], mean_best_reward: --
 27284/100000: episode: 1305, duration: 0.003s, episode steps: 13, steps per second: 5047, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.096 [-1.446, 0.834], mean_best_reward: --
 27313/100000: episode: 1306, duration: 0.005s, episode steps: 29, steps per second: 5782, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.061 [-1.133, 1.883], mean_best_reward: --
 27364/100000: episode: 1307, duration: 0.008s, episode steps: 51, steps per second: 6120, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.082 [-1.187, 1.922], mean_best_reward: --
 27400/100000: episode: 1308, duration: 0.006s, episode steps: 36, steps per second: 5993, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.001 [-1.200, 1.708], mean_best_reward: --
 27411/100000: episode: 1309, duration: 0.002s, episode steps: 11, steps per second: 4705, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.080 [-1.401, 1.999], mean_best_reward: --
 27458/100000: episode: 1310, duration: 0.008s, episode steps: 47, steps per second: 6155, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.073 [-0.847, 0.792], mean_best_reward: --
 27498/100000: episode: 1311, duration: 0.007s, episode steps: 40, steps per second: 6057, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.544, 1.027], mean_best_reward: --
 27522/100000: episode: 1312, duration: 0.004s, episode steps: 24, steps per second: 5718, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.047 [-0.793, 1.224], mean_best_reward: --
 27539/100000: episode: 1313, duration: 0.003s, episode steps: 17, steps per second: 5328, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.109 [-0.613, 1.081], mean_best_reward: --
 27574/100000: episode: 1314, duration: 0.006s, episode steps: 35, steps per second: 5604, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.116 [-0.406, 1.014], mean_best_reward: --
 27631/100000: episode: 1315, duration: 0.009s, episode steps: 57, steps per second: 6006, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.129 [-1.685, 0.690], mean_best_reward: --
 27647/100000: episode: 1316, duration: 0.003s, episode steps: 16, steps per second: 5482, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.091 [-1.302, 0.609], mean_best_reward: --
 27686/100000: episode: 1317, duration: 0.006s, episode steps: 39, steps per second: 6023, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.131 [-0.743, 0.221], mean_best_reward: --
 27703/100000: episode: 1318, duration: 0.003s, episode steps: 17, steps per second: 5462, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.105 [-0.746, 1.514], mean_best_reward: --
 27764/100000: episode: 1319, duration: 0.010s, episode steps: 61, steps per second: 6127, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.098 [-1.029, 0.959], mean_best_reward: --
 27789/100000: episode: 1320, duration: 0.004s, episode steps: 25, steps per second: 5718, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.031 [-0.794, 1.238], mean_best_reward: --
 27808/100000: episode: 1321, duration: 0.003s, episode steps: 19, steps per second: 5566, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.091 [-1.084, 0.583], mean_best_reward: --
 27820/100000: episode: 1322, duration: 0.003s, episode steps: 12, steps per second: 4105, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.530, 0.960], mean_best_reward: --
 27881/100000: episode: 1323, duration: 0.010s, episode steps: 61, steps per second: 5972, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.057 [-0.819, 1.384], mean_best_reward: --
 27911/100000: episode: 1324, duration: 0.005s, episode steps: 30, steps per second: 5837, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: 0.037 [-1.756, 1.395], mean_best_reward: --
 27938/100000: episode: 1325, duration: 0.005s, episode steps: 27, steps per second: 5842, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.044 [-1.185, 0.753], mean_best_reward: --
 27954/100000: episode: 1326, duration: 0.003s, episode steps: 16, steps per second: 5032, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.088 [-0.786, 1.595], mean_best_reward: --
 27970/100000: episode: 1327, duration: 0.003s, episode steps: 16, steps per second: 5399, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.100 [-0.763, 1.201], mean_best_reward: --
 27987/100000: episode: 1328, duration: 0.003s, episode steps: 17, steps per second: 5526, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.067 [-0.809, 1.174], mean_best_reward: --
 27997/100000: episode: 1329, duration: 0.002s, episode steps: 10, steps per second: 5034, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.122 [-1.690, 1.006], mean_best_reward: --
 28009/100000: episode: 1330, duration: 0.002s, episode steps: 12, steps per second: 5165, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.123 [-1.152, 2.009], mean_best_reward: --
 28037/100000: episode: 1331, duration: 0.005s, episode steps: 28, steps per second: 5555, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.046 [-0.581, 1.250], mean_best_reward: --
 28047/100000: episode: 1332, duration: 0.002s, episode steps: 10, steps per second: 4885, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.101 [-1.226, 1.991], mean_best_reward: --
 28085/100000: episode: 1333, duration: 0.007s, episode steps: 38, steps per second: 5716, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.038 [-2.163, 1.219], mean_best_reward: --
 28123/100000: episode: 1334, duration: 0.006s, episode steps: 38, steps per second: 6003, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.033 [-1.185, 1.816], mean_best_reward: --
 28172/100000: episode: 1335, duration: 0.008s, episode steps: 49, steps per second: 5944, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.025 [-0.829, 1.269], mean_best_reward: --
 28194/100000: episode: 1336, duration: 0.004s, episode steps: 22, steps per second: 5564, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.119, 0.600], mean_best_reward: --
 28210/100000: episode: 1337, duration: 0.003s, episode steps: 16, steps per second: 5474, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.113 [-1.526, 2.586], mean_best_reward: --
 28241/100000: episode: 1338, duration: 0.005s, episode steps: 31, steps per second: 5839, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.085 [-0.401, 0.734], mean_best_reward: --
 28278/100000: episode: 1339, duration: 0.006s, episode steps: 37, steps per second: 6032, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: 0.012 [-1.181, 1.677], mean_best_reward: --
 28296/100000: episode: 1340, duration: 0.003s, episode steps: 18, steps per second: 5592, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.063 [-2.361, 1.523], mean_best_reward: --
 28305/100000: episode: 1341, duration: 0.002s, episode steps: 9, steps per second: 4439, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.108 [-1.389, 2.206], mean_best_reward: --
 28316/100000: episode: 1342, duration: 0.002s, episode steps: 11, steps per second: 4903, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-0.785, 1.417], mean_best_reward: --
 28326/100000: episode: 1343, duration: 0.002s, episode steps: 10, steps per second: 4855, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.121 [-2.197, 1.400], mean_best_reward: --
 28349/100000: episode: 1344, duration: 0.004s, episode steps: 23, steps per second: 5363, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.098 [-0.581, 0.975], mean_best_reward: --
 28392/100000: episode: 1345, duration: 0.007s, episode steps: 43, steps per second: 6023, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.065 [-1.045, 0.519], mean_best_reward: --
 28416/100000: episode: 1346, duration: 0.004s, episode steps: 24, steps per second: 5735, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.098 [-0.763, 1.541], mean_best_reward: --
 28431/100000: episode: 1347, duration: 0.003s, episode steps: 15, steps per second: 5451, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.087 [-1.035, 1.820], mean_best_reward: --
 28442/100000: episode: 1348, duration: 0.002s, episode steps: 11, steps per second: 5145, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-1.153, 1.925], mean_best_reward: --
 28458/100000: episode: 1349, duration: 0.003s, episode steps: 16, steps per second: 5508, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.084 [-1.731, 2.608], mean_best_reward: --
 28477/100000: episode: 1350, duration: 0.004s, episode steps: 19, steps per second: 5135, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.067 [-1.140, 1.911], mean_best_reward: --
 28527/100000: episode: 1351, duration: 0.008s, episode steps: 50, steps per second: 5939, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.127 [-1.650, 1.868], mean_best_reward: 77.500000
 28536/100000: episode: 1352, duration: 0.002s, episode steps: 9, steps per second: 4893, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.130 [-1.543, 2.434], mean_best_reward: --
 28558/100000: episode: 1353, duration: 0.004s, episode steps: 22, steps per second: 5399, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.072 [-0.951, 1.544], mean_best_reward: --
 28569/100000: episode: 1354, duration: 0.002s, episode steps: 11, steps per second: 4814, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.230, 1.391], mean_best_reward: --
 28581/100000: episode: 1355, duration: 0.002s, episode steps: 12, steps per second: 5191, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.094 [-1.563, 1.030], mean_best_reward: --
 28594/100000: episode: 1356, duration: 0.003s, episode steps: 13, steps per second: 5067, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.106 [-1.783, 0.971], mean_best_reward: --
 28617/100000: episode: 1357, duration: 0.004s, episode steps: 23, steps per second: 5145, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.053 [-1.154, 0.779], mean_best_reward: --
 28662/100000: episode: 1358, duration: 0.009s, episode steps: 45, steps per second: 5190, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.070 [-1.408, 0.579], mean_best_reward: --
 28736/100000: episode: 1359, duration: 0.012s, episode steps: 74, steps per second: 6224, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.047 [-1.454, 0.889], mean_best_reward: --
 28748/100000: episode: 1360, duration: 0.002s, episode steps: 12, steps per second: 5099, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.115 [-2.734, 1.801], mean_best_reward: --
 28825/100000: episode: 1361, duration: 0.012s, episode steps: 77, steps per second: 6288, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.155 [-0.554, 0.970], mean_best_reward: --
 28836/100000: episode: 1362, duration: 0.002s, episode steps: 11, steps per second: 4476, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.127 [-1.136, 1.892], mean_best_reward: --
 28867/100000: episode: 1363, duration: 0.005s, episode steps: 31, steps per second: 5813, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.066 [-1.189, 0.440], mean_best_reward: --
 28883/100000: episode: 1364, duration: 0.003s, episode steps: 16, steps per second: 4958, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.095 [-1.157, 1.945], mean_best_reward: --
 28899/100000: episode: 1365, duration: 0.003s, episode steps: 16, steps per second: 5344, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.075 [-1.208, 0.807], mean_best_reward: --
 28926/100000: episode: 1366, duration: 0.005s, episode steps: 27, steps per second: 5451, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.000 [-1.710, 2.410], mean_best_reward: --
 28936/100000: episode: 1367, duration: 0.002s, episode steps: 10, steps per second: 5008, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.546, 2.382], mean_best_reward: --
 28948/100000: episode: 1368, duration: 0.002s, episode steps: 12, steps per second: 5216, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.107 [-2.115, 1.232], mean_best_reward: --
 28962/100000: episode: 1369, duration: 0.003s, episode steps: 14, steps per second: 5116, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.085 [-1.193, 2.029], mean_best_reward: --
 28988/100000: episode: 1370, duration: 0.004s, episode steps: 26, steps per second: 5822, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.081 [-1.225, 2.302], mean_best_reward: --
 29005/100000: episode: 1371, duration: 0.003s, episode steps: 17, steps per second: 5016, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.073 [-2.219, 1.337], mean_best_reward: --
 29019/100000: episode: 1372, duration: 0.003s, episode steps: 14, steps per second: 5367, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.088 [-1.994, 3.076], mean_best_reward: --
 29086/100000: episode: 1373, duration: 0.011s, episode steps: 67, steps per second: 6238, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.041 [-0.935, 0.621], mean_best_reward: --
 29104/100000: episode: 1374, duration: 0.003s, episode steps: 18, steps per second: 5442, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.075 [-1.520, 0.813], mean_best_reward: --
 29131/100000: episode: 1375, duration: 0.005s, episode steps: 27, steps per second: 4924, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.096 [-0.890, 0.352], mean_best_reward: --
 29180/100000: episode: 1376, duration: 0.008s, episode steps: 49, steps per second: 5864, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.100 [-0.777, 1.700], mean_best_reward: --
 29189/100000: episode: 1377, duration: 0.002s, episode steps: 9, steps per second: 4707, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.357, 2.264], mean_best_reward: --
 29215/100000: episode: 1378, duration: 0.004s, episode steps: 26, steps per second: 5851, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.130 [-0.874, 0.411], mean_best_reward: --
 29227/100000: episode: 1379, duration: 0.002s, episode steps: 12, steps per second: 5225, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.097 [-1.177, 1.835], mean_best_reward: --
 29237/100000: episode: 1380, duration: 0.002s, episode steps: 10, steps per second: 5043, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.036, 1.957], mean_best_reward: --
 29264/100000: episode: 1381, duration: 0.005s, episode steps: 27, steps per second: 5470, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.118 [-1.126, 0.381], mean_best_reward: --
 29274/100000: episode: 1382, duration: 0.002s, episode steps: 10, steps per second: 4999, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.143 [-1.965, 1.152], mean_best_reward: --
 29285/100000: episode: 1383, duration: 0.002s, episode steps: 11, steps per second: 5143, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.111 [-1.510, 0.834], mean_best_reward: --
 29332/100000: episode: 1384, duration: 0.008s, episode steps: 47, steps per second: 6131, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.141 [-0.685, 0.985], mean_best_reward: --
 29350/100000: episode: 1385, duration: 0.004s, episode steps: 18, steps per second: 5065, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.078 [-1.268, 0.773], mean_best_reward: --
 29375/100000: episode: 1386, duration: 0.004s, episode steps: 25, steps per second: 5686, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.059 [-1.637, 0.947], mean_best_reward: --
 29385/100000: episode: 1387, duration: 0.003s, episode steps: 10, steps per second: 3470, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.103 [-1.419, 2.114], mean_best_reward: --
 29396/100000: episode: 1388, duration: 0.002s, episode steps: 11, steps per second: 5047, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.102 [-1.581, 1.028], mean_best_reward: --
 29405/100000: episode: 1389, duration: 0.002s, episode steps: 9, steps per second: 4874, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.154 [-1.326, 2.276], mean_best_reward: --
 29446/100000: episode: 1390, duration: 0.007s, episode steps: 41, steps per second: 5757, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.390 [0.000, 1.000], mean observation: 0.025 [-1.727, 2.661], mean_best_reward: --
 29465/100000: episode: 1391, duration: 0.003s, episode steps: 19, steps per second: 5616, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.073 [-1.288, 0.785], mean_best_reward: --
 29483/100000: episode: 1392, duration: 0.003s, episode steps: 18, steps per second: 5563, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.998, 1.473], mean_best_reward: --
 29501/100000: episode: 1393, duration: 0.003s, episode steps: 18, steps per second: 5575, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.069 [-1.126, 1.665], mean_best_reward: --
 29550/100000: episode: 1394, duration: 0.008s, episode steps: 49, steps per second: 5895, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.090 [-1.095, 0.690], mean_best_reward: --
 29563/100000: episode: 1395, duration: 0.003s, episode steps: 13, steps per second: 5058, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.119 [-2.358, 1.390], mean_best_reward: --
 29578/100000: episode: 1396, duration: 0.003s, episode steps: 15, steps per second: 5356, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.098 [-1.411, 2.359], mean_best_reward: --
 29588/100000: episode: 1397, duration: 0.002s, episode steps: 10, steps per second: 5011, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.575, 2.564], mean_best_reward: --
 29597/100000: episode: 1398, duration: 0.002s, episode steps: 9, steps per second: 4179, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.375, 2.275], mean_best_reward: --
 29641/100000: episode: 1399, duration: 0.008s, episode steps: 44, steps per second: 5619, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.047 [-0.722, 0.431], mean_best_reward: --
 29676/100000: episode: 1400, duration: 0.006s, episode steps: 35, steps per second: 5885, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.050 [-1.093, 0.440], mean_best_reward: --
 29692/100000: episode: 1401, duration: 0.004s, episode steps: 16, steps per second: 4008, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-1.017, 1.467], mean_best_reward: 66.500000
 29724/100000: episode: 1402, duration: 0.005s, episode steps: 32, steps per second: 5934, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.378, 1.286], mean_best_reward: --
 29765/100000: episode: 1403, duration: 0.007s, episode steps: 41, steps per second: 6072, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.035 [-1.545, 0.766], mean_best_reward: --
 29793/100000: episode: 1404, duration: 0.005s, episode steps: 28, steps per second: 5671, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.022 [-1.732, 2.344], mean_best_reward: --
 29806/100000: episode: 1405, duration: 0.003s, episode steps: 13, steps per second: 5196, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.096 [-2.326, 1.418], mean_best_reward: --
 29819/100000: episode: 1406, duration: 0.003s, episode steps: 13, steps per second: 4921, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.127 [-1.146, 1.873], mean_best_reward: --
 29838/100000: episode: 1407, duration: 0.004s, episode steps: 19, steps per second: 5307, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.070 [-1.054, 0.631], mean_best_reward: --
 29854/100000: episode: 1408, duration: 0.003s, episode steps: 16, steps per second: 4835, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.099 [-1.589, 0.815], mean_best_reward: --
 29925/100000: episode: 1409, duration: 0.013s, episode steps: 71, steps per second: 5619, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.120 [-0.686, 1.035], mean_best_reward: --
 29946/100000: episode: 1410, duration: 0.004s, episode steps: 21, steps per second: 5193, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.022 [-0.838, 1.356], mean_best_reward: --
 29962/100000: episode: 1411, duration: 0.003s, episode steps: 16, steps per second: 5319, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.070 [-1.034, 1.775], mean_best_reward: --
 29973/100000: episode: 1412, duration: 0.002s, episode steps: 11, steps per second: 5099, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.146 [-2.296, 1.338], mean_best_reward: --
 29992/100000: episode: 1413, duration: 0.003s, episode steps: 19, steps per second: 5591, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.068 [-1.018, 1.792], mean_best_reward: --
 30011/100000: episode: 1414, duration: 0.003s, episode steps: 19, steps per second: 5577, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.115 [-1.135, 0.558], mean_best_reward: --
 30039/100000: episode: 1415, duration: 0.005s, episode steps: 28, steps per second: 5577, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.021 [-2.322, 1.535], mean_best_reward: --
 30054/100000: episode: 1416, duration: 0.003s, episode steps: 15, steps per second: 5381, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.093 [-0.751, 1.183], mean_best_reward: --
 30122/100000: episode: 1417, duration: 0.011s, episode steps: 68, steps per second: 6189, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.068 [-0.984, 1.344], mean_best_reward: --
 30231/100000: episode: 1418, duration: 0.018s, episode steps: 109, steps per second: 6070, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.013 [-0.943, 1.165], mean_best_reward: --
 30259/100000: episode: 1419, duration: 0.005s, episode steps: 28, steps per second: 5805, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.636, 1.021], mean_best_reward: --
 30284/100000: episode: 1420, duration: 0.004s, episode steps: 25, steps per second: 5795, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.048 [-1.593, 2.501], mean_best_reward: --
 30303/100000: episode: 1421, duration: 0.003s, episode steps: 19, steps per second: 5578, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.100 [-0.749, 1.489], mean_best_reward: --
 30331/100000: episode: 1422, duration: 0.005s, episode steps: 28, steps per second: 5751, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.078 [-0.584, 0.877], mean_best_reward: --
 30344/100000: episode: 1423, duration: 0.003s, episode steps: 13, steps per second: 5189, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.119 [-1.329, 2.213], mean_best_reward: --
 30403/100000: episode: 1424, duration: 0.010s, episode steps: 59, steps per second: 5824, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.119 [-0.657, 1.537], mean_best_reward: --
 30415/100000: episode: 1425, duration: 0.002s, episode steps: 12, steps per second: 5173, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.133 [-1.556, 2.625], mean_best_reward: --
 30439/100000: episode: 1426, duration: 0.004s, episode steps: 24, steps per second: 5401, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.150, 0.805], mean_best_reward: --
 30454/100000: episode: 1427, duration: 0.003s, episode steps: 15, steps per second: 5448, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.106 [-1.370, 2.372], mean_best_reward: --
 30483/100000: episode: 1428, duration: 0.005s, episode steps: 29, steps per second: 5578, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.053 [-0.763, 1.413], mean_best_reward: --
 30542/100000: episode: 1429, duration: 0.009s, episode steps: 59, steps per second: 6254, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.119 [-0.573, 1.000], mean_best_reward: --
 30555/100000: episode: 1430, duration: 0.002s, episode steps: 13, steps per second: 5317, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.106 [-1.374, 2.194], mean_best_reward: --
 30565/100000: episode: 1431, duration: 0.002s, episode steps: 10, steps per second: 4843, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.135 [-1.957, 1.186], mean_best_reward: --
 30631/100000: episode: 1432, duration: 0.011s, episode steps: 66, steps per second: 6181, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.094 [-1.110, 0.693], mean_best_reward: --
 30645/100000: episode: 1433, duration: 0.003s, episode steps: 14, steps per second: 5375, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.070 [-1.203, 1.930], mean_best_reward: --
 30660/100000: episode: 1434, duration: 0.003s, episode steps: 15, steps per second: 5482, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.072 [-1.755, 2.607], mean_best_reward: --
 30691/100000: episode: 1435, duration: 0.005s, episode steps: 31, steps per second: 5907, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.047 [-0.828, 1.334], mean_best_reward: --
 30703/100000: episode: 1436, duration: 0.003s, episode steps: 12, steps per second: 4715, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.121 [-2.607, 1.547], mean_best_reward: --
 30762/100000: episode: 1437, duration: 0.010s, episode steps: 59, steps per second: 5813, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.064 [-0.936, 1.330], mean_best_reward: --
 30787/100000: episode: 1438, duration: 0.004s, episode steps: 25, steps per second: 5776, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.116 [-0.389, 0.914], mean_best_reward: --
 30821/100000: episode: 1439, duration: 0.006s, episode steps: 34, steps per second: 6004, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.041 [-0.925, 1.413], mean_best_reward: --
 30837/100000: episode: 1440, duration: 0.003s, episode steps: 16, steps per second: 5468, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.101 [-1.006, 1.727], mean_best_reward: --
 30881/100000: episode: 1441, duration: 0.007s, episode steps: 44, steps per second: 5948, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.007 [-0.776, 1.055], mean_best_reward: --
 30896/100000: episode: 1442, duration: 0.003s, episode steps: 15, steps per second: 5281, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.090 [-1.927, 1.179], mean_best_reward: --
 30911/100000: episode: 1443, duration: 0.003s, episode steps: 15, steps per second: 5458, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.096 [-1.368, 2.252], mean_best_reward: --
 30951/100000: episode: 1444, duration: 0.007s, episode steps: 40, steps per second: 6005, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.080 [-1.153, 0.828], mean_best_reward: --
 30964/100000: episode: 1445, duration: 0.003s, episode steps: 13, steps per second: 5134, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.104 [-1.125, 1.737], mean_best_reward: --
 30977/100000: episode: 1446, duration: 0.003s, episode steps: 13, steps per second: 4761, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.098 [-2.167, 1.334], mean_best_reward: --
 30995/100000: episode: 1447, duration: 0.003s, episode steps: 18, steps per second: 5578, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.067 [-1.182, 1.860], mean_best_reward: --
 31033/100000: episode: 1448, duration: 0.006s, episode steps: 38, steps per second: 5955, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.528, 1.104], mean_best_reward: --
 31126/100000: episode: 1449, duration: 0.015s, episode steps: 93, steps per second: 6311, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.070 [-0.806, 0.651], mean_best_reward: --
 31142/100000: episode: 1450, duration: 0.003s, episode steps: 16, steps per second: 5487, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.095 [-2.624, 1.546], mean_best_reward: --
 31158/100000: episode: 1451, duration: 0.003s, episode steps: 16, steps per second: 4993, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.122 [-1.206, 0.749], mean_best_reward: 87.000000
 31176/100000: episode: 1452, duration: 0.003s, episode steps: 18, steps per second: 5592, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.102 [-0.569, 1.221], mean_best_reward: --
 31276/100000: episode: 1453, duration: 0.016s, episode steps: 100, steps per second: 6197, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.168 [-0.816, 1.202], mean_best_reward: --
 31315/100000: episode: 1454, duration: 0.007s, episode steps: 39, steps per second: 5751, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: 0.106 [-0.977, 2.292], mean_best_reward: --
 31338/100000: episode: 1455, duration: 0.004s, episode steps: 23, steps per second: 5689, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.044 [-1.263, 0.829], mean_best_reward: --
 31357/100000: episode: 1456, duration: 0.003s, episode steps: 19, steps per second: 5608, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.073 [-0.767, 1.204], mean_best_reward: --
 31397/100000: episode: 1457, duration: 0.007s, episode steps: 40, steps per second: 6075, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-0.984, 0.571], mean_best_reward: --
 31432/100000: episode: 1458, duration: 0.006s, episode steps: 35, steps per second: 5937, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.371 [0.000, 1.000], mean observation: -0.035 [-1.713, 2.220], mean_best_reward: --
 31536/100000: episode: 1459, duration: 0.017s, episode steps: 104, steps per second: 6258, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.327 [-1.808, 0.757], mean_best_reward: --
 31553/100000: episode: 1460, duration: 0.003s, episode steps: 17, steps per second: 4948, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.086 [-1.153, 0.792], mean_best_reward: --
 31573/100000: episode: 1461, duration: 0.004s, episode steps: 20, steps per second: 5670, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.108 [-1.194, 0.414], mean_best_reward: --
 31591/100000: episode: 1462, duration: 0.003s, episode steps: 18, steps per second: 5364, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.109 [-1.011, 0.580], mean_best_reward: --
 31644/100000: episode: 1463, duration: 0.009s, episode steps: 53, steps per second: 6174, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.133 [-1.862, 1.486], mean_best_reward: --
 31667/100000: episode: 1464, duration: 0.004s, episode steps: 23, steps per second: 5754, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.072 [-0.903, 0.553], mean_best_reward: --
 31702/100000: episode: 1465, duration: 0.006s, episode steps: 35, steps per second: 5953, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.129 [-0.943, 0.562], mean_best_reward: --
 31730/100000: episode: 1466, duration: 0.005s, episode steps: 28, steps per second: 5891, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: 0.009 [-2.304, 1.779], mean_best_reward: --
 31747/100000: episode: 1467, duration: 0.003s, episode steps: 17, steps per second: 5531, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.093 [-1.460, 0.929], mean_best_reward: --
 31787/100000: episode: 1468, duration: 0.007s, episode steps: 40, steps per second: 5886, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.010 [-1.159, 0.997], mean_best_reward: --
 31797/100000: episode: 1469, duration: 0.002s, episode steps: 10, steps per second: 4980, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.139 [-2.575, 1.553], mean_best_reward: --
 31838/100000: episode: 1470, duration: 0.007s, episode steps: 41, steps per second: 5821, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.173 [-0.957, 0.539], mean_best_reward: --
 31852/100000: episode: 1471, duration: 0.003s, episode steps: 14, steps per second: 5235, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.090 [-1.548, 0.798], mean_best_reward: --
 31889/100000: episode: 1472, duration: 0.006s, episode steps: 37, steps per second: 5986, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.103 [-1.150, 0.550], mean_best_reward: --
 31985/100000: episode: 1473, duration: 0.015s, episode steps: 96, steps per second: 6336, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.310 [-1.851, 0.796], mean_best_reward: --
 32003/100000: episode: 1474, duration: 0.003s, episode steps: 18, steps per second: 5531, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.060 [-1.597, 1.012], mean_best_reward: --
 32036/100000: episode: 1475, duration: 0.006s, episode steps: 33, steps per second: 5906, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.083 [-1.197, 0.554], mean_best_reward: --
 32051/100000: episode: 1476, duration: 0.004s, episode steps: 15, steps per second: 4202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.076 [-1.571, 0.995], mean_best_reward: --
 32145/100000: episode: 1477, duration: 0.015s, episode steps: 94, steps per second: 6100, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.164 [-1.509, 1.107], mean_best_reward: --
 32163/100000: episode: 1478, duration: 0.003s, episode steps: 18, steps per second: 5391, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.076 [-1.443, 0.958], mean_best_reward: --
 32173/100000: episode: 1479, duration: 0.002s, episode steps: 10, steps per second: 4974, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.331, 2.265], mean_best_reward: --
 32184/100000: episode: 1480, duration: 0.002s, episode steps: 11, steps per second: 5102, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.109 [-2.264, 1.362], mean_best_reward: --
 32196/100000: episode: 1481, duration: 0.002s, episode steps: 12, steps per second: 5038, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.141 [-1.344, 2.327], mean_best_reward: --
 32212/100000: episode: 1482, duration: 0.003s, episode steps: 16, steps per second: 5495, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.100 [-1.470, 0.764], mean_best_reward: --
 32231/100000: episode: 1483, duration: 0.004s, episode steps: 19, steps per second: 5100, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.090 [-0.976, 0.581], mean_best_reward: --
 32249/100000: episode: 1484, duration: 0.003s, episode steps: 18, steps per second: 5539, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.080 [-1.164, 0.783], mean_best_reward: --
 32318/100000: episode: 1485, duration: 0.011s, episode steps: 69, steps per second: 6087, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.085 [-1.335, 1.398], mean_best_reward: --
 32335/100000: episode: 1486, duration: 0.003s, episode steps: 17, steps per second: 5370, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.090 [-1.956, 1.146], mean_best_reward: --
 32355/100000: episode: 1487, duration: 0.004s, episode steps: 20, steps per second: 4575, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.082 [-1.271, 0.748], mean_best_reward: --
 32480/100000: episode: 1488, duration: 0.020s, episode steps: 125, steps per second: 6288, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.109 [-1.874, 2.049], mean_best_reward: --
 32489/100000: episode: 1489, duration: 0.002s, episode steps: 9, steps per second: 4570, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.135 [-1.729, 0.940], mean_best_reward: --
 32577/100000: episode: 1490, duration: 0.014s, episode steps: 88, steps per second: 6305, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.165 [-1.165, 0.813], mean_best_reward: --
 32641/100000: episode: 1491, duration: 0.011s, episode steps: 64, steps per second: 5825, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.190 [-0.664, 1.059], mean_best_reward: --
 32653/100000: episode: 1492, duration: 0.003s, episode steps: 12, steps per second: 4001, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.102 [-1.541, 0.966], mean_best_reward: --
 32735/100000: episode: 1493, duration: 0.013s, episode steps: 82, steps per second: 6176, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.188 [-1.652, 0.794], mean_best_reward: --
 32756/100000: episode: 1494, duration: 0.004s, episode steps: 21, steps per second: 5558, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.071 [-1.889, 1.152], mean_best_reward: --
 32790/100000: episode: 1495, duration: 0.006s, episode steps: 34, steps per second: 5812, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.109 [-2.497, 1.199], mean_best_reward: --
 32810/100000: episode: 1496, duration: 0.004s, episode steps: 20, steps per second: 5619, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.046, 0.600], mean_best_reward: --
 32848/100000: episode: 1497, duration: 0.006s, episode steps: 38, steps per second: 6034, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-0.959, 0.389], mean_best_reward: --
 32922/100000: episode: 1498, duration: 0.012s, episode steps: 74, steps per second: 6226, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.015 [-1.434, 1.098], mean_best_reward: --
 32972/100000: episode: 1499, duration: 0.009s, episode steps: 50, steps per second: 5498, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.058 [-1.448, 0.809], mean_best_reward: --
 32984/100000: episode: 1500, duration: 0.002s, episode steps: 12, steps per second: 4995, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.121 [-1.220, 0.605], mean_best_reward: --
 33015/100000: episode: 1501, duration: 0.006s, episode steps: 31, steps per second: 5595, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.034 [-1.102, 0.727], mean_best_reward: 99.500000
 33045/100000: episode: 1502, duration: 0.005s, episode steps: 30, steps per second: 5924, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.046 [-0.892, 0.633], mean_best_reward: --
 33198/100000: episode: 1503, duration: 0.024s, episode steps: 153, steps per second: 6315, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.029 [-1.082, 0.915], mean_best_reward: --
 33215/100000: episode: 1504, duration: 0.004s, episode steps: 17, steps per second: 4856, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.114 [-0.996, 0.591], mean_best_reward: --
 33251/100000: episode: 1505, duration: 0.006s, episode steps: 36, steps per second: 6057, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.101 [-0.555, 0.911], mean_best_reward: --
 33332/100000: episode: 1506, duration: 0.013s, episode steps: 81, steps per second: 6137, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.300 [-2.607, 1.440], mean_best_reward: --
 33358/100000: episode: 1507, duration: 0.004s, episode steps: 26, steps per second: 5781, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-0.882, 0.601], mean_best_reward: --
 33384/100000: episode: 1508, duration: 0.004s, episode steps: 26, steps per second: 5868, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.342, 0.778], mean_best_reward: --
 33425/100000: episode: 1509, duration: 0.007s, episode steps: 41, steps per second: 6110, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.112 [-0.901, 0.364], mean_best_reward: --
 33550/100000: episode: 1510, duration: 0.020s, episode steps: 125, steps per second: 6225, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.226 [-1.640, 0.954], mean_best_reward: --
 33696/100000: episode: 1511, duration: 0.023s, episode steps: 146, steps per second: 6372, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.312 [-1.730, 0.905], mean_best_reward: --
 33804/100000: episode: 1512, duration: 0.018s, episode steps: 108, steps per second: 5936, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.148 [-1.839, 1.245], mean_best_reward: --
 33834/100000: episode: 1513, duration: 0.005s, episode steps: 30, steps per second: 5860, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.082 [-1.395, 0.804], mean_best_reward: --
 33853/100000: episode: 1514, duration: 0.004s, episode steps: 19, steps per second: 5312, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.087 [-1.027, 1.858], mean_best_reward: --
 33970/100000: episode: 1515, duration: 0.019s, episode steps: 117, steps per second: 6068, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.168 [-1.362, 0.937], mean_best_reward: --
 34003/100000: episode: 1516, duration: 0.006s, episode steps: 33, steps per second: 5970, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.039 [-1.092, 0.641], mean_best_reward: --
 34044/100000: episode: 1517, duration: 0.007s, episode steps: 41, steps per second: 5820, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.077 [-1.339, 0.588], mean_best_reward: --
 34157/100000: episode: 1518, duration: 0.019s, episode steps: 113, steps per second: 6056, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.098 [-1.294, 1.009], mean_best_reward: --
 34209/100000: episode: 1519, duration: 0.008s, episode steps: 52, steps per second: 6156, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.038 [-1.026, 0.840], mean_best_reward: --
 34227/100000: episode: 1520, duration: 0.003s, episode steps: 18, steps per second: 5539, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.068 [-1.171, 0.806], mean_best_reward: --
 34275/100000: episode: 1521, duration: 0.008s, episode steps: 48, steps per second: 5958, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.096 [-0.913, 0.634], mean_best_reward: --
 34355/100000: episode: 1522, duration: 0.013s, episode steps: 80, steps per second: 6170, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.116 [-0.991, 1.436], mean_best_reward: --
 34442/100000: episode: 1523, duration: 0.014s, episode steps: 87, steps per second: 6132, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.012 [-0.742, 1.410], mean_best_reward: --
 34477/100000: episode: 1524, duration: 0.006s, episode steps: 35, steps per second: 5975, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.127 [-0.422, 0.944], mean_best_reward: --
 34519/100000: episode: 1525, duration: 0.007s, episode steps: 42, steps per second: 6083, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.004 [-1.147, 0.638], mean_best_reward: --
 34554/100000: episode: 1526, duration: 0.006s, episode steps: 35, steps per second: 5755, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.127 [-0.697, 0.416], mean_best_reward: --
 34599/100000: episode: 1527, duration: 0.007s, episode steps: 45, steps per second: 6084, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.030 [-1.414, 0.595], mean_best_reward: --
 34618/100000: episode: 1528, duration: 0.004s, episode steps: 19, steps per second: 5044, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.056 [-0.822, 1.452], mean_best_reward: --
 34649/100000: episode: 1529, duration: 0.005s, episode steps: 31, steps per second: 5806, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.092 [-0.896, 0.403], mean_best_reward: --
 34718/100000: episode: 1530, duration: 0.012s, episode steps: 69, steps per second: 5720, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.121 [-0.768, 1.130], mean_best_reward: --
 34766/100000: episode: 1531, duration: 0.008s, episode steps: 48, steps per second: 6113, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.100 [-1.041, 0.740], mean_best_reward: --
 34824/100000: episode: 1532, duration: 0.010s, episode steps: 58, steps per second: 6047, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.127 [-1.316, 1.527], mean_best_reward: --
 34845/100000: episode: 1533, duration: 0.004s, episode steps: 21, steps per second: 5635, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.090 [-0.584, 1.331], mean_best_reward: --
 34955/100000: episode: 1534, duration: 0.018s, episode steps: 110, steps per second: 6005, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.013 [-1.297, 1.014], mean_best_reward: --
 34992/100000: episode: 1535, duration: 0.006s, episode steps: 37, steps per second: 5707, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.058 [-1.060, 0.778], mean_best_reward: --
 35032/100000: episode: 1536, duration: 0.007s, episode steps: 40, steps per second: 5959, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.161 [-1.194, 0.403], mean_best_reward: --
 35098/100000: episode: 1537, duration: 0.011s, episode steps: 66, steps per second: 6222, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.102 [-1.432, 1.141], mean_best_reward: --
 35129/100000: episode: 1538, duration: 0.005s, episode steps: 31, steps per second: 5863, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.139 [-0.683, 0.373], mean_best_reward: --
 35210/100000: episode: 1539, duration: 0.013s, episode steps: 81, steps per second: 6001, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.107 [-0.985, 1.106], mean_best_reward: --
 35234/100000: episode: 1540, duration: 0.005s, episode steps: 24, steps per second: 4483, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.003, 0.544], mean_best_reward: --
 35330/100000: episode: 1541, duration: 0.016s, episode steps: 96, steps per second: 6083, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.193 [-1.710, 1.235], mean_best_reward: --
 35440/100000: episode: 1542, duration: 0.018s, episode steps: 110, steps per second: 6249, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.337 [-1.468, 0.735], mean_best_reward: --
 35455/100000: episode: 1543, duration: 0.003s, episode steps: 15, steps per second: 5390, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.094 [-1.714, 2.840], mean_best_reward: --
 35612/100000: episode: 1544, duration: 0.025s, episode steps: 157, steps per second: 6241, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.461 [-2.411, 0.658], mean_best_reward: --
 35632/100000: episode: 1545, duration: 0.004s, episode steps: 20, steps per second: 5594, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.083 [-1.140, 0.575], mean_best_reward: --
 35658/100000: episode: 1546, duration: 0.004s, episode steps: 26, steps per second: 5812, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.089 [-1.463, 0.629], mean_best_reward: --
 35695/100000: episode: 1547, duration: 0.006s, episode steps: 37, steps per second: 5959, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.113 [-0.878, 0.443], mean_best_reward: --
 35726/100000: episode: 1548, duration: 0.005s, episode steps: 31, steps per second: 5785, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.052 [-1.852, 0.959], mean_best_reward: --
 35743/100000: episode: 1549, duration: 0.003s, episode steps: 17, steps per second: 5281, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.087 [-1.516, 0.776], mean_best_reward: --
 35788/100000: episode: 1550, duration: 0.008s, episode steps: 45, steps per second: 5784, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.041 [-0.861, 1.116], mean_best_reward: --
 35864/100000: episode: 1551, duration: 0.013s, episode steps: 76, steps per second: 5751, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.255 [-2.464, 1.997], mean_best_reward: 102.000000
 35886/100000: episode: 1552, duration: 0.004s, episode steps: 22, steps per second: 5532, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.071 [-1.585, 0.941], mean_best_reward: --
 35909/100000: episode: 1553, duration: 0.004s, episode steps: 23, steps per second: 5764, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.086 [-1.396, 0.614], mean_best_reward: --
 35960/100000: episode: 1554, duration: 0.009s, episode steps: 51, steps per second: 5858, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.147 [-0.620, 1.867], mean_best_reward: --
 36064/100000: episode: 1555, duration: 0.016s, episode steps: 104, steps per second: 6319, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.401 [-2.465, 1.178], mean_best_reward: --
 36116/100000: episode: 1556, duration: 0.009s, episode steps: 52, steps per second: 5881, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.107 [-1.729, 1.637], mean_best_reward: --
 36221/100000: episode: 1557, duration: 0.017s, episode steps: 105, steps per second: 6344, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.207 [-1.773, 1.150], mean_best_reward: --
 36318/100000: episode: 1558, duration: 0.016s, episode steps: 97, steps per second: 6198, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.210 [-1.763, 0.951], mean_best_reward: --
 36369/100000: episode: 1559, duration: 0.009s, episode steps: 51, steps per second: 5875, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.049 [-1.569, 1.697], mean_best_reward: --
 36438/100000: episode: 1560, duration: 0.012s, episode steps: 69, steps per second: 5846, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.021 [-1.142, 1.209], mean_best_reward: --
 36544/100000: episode: 1561, duration: 0.017s, episode steps: 106, steps per second: 6215, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.539 [-2.442, 1.040], mean_best_reward: --
 36620/100000: episode: 1562, duration: 0.012s, episode steps: 76, steps per second: 6285, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.167 [-1.904, 1.357], mean_best_reward: --
 36720/100000: episode: 1563, duration: 0.016s, episode steps: 100, steps per second: 6074, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.121 [-1.370, 1.277], mean_best_reward: --
 36737/100000: episode: 1564, duration: 0.003s, episode steps: 17, steps per second: 4911, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.080 [-0.974, 1.413], mean_best_reward: --
 36799/100000: episode: 1565, duration: 0.010s, episode steps: 62, steps per second: 6238, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.105 [-0.823, 0.999], mean_best_reward: --
 36825/100000: episode: 1566, duration: 0.005s, episode steps: 26, steps per second: 5443, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-0.929, 1.348], mean_best_reward: --
 36837/100000: episode: 1567, duration: 0.002s, episode steps: 12, steps per second: 5013, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.119 [-1.225, 2.115], mean_best_reward: --
 36852/100000: episode: 1568, duration: 0.003s, episode steps: 15, steps per second: 5435, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.080 [-1.481, 0.952], mean_best_reward: --
 36881/100000: episode: 1569, duration: 0.005s, episode steps: 29, steps per second: 5924, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.017 [-0.936, 1.339], mean_best_reward: --
 36907/100000: episode: 1570, duration: 0.004s, episode steps: 26, steps per second: 5826, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.052 [-1.317, 2.060], mean_best_reward: --
 36975/100000: episode: 1571, duration: 0.011s, episode steps: 68, steps per second: 5933, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.031 [-0.992, 1.000], mean_best_reward: --
 37048/100000: episode: 1572, duration: 0.012s, episode steps: 73, steps per second: 6073, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.223 [-1.851, 1.268], mean_best_reward: --
 37073/100000: episode: 1573, duration: 0.004s, episode steps: 25, steps per second: 5782, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.082 [-0.769, 1.499], mean_best_reward: --
 37104/100000: episode: 1574, duration: 0.006s, episode steps: 31, steps per second: 5583, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.064 [-0.457, 1.000], mean_best_reward: --
 37234/100000: episode: 1575, duration: 0.021s, episode steps: 130, steps per second: 6314, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.133 [-1.137, 0.940], mean_best_reward: --
 37302/100000: episode: 1576, duration: 0.011s, episode steps: 68, steps per second: 6019, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.090 [-0.860, 2.187], mean_best_reward: --
 37346/100000: episode: 1577, duration: 0.007s, episode steps: 44, steps per second: 6083, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.055 [-1.108, 0.873], mean_best_reward: --
 37457/100000: episode: 1578, duration: 0.017s, episode steps: 111, steps per second: 6367, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.463 [-2.427, 1.439], mean_best_reward: --
 37480/100000: episode: 1579, duration: 0.004s, episode steps: 23, steps per second: 5742, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.039 [-1.866, 1.321], mean_best_reward: --
 37553/100000: episode: 1580, duration: 0.013s, episode steps: 73, steps per second: 5790, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.036 [-1.191, 1.244], mean_best_reward: --
 37570/100000: episode: 1581, duration: 0.003s, episode steps: 17, steps per second: 5206, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.086 [-2.559, 1.582], mean_best_reward: --
 37599/100000: episode: 1582, duration: 0.006s, episode steps: 29, steps per second: 5178, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.104 [-1.006, 0.560], mean_best_reward: --
 37646/100000: episode: 1583, duration: 0.008s, episode steps: 47, steps per second: 6137, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.088 [-0.509, 1.309], mean_best_reward: --
 37689/100000: episode: 1584, duration: 0.007s, episode steps: 43, steps per second: 5807, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: 0.002 [-1.365, 1.934], mean_best_reward: --
 37725/100000: episode: 1585, duration: 0.006s, episode steps: 36, steps per second: 5913, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.016 [-1.063, 0.600], mean_best_reward: --
 37764/100000: episode: 1586, duration: 0.006s, episode steps: 39, steps per second: 6006, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.123 [-0.436, 0.913], mean_best_reward: --
 37774/100000: episode: 1587, duration: 0.002s, episode steps: 10, steps per second: 4304, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.117 [-1.398, 2.240], mean_best_reward: --
 37820/100000: episode: 1588, duration: 0.009s, episode steps: 46, steps per second: 5281, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.413 [0.000, 1.000], mean observation: -0.177 [-1.727, 1.474], mean_best_reward: --
 37891/100000: episode: 1589, duration: 0.012s, episode steps: 71, steps per second: 5993, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.039 [-1.516, 1.132], mean_best_reward: --
 37903/100000: episode: 1590, duration: 0.002s, episode steps: 12, steps per second: 5180, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.097 [-0.631, 1.072], mean_best_reward: --
 37937/100000: episode: 1591, duration: 0.006s, episode steps: 34, steps per second: 5952, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.147 [-0.819, 0.349], mean_best_reward: --
 37968/100000: episode: 1592, duration: 0.006s, episode steps: 31, steps per second: 5546, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.067 [-0.837, 1.680], mean_best_reward: --
 38072/100000: episode: 1593, duration: 0.017s, episode steps: 104, steps per second: 6229, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.531 [-2.946, 0.831], mean_best_reward: --
 38138/100000: episode: 1594, duration: 0.011s, episode steps: 66, steps per second: 6090, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.034 [-1.249, 0.776], mean_best_reward: --
 38159/100000: episode: 1595, duration: 0.004s, episode steps: 21, steps per second: 5335, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.048 [-1.620, 0.989], mean_best_reward: --
 38215/100000: episode: 1596, duration: 0.009s, episode steps: 56, steps per second: 6197, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.023 [-1.718, 1.867], mean_best_reward: --
 38325/100000: episode: 1597, duration: 0.017s, episode steps: 110, steps per second: 6358, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.131 [-1.396, 0.814], mean_best_reward: --
 38460/100000: episode: 1598, duration: 0.022s, episode steps: 135, steps per second: 6201, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.188 [-1.280, 1.469], mean_best_reward: --
 38548/100000: episode: 1599, duration: 0.014s, episode steps: 88, steps per second: 6209, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.003 [-1.005, 1.268], mean_best_reward: --
 38595/100000: episode: 1600, duration: 0.008s, episode steps: 47, steps per second: 6102, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.004 [-1.062, 0.787], mean_best_reward: --
 38690/100000: episode: 1601, duration: 0.016s, episode steps: 95, steps per second: 6115, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.226 [-0.946, 1.661], mean_best_reward: 72.000000
 38705/100000: episode: 1602, duration: 0.003s, episode steps: 15, steps per second: 4666, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.082 [-0.971, 0.629], mean_best_reward: --
 38831/100000: episode: 1603, duration: 0.020s, episode steps: 126, steps per second: 6307, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.060 [-1.618, 0.616], mean_best_reward: --
 38860/100000: episode: 1604, duration: 0.005s, episode steps: 29, steps per second: 5616, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.115 [-0.759, 0.475], mean_best_reward: --
 38897/100000: episode: 1605, duration: 0.006s, episode steps: 37, steps per second: 5867, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.043 [-0.967, 0.439], mean_best_reward: --
 38960/100000: episode: 1606, duration: 0.010s, episode steps: 63, steps per second: 6221, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.133 [-1.481, 1.145], mean_best_reward: --
 39009/100000: episode: 1607, duration: 0.008s, episode steps: 49, steps per second: 5838, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.074 [-1.489, 0.569], mean_best_reward: --
 39059/100000: episode: 1608, duration: 0.008s, episode steps: 50, steps per second: 6119, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.340 [0.000, 1.000], mean observation: -0.055 [-3.018, 3.492], mean_best_reward: --
 39187/100000: episode: 1609, duration: 0.020s, episode steps: 128, steps per second: 6304, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.178 [-0.850, 1.360], mean_best_reward: --
 39218/100000: episode: 1610, duration: 0.005s, episode steps: 31, steps per second: 5907, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.083 [-0.995, 0.571], mean_best_reward: --
 39230/100000: episode: 1611, duration: 0.003s, episode steps: 12, steps per second: 4710, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.086 [-2.020, 1.346], mean_best_reward: --
 39271/100000: episode: 1612, duration: 0.007s, episode steps: 41, steps per second: 5961, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.390 [0.000, 1.000], mean observation: -0.080 [-1.954, 2.314], mean_best_reward: --
 39283/100000: episode: 1613, duration: 0.003s, episode steps: 12, steps per second: 4466, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.106 [-1.866, 1.169], mean_best_reward: --
 39305/100000: episode: 1614, duration: 0.004s, episode steps: 22, steps per second: 5340, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.118, 0.452], mean_best_reward: --
 39322/100000: episode: 1615, duration: 0.003s, episode steps: 17, steps per second: 5088, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.110 [-0.989, 0.569], mean_best_reward: --
 39382/100000: episode: 1616, duration: 0.010s, episode steps: 60, steps per second: 6209, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.012 [-1.195, 1.291], mean_best_reward: --
 39438/100000: episode: 1617, duration: 0.009s, episode steps: 56, steps per second: 6149, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: -0.124 [-2.701, 2.815], mean_best_reward: --
 39524/100000: episode: 1618, duration: 0.014s, episode steps: 86, steps per second: 6119, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.100 [-2.295, 2.434], mean_best_reward: --
 39596/100000: episode: 1619, duration: 0.012s, episode steps: 72, steps per second: 5993, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.107 [-1.667, 1.381], mean_best_reward: --
 39673/100000: episode: 1620, duration: 0.013s, episode steps: 77, steps per second: 6147, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.377 [0.000, 1.000], mean observation: -0.177 [-3.588, 3.428], mean_best_reward: --
 39707/100000: episode: 1621, duration: 0.006s, episode steps: 34, steps per second: 5712, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: -0.059 [-1.527, 1.562], mean_best_reward: --
 39740/100000: episode: 1622, duration: 0.006s, episode steps: 33, steps per second: 5965, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: 0.010 [-1.351, 1.833], mean_best_reward: --
 39754/100000: episode: 1623, duration: 0.003s, episode steps: 14, steps per second: 5371, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.079 [-1.345, 0.836], mean_best_reward: --
 39771/100000: episode: 1624, duration: 0.003s, episode steps: 17, steps per second: 5531, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.076 [-2.352, 1.414], mean_best_reward: --
 39796/100000: episode: 1625, duration: 0.005s, episode steps: 25, steps per second: 5494, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.123 [-1.270, 0.388], mean_best_reward: --
 39845/100000: episode: 1626, duration: 0.009s, episode steps: 49, steps per second: 5588, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.367 [0.000, 1.000], mean observation: -0.153 [-2.411, 2.196], mean_best_reward: --
 39885/100000: episode: 1627, duration: 0.007s, episode steps: 40, steps per second: 5890, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.097 [-1.011, 0.694], mean_best_reward: --
 39947/100000: episode: 1628, duration: 0.010s, episode steps: 62, steps per second: 6214, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.500, 0.571], mean_best_reward: --
 40007/100000: episode: 1629, duration: 0.010s, episode steps: 60, steps per second: 5802, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.100 [-1.557, 1.845], mean_best_reward: --
 40050/100000: episode: 1630, duration: 0.007s, episode steps: 43, steps per second: 5940, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.372 [0.000, 1.000], mean observation: -0.052 [-2.069, 2.431], mean_best_reward: --
 40088/100000: episode: 1631, duration: 0.007s, episode steps: 38, steps per second: 5722, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.006 [-1.320, 1.793], mean_best_reward: --
 40121/100000: episode: 1632, duration: 0.006s, episode steps: 33, steps per second: 5282, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.095 [-1.268, 0.465], mean_best_reward: --
 40163/100000: episode: 1633, duration: 0.007s, episode steps: 42, steps per second: 5692, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.070 [-0.878, 0.578], mean_best_reward: --
 40187/100000: episode: 1634, duration: 0.004s, episode steps: 24, steps per second: 5806, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.112 [-1.209, 0.821], mean_best_reward: --
 40236/100000: episode: 1635, duration: 0.008s, episode steps: 49, steps per second: 6158, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.099 [-1.107, 0.595], mean_best_reward: --
 40268/100000: episode: 1636, duration: 0.006s, episode steps: 32, steps per second: 5567, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.406 [0.000, 1.000], mean observation: -0.030 [-1.387, 1.692], mean_best_reward: --
 40325/100000: episode: 1637, duration: 0.009s, episode steps: 57, steps per second: 6208, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.160 [-0.968, 0.611], mean_best_reward: --
 40347/100000: episode: 1638, duration: 0.004s, episode steps: 22, steps per second: 5202, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.316, 0.568], mean_best_reward: --
 40415/100000: episode: 1639, duration: 0.011s, episode steps: 68, steps per second: 6069, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.397 [0.000, 1.000], mean observation: -0.212 [-2.691, 2.395], mean_best_reward: --
 40433/100000: episode: 1640, duration: 0.004s, episode steps: 18, steps per second: 4917, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.098, 0.634], mean_best_reward: --
 40457/100000: episode: 1641, duration: 0.004s, episode steps: 24, steps per second: 5706, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.073 [-1.599, 0.787], mean_best_reward: --
 40513/100000: episode: 1642, duration: 0.009s, episode steps: 56, steps per second: 6191, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.099 [-1.203, 1.301], mean_best_reward: --
 40536/100000: episode: 1643, duration: 0.004s, episode steps: 23, steps per second: 5278, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.094 [-1.218, 0.594], mean_best_reward: --
 40572/100000: episode: 1644, duration: 0.006s, episode steps: 36, steps per second: 6012, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-1.374, 0.829], mean_best_reward: --
 40593/100000: episode: 1645, duration: 0.004s, episode steps: 21, steps per second: 5677, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.081 [-1.115, 0.628], mean_best_reward: --
 40615/100000: episode: 1646, duration: 0.004s, episode steps: 22, steps per second: 5450, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.639, 1.356], mean_best_reward: --
 40638/100000: episode: 1647, duration: 0.004s, episode steps: 23, steps per second: 5688, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.027 [-1.894, 2.735], mean_best_reward: --
 40749/100000: episode: 1648, duration: 0.018s, episode steps: 111, steps per second: 6088, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.136 [-1.961, 2.532], mean_best_reward: --
 40787/100000: episode: 1649, duration: 0.006s, episode steps: 38, steps per second: 6017, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.016 [-1.558, 0.971], mean_best_reward: --
 40818/100000: episode: 1650, duration: 0.006s, episode steps: 31, steps per second: 5544, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.052 [-1.216, 0.756], mean_best_reward: --
 40897/100000: episode: 1651, duration: 0.013s, episode steps: 79, steps per second: 6195, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.405 [0.000, 1.000], mean observation: -0.149 [-2.843, 2.980], mean_best_reward: 116.000000
 40906/100000: episode: 1652, duration: 0.002s, episode steps: 9, steps per second: 4068, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.150 [-1.177, 1.965], mean_best_reward: --
 40921/100000: episode: 1653, duration: 0.003s, episode steps: 15, steps per second: 5421, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.813, 1.281], mean_best_reward: --
 40934/100000: episode: 1654, duration: 0.002s, episode steps: 13, steps per second: 5245, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.083 [-1.341, 2.038], mean_best_reward: --
 40948/100000: episode: 1655, duration: 0.003s, episode steps: 14, steps per second: 4025, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.080 [-1.024, 1.634], mean_best_reward: --
 40974/100000: episode: 1656, duration: 0.005s, episode steps: 26, steps per second: 5746, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.063 [-0.757, 1.357], mean_best_reward: --
 40997/100000: episode: 1657, duration: 0.004s, episode steps: 23, steps per second: 5368, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.081 [-1.233, 0.625], mean_best_reward: --
 41018/100000: episode: 1658, duration: 0.004s, episode steps: 21, steps per second: 5686, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.048 [-1.752, 1.005], mean_best_reward: --
 41042/100000: episode: 1659, duration: 0.004s, episode steps: 24, steps per second: 5806, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-0.966, 0.546], mean_best_reward: --
 41066/100000: episode: 1660, duration: 0.004s, episode steps: 24, steps per second: 5460, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.074 [-1.429, 0.632], mean_best_reward: --
 41076/100000: episode: 1661, duration: 0.002s, episode steps: 10, steps per second: 4877, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.942, 3.058], mean_best_reward: --
 41105/100000: episode: 1662, duration: 0.005s, episode steps: 29, steps per second: 5898, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.034 [-0.596, 1.062], mean_best_reward: --
 41145/100000: episode: 1663, duration: 0.007s, episode steps: 40, steps per second: 5953, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.055 [-0.618, 1.453], mean_best_reward: --
 41192/100000: episode: 1664, duration: 0.008s, episode steps: 47, steps per second: 5811, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.158 [-1.248, 0.574], mean_best_reward: --
 41260/100000: episode: 1665, duration: 0.011s, episode steps: 68, steps per second: 5942, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.056 [-1.049, 0.675], mean_best_reward: --
 41270/100000: episode: 1666, duration: 0.002s, episode steps: 10, steps per second: 5011, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.093 [-1.536, 1.027], mean_best_reward: --
 41298/100000: episode: 1667, duration: 0.005s, episode steps: 28, steps per second: 5933, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.119 [-0.575, 1.417], mean_best_reward: --
 41361/100000: episode: 1668, duration: 0.010s, episode steps: 63, steps per second: 6036, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.176 [-1.504, 1.165], mean_best_reward: --
 41377/100000: episode: 1669, duration: 0.003s, episode steps: 16, steps per second: 5471, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.073 [-0.964, 1.396], mean_best_reward: --
 41399/100000: episode: 1670, duration: 0.004s, episode steps: 22, steps per second: 5742, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-0.988, 0.633], mean_best_reward: --
 41407/100000: episode: 1671, duration: 0.002s, episode steps: 8, steps per second: 4761, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.139 [-1.394, 2.223], mean_best_reward: --
 41421/100000: episode: 1672, duration: 0.003s, episode steps: 14, steps per second: 5385, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.120 [-0.947, 1.762], mean_best_reward: --
 41439/100000: episode: 1673, duration: 0.004s, episode steps: 18, steps per second: 5008, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.098 [-0.799, 1.698], mean_best_reward: --
 41447/100000: episode: 1674, duration: 0.002s, episode steps: 8, steps per second: 4748, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.584, 2.576], mean_best_reward: --
 41478/100000: episode: 1675, duration: 0.006s, episode steps: 31, steps per second: 5614, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.079 [-0.579, 1.042], mean_best_reward: --
 41510/100000: episode: 1676, duration: 0.005s, episode steps: 32, steps per second: 5832, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.112 [-0.802, 0.386], mean_best_reward: --
 41545/100000: episode: 1677, duration: 0.006s, episode steps: 35, steps per second: 5921, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.048 [-0.767, 1.418], mean_best_reward: --
 41567/100000: episode: 1678, duration: 0.004s, episode steps: 22, steps per second: 5755, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.025 [-1.956, 2.879], mean_best_reward: --
 41580/100000: episode: 1679, duration: 0.003s, episode steps: 13, steps per second: 5025, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.123 [-1.377, 0.749], mean_best_reward: --
 41600/100000: episode: 1680, duration: 0.004s, episode steps: 20, steps per second: 5601, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.358, 0.914], mean_best_reward: --
 41618/100000: episode: 1681, duration: 0.003s, episode steps: 18, steps per second: 5209, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.084 [-1.099, 0.641], mean_best_reward: --
 41647/100000: episode: 1682, duration: 0.005s, episode steps: 29, steps per second: 5906, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.057 [-1.003, 1.889], mean_best_reward: --
 41659/100000: episode: 1683, duration: 0.002s, episode steps: 12, steps per second: 5210, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.123 [-1.560, 2.524], mean_best_reward: --
 41670/100000: episode: 1684, duration: 0.002s, episode steps: 11, steps per second: 5149, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-0.839, 1.469], mean_best_reward: --
 41685/100000: episode: 1685, duration: 0.003s, episode steps: 15, steps per second: 5342, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.105 [-1.384, 0.785], mean_best_reward: --
 41745/100000: episode: 1686, duration: 0.010s, episode steps: 60, steps per second: 5913, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.229 [-1.704, 1.147], mean_best_reward: --
 41782/100000: episode: 1687, duration: 0.007s, episode steps: 37, steps per second: 5643, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.047 [-0.816, 0.557], mean_best_reward: --
 41797/100000: episode: 1688, duration: 0.003s, episode steps: 15, steps per second: 5416, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.086 [-1.197, 1.888], mean_best_reward: --
 41820/100000: episode: 1689, duration: 0.004s, episode steps: 23, steps per second: 5709, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.050 [-1.008, 1.816], mean_best_reward: --
 41842/100000: episode: 1690, duration: 0.004s, episode steps: 22, steps per second: 5752, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.098 [-0.904, 0.564], mean_best_reward: --
 41857/100000: episode: 1691, duration: 0.003s, episode steps: 15, steps per second: 5465, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.105 [-1.327, 0.560], mean_best_reward: --
 41918/100000: episode: 1692, duration: 0.010s, episode steps: 61, steps per second: 6204, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.108 [-1.101, 0.671], mean_best_reward: --
 41941/100000: episode: 1693, duration: 0.004s, episode steps: 23, steps per second: 5758, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.067 [-1.459, 0.789], mean_best_reward: --
 41955/100000: episode: 1694, duration: 0.003s, episode steps: 14, steps per second: 5359, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.092, 0.547], mean_best_reward: --
 41964/100000: episode: 1695, duration: 0.002s, episode steps: 9, steps per second: 4797, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.780, 2.764], mean_best_reward: --
 42000/100000: episode: 1696, duration: 0.006s, episode steps: 36, steps per second: 6020, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.262, 0.881], mean_best_reward: --
 42012/100000: episode: 1697, duration: 0.003s, episode steps: 12, steps per second: 4429, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.135 [-0.940, 1.695], mean_best_reward: --
 42033/100000: episode: 1698, duration: 0.004s, episode steps: 21, steps per second: 5671, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.098 [-0.609, 0.964], mean_best_reward: --
 42054/100000: episode: 1699, duration: 0.004s, episode steps: 21, steps per second: 5599, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.117 [-0.731, 1.182], mean_best_reward: --
 42071/100000: episode: 1700, duration: 0.003s, episode steps: 17, steps per second: 5500, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.091 [-1.526, 2.449], mean_best_reward: --
 42083/100000: episode: 1701, duration: 0.003s, episode steps: 12, steps per second: 4645, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-0.961, 1.667], mean_best_reward: 114.500000
 42141/100000: episode: 1702, duration: 0.009s, episode steps: 58, steps per second: 6204, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.701, 1.089], mean_best_reward: --
 42188/100000: episode: 1703, duration: 0.008s, episode steps: 47, steps per second: 6155, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.070 [-0.911, 0.354], mean_best_reward: --
 42201/100000: episode: 1704, duration: 0.003s, episode steps: 13, steps per second: 5042, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.103 [-1.707, 1.149], mean_best_reward: --
 42235/100000: episode: 1705, duration: 0.006s, episode steps: 34, steps per second: 5919, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.127 [-0.923, 0.636], mean_best_reward: --
 42254/100000: episode: 1706, duration: 0.003s, episode steps: 19, steps per second: 5617, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.038 [-1.391, 0.965], mean_best_reward: --
 42265/100000: episode: 1707, duration: 0.002s, episode steps: 11, steps per second: 5089, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.118 [-1.172, 1.813], mean_best_reward: --
 42307/100000: episode: 1708, duration: 0.007s, episode steps: 42, steps per second: 5827, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.101 [-0.793, 0.386], mean_best_reward: --
 42322/100000: episode: 1709, duration: 0.003s, episode steps: 15, steps per second: 4836, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.107 [-0.845, 1.579], mean_best_reward: --
 42346/100000: episode: 1710, duration: 0.004s, episode steps: 24, steps per second: 5806, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.064 [-1.539, 0.825], mean_best_reward: --
 42360/100000: episode: 1711, duration: 0.003s, episode steps: 14, steps per second: 5391, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.085 [-1.573, 1.007], mean_best_reward: --
 42372/100000: episode: 1712, duration: 0.002s, episode steps: 12, steps per second: 5177, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.128 [-1.392, 2.306], mean_best_reward: --
 42381/100000: episode: 1713, duration: 0.002s, episode steps: 9, steps per second: 4913, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.152 [-1.534, 2.509], mean_best_reward: --
 42404/100000: episode: 1714, duration: 0.004s, episode steps: 23, steps per second: 5278, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.093 [-0.957, 1.876], mean_best_reward: --
 42433/100000: episode: 1715, duration: 0.005s, episode steps: 29, steps per second: 5909, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.109 [-0.961, 0.301], mean_best_reward: --
 42447/100000: episode: 1716, duration: 0.003s, episode steps: 14, steps per second: 5364, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.074 [-1.940, 1.214], mean_best_reward: --
 42496/100000: episode: 1717, duration: 0.008s, episode steps: 49, steps per second: 5902, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: 0.102 [-0.960, 2.206], mean_best_reward: --
 42541/100000: episode: 1718, duration: 0.008s, episode steps: 45, steps per second: 5496, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.049 [-0.553, 0.942], mean_best_reward: --
 42559/100000: episode: 1719, duration: 0.003s, episode steps: 18, steps per second: 5523, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.064 [-1.596, 2.461], mean_best_reward: --
 42572/100000: episode: 1720, duration: 0.003s, episode steps: 13, steps per second: 4675, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.104 [-1.592, 0.978], mean_best_reward: --
 42675/100000: episode: 1721, duration: 0.017s, episode steps: 103, steps per second: 6233, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.159 [-1.445, 0.920], mean_best_reward: --
 42702/100000: episode: 1722, duration: 0.005s, episode steps: 27, steps per second: 5812, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.052 [-0.627, 1.005], mean_best_reward: --
 42711/100000: episode: 1723, duration: 0.002s, episode steps: 9, steps per second: 4870, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.123 [-0.987, 1.710], mean_best_reward: --
 42729/100000: episode: 1724, duration: 0.003s, episode steps: 18, steps per second: 5546, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.035 [-1.610, 2.389], mean_best_reward: --
 42746/100000: episode: 1725, duration: 0.003s, episode steps: 17, steps per second: 5509, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.033 [-2.781, 1.953], mean_best_reward: --
 42776/100000: episode: 1726, duration: 0.005s, episode steps: 30, steps per second: 5857, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.075 [-1.082, 0.640], mean_best_reward: --
 42785/100000: episode: 1727, duration: 0.002s, episode steps: 9, steps per second: 4891, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.116 [-1.413, 2.249], mean_best_reward: --
 42796/100000: episode: 1728, duration: 0.002s, episode steps: 11, steps per second: 5094, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.130 [-1.380, 2.303], mean_best_reward: --
 42817/100000: episode: 1729, duration: 0.004s, episode steps: 21, steps per second: 5238, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.050 [-1.736, 2.697], mean_best_reward: --
 42833/100000: episode: 1730, duration: 0.003s, episode steps: 16, steps per second: 5497, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.089 [-1.983, 1.211], mean_best_reward: --
 42846/100000: episode: 1731, duration: 0.002s, episode steps: 13, steps per second: 5218, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.118 [-1.458, 0.779], mean_best_reward: --
 42866/100000: episode: 1732, duration: 0.004s, episode steps: 20, steps per second: 5665, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.754, 1.548], mean_best_reward: --
 42881/100000: episode: 1733, duration: 0.003s, episode steps: 15, steps per second: 5389, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.068 [-1.409, 2.214], mean_best_reward: --
 42899/100000: episode: 1734, duration: 0.003s, episode steps: 18, steps per second: 5571, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.056 [-1.184, 1.956], mean_best_reward: --
 42916/100000: episode: 1735, duration: 0.003s, episode steps: 17, steps per second: 5414, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.095 [-0.749, 1.335], mean_best_reward: --
 42927/100000: episode: 1736, duration: 0.002s, episode steps: 11, steps per second: 4960, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.113 [-1.593, 2.475], mean_best_reward: --
 42939/100000: episode: 1737, duration: 0.002s, episode steps: 12, steps per second: 5192, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.111 [-1.411, 2.099], mean_best_reward: --
 43003/100000: episode: 1738, duration: 0.010s, episode steps: 64, steps per second: 6271, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.199 [-1.308, 0.623], mean_best_reward: --
 43017/100000: episode: 1739, duration: 0.003s, episode steps: 14, steps per second: 4722, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.083 [-2.222, 1.402], mean_best_reward: --
 43064/100000: episode: 1740, duration: 0.008s, episode steps: 47, steps per second: 6079, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.115 [-0.956, 0.395], mean_best_reward: --
 43178/100000: episode: 1741, duration: 0.019s, episode steps: 114, steps per second: 6127, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.513 [-2.409, 0.947], mean_best_reward: --
 43192/100000: episode: 1742, duration: 0.003s, episode steps: 14, steps per second: 5278, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.096 [-1.256, 0.593], mean_best_reward: --
 43222/100000: episode: 1743, duration: 0.005s, episode steps: 30, steps per second: 5523, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.087 [-1.493, 0.585], mean_best_reward: --
 43234/100000: episode: 1744, duration: 0.002s, episode steps: 12, steps per second: 5128, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.131 [-2.638, 1.579], mean_best_reward: --
 43247/100000: episode: 1745, duration: 0.002s, episode steps: 13, steps per second: 5264, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.073 [-1.199, 1.777], mean_best_reward: --
 43277/100000: episode: 1746, duration: 0.005s, episode steps: 30, steps per second: 5901, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-0.961, 0.587], mean_best_reward: --
 43316/100000: episode: 1747, duration: 0.007s, episode steps: 39, steps per second: 5833, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.082 [-1.373, 0.561], mean_best_reward: --
 43325/100000: episode: 1748, duration: 0.002s, episode steps: 9, steps per second: 4797, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.450, 1.592], mean_best_reward: --
 43337/100000: episode: 1749, duration: 0.002s, episode steps: 12, steps per second: 4977, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.125 [-2.077, 1.190], mean_best_reward: --
 43350/100000: episode: 1750, duration: 0.003s, episode steps: 13, steps per second: 4810, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.111 [-1.462, 0.790], mean_best_reward: --
 43360/100000: episode: 1751, duration: 0.002s, episode steps: 10, steps per second: 4310, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.134 [-2.636, 1.605], mean_best_reward: 140.500000
 43422/100000: episode: 1752, duration: 0.010s, episode steps: 62, steps per second: 6164, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.141 [-1.683, 0.828], mean_best_reward: --
 43461/100000: episode: 1753, duration: 0.007s, episode steps: 39, steps per second: 5896, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.052 [-0.580, 1.513], mean_best_reward: --
 43474/100000: episode: 1754, duration: 0.003s, episode steps: 13, steps per second: 5085, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.091 [-1.633, 1.034], mean_best_reward: --
 43492/100000: episode: 1755, duration: 0.003s, episode steps: 18, steps per second: 5548, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.091 [-2.293, 1.397], mean_best_reward: --
 43563/100000: episode: 1756, duration: 0.011s, episode steps: 71, steps per second: 6279, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.408 [0.000, 1.000], mean observation: -0.009 [-2.488, 2.995], mean_best_reward: --
 43582/100000: episode: 1757, duration: 0.003s, episode steps: 19, steps per second: 5529, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.102 [-1.223, 0.635], mean_best_reward: --
 43678/100000: episode: 1758, duration: 0.016s, episode steps: 96, steps per second: 6110, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.005 [-1.112, 1.367], mean_best_reward: --
 43688/100000: episode: 1759, duration: 0.002s, episode steps: 10, steps per second: 4931, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.110 [-1.999, 3.030], mean_best_reward: --
 43808/100000: episode: 1760, duration: 0.019s, episode steps: 120, steps per second: 6388, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.149 [-1.713, 1.486], mean_best_reward: --
 43822/100000: episode: 1761, duration: 0.003s, episode steps: 14, steps per second: 5197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.120 [-0.774, 1.559], mean_best_reward: --
 43864/100000: episode: 1762, duration: 0.007s, episode steps: 42, steps per second: 5873, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.013 [-1.130, 1.404], mean_best_reward: --
 44021/100000: episode: 1763, duration: 0.025s, episode steps: 157, steps per second: 6290, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.004 [-1.681, 1.748], mean_best_reward: --
 44054/100000: episode: 1764, duration: 0.006s, episode steps: 33, steps per second: 5734, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.096 [-0.571, 1.316], mean_best_reward: --
 44188/100000: episode: 1765, duration: 0.021s, episode steps: 134, steps per second: 6327, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.098 [-1.364, 0.971], mean_best_reward: --
 44301/100000: episode: 1766, duration: 0.018s, episode steps: 113, steps per second: 6164, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.056 [-0.930, 1.282], mean_best_reward: --
 44432/100000: episode: 1767, duration: 0.020s, episode steps: 131, steps per second: 6392, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.204 [-1.305, 1.007], mean_best_reward: --
 44494/100000: episode: 1768, duration: 0.010s, episode steps: 62, steps per second: 6170, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-1.005, 0.801], mean_best_reward: --
 44510/100000: episode: 1769, duration: 0.004s, episode steps: 16, steps per second: 4453, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.098 [-1.385, 0.751], mean_best_reward: --
 44577/100000: episode: 1770, duration: 0.011s, episode steps: 67, steps per second: 6179, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.403 [0.000, 1.000], mean observation: -0.088 [-2.433, 2.542], mean_best_reward: --
 44727/100000: episode: 1771, duration: 0.024s, episode steps: 150, steps per second: 6372, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.230 [-1.168, 0.773], mean_best_reward: --
 44914/100000: episode: 1772, duration: 0.030s, episode steps: 187, steps per second: 6249, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.245 [-1.596, 0.778], mean_best_reward: --
 44948/100000: episode: 1773, duration: 0.006s, episode steps: 34, steps per second: 5904, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.952, 1.357], mean_best_reward: --
 44982/100000: episode: 1774, duration: 0.006s, episode steps: 34, steps per second: 5884, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.112 [-0.296, 0.827], mean_best_reward: --
 45009/100000: episode: 1775, duration: 0.005s, episode steps: 27, steps per second: 5864, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.110 [-0.933, 0.611], mean_best_reward: --
 45040/100000: episode: 1776, duration: 0.005s, episode steps: 31, steps per second: 5695, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.080 [-0.469, 1.103], mean_best_reward: --
 45164/100000: episode: 1777, duration: 0.021s, episode steps: 124, steps per second: 6022, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.253 [-1.322, 0.816], mean_best_reward: --
 45183/100000: episode: 1778, duration: 0.003s, episode steps: 19, steps per second: 5571, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.075 [-1.130, 1.757], mean_best_reward: --
 45235/100000: episode: 1779, duration: 0.009s, episode steps: 52, steps per second: 5975, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-1.287, 0.771], mean_best_reward: --
 45271/100000: episode: 1780, duration: 0.006s, episode steps: 36, steps per second: 5936, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.063 [-1.320, 0.557], mean_best_reward: --
 45381/100000: episode: 1781, duration: 0.018s, episode steps: 110, steps per second: 6218, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.276 [-1.697, 1.088], mean_best_reward: --
 45438/100000: episode: 1782, duration: 0.010s, episode steps: 57, steps per second: 5456, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.084 [-1.572, 1.599], mean_best_reward: --
 45525/100000: episode: 1783, duration: 0.014s, episode steps: 87, steps per second: 6274, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.190 [-1.483, 1.024], mean_best_reward: --
 45656/100000: episode: 1784, duration: 0.021s, episode steps: 131, steps per second: 6375, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.184 [-1.370, 1.001], mean_best_reward: --
 45676/100000: episode: 1785, duration: 0.004s, episode steps: 20, steps per second: 5308, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.425, 1.017], mean_best_reward: --
 45749/100000: episode: 1786, duration: 0.012s, episode steps: 73, steps per second: 6138, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.019 [-0.594, 1.196], mean_best_reward: --
 45936/100000: episode: 1787, duration: 0.029s, episode steps: 187, steps per second: 6364, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.224 [-1.006, 1.767], mean_best_reward: --
 46078/100000: episode: 1788, duration: 0.023s, episode steps: 142, steps per second: 6203, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.198 [-1.276, 0.712], mean_best_reward: --
 46139/100000: episode: 1789, duration: 0.010s, episode steps: 61, steps per second: 6093, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.097 [-1.107, 1.358], mean_best_reward: --
 46153/100000: episode: 1790, duration: 0.003s, episode steps: 14, steps per second: 5345, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.104 [-1.582, 2.537], mean_best_reward: --
 46170/100000: episode: 1791, duration: 0.003s, episode steps: 17, steps per second: 5454, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.068 [-1.444, 0.986], mean_best_reward: --
 46321/100000: episode: 1792, duration: 0.024s, episode steps: 151, steps per second: 6209, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.327 [-1.004, 1.775], mean_best_reward: --
 46343/100000: episode: 1793, duration: 0.004s, episode steps: 22, steps per second: 5538, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.049 [-1.355, 0.837], mean_best_reward: --
 46414/100000: episode: 1794, duration: 0.011s, episode steps: 71, steps per second: 6265, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.191 [-0.768, 2.035], mean_best_reward: --
 46534/100000: episode: 1795, duration: 0.019s, episode steps: 120, steps per second: 6168, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.298 [-0.909, 1.335], mean_best_reward: --
 46629/100000: episode: 1796, duration: 0.016s, episode steps: 95, steps per second: 5948, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.160 [-1.542, 1.136], mean_best_reward: --
 46828/100000: episode: 1797, duration: 0.031s, episode steps: 199, steps per second: 6320, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.068 [-1.491, 1.158], mean_best_reward: --
 46948/100000: episode: 1798, duration: 0.019s, episode steps: 120, steps per second: 6177, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.045 [-0.835, 1.783], mean_best_reward: --
 46975/100000: episode: 1799, duration: 0.005s, episode steps: 27, steps per second: 5828, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.101 [-1.004, 0.400], mean_best_reward: --
 47032/100000: episode: 1800, duration: 0.010s, episode steps: 57, steps per second: 5962, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.076 [-1.693, 1.871], mean_best_reward: --
 47086/100000: episode: 1801, duration: 0.009s, episode steps: 54, steps per second: 5976, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.153 [-0.507, 1.557], mean_best_reward: 127.500000
 47096/100000: episode: 1802, duration: 0.002s, episode steps: 10, steps per second: 4974, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.521, 2.518], mean_best_reward: --
 47116/100000: episode: 1803, duration: 0.004s, episode steps: 20, steps per second: 5082, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.070 [-1.282, 0.810], mean_best_reward: --
 47137/100000: episode: 1804, duration: 0.004s, episode steps: 21, steps per second: 5682, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.067 [-1.041, 0.601], mean_best_reward: --
 47158/100000: episode: 1805, duration: 0.004s, episode steps: 21, steps per second: 5330, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.057 [-1.450, 0.825], mean_best_reward: --
 47169/100000: episode: 1806, duration: 0.003s, episode steps: 11, steps per second: 4381, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.137 [-1.328, 2.254], mean_best_reward: --
 47186/100000: episode: 1807, duration: 0.003s, episode steps: 17, steps per second: 5327, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.091 [-1.535, 2.425], mean_best_reward: --
 47200/100000: episode: 1808, duration: 0.003s, episode steps: 14, steps per second: 5044, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.107 [-2.129, 1.203], mean_best_reward: --
 47284/100000: episode: 1809, duration: 0.014s, episode steps: 84, steps per second: 6194, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.267 [-2.413, 1.949], mean_best_reward: --
 47339/100000: episode: 1810, duration: 0.009s, episode steps: 55, steps per second: 6127, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.145 [-0.509, 1.074], mean_best_reward: --
 47352/100000: episode: 1811, duration: 0.003s, episode steps: 13, steps per second: 5190, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.062 [-2.253, 1.587], mean_best_reward: --
 47371/100000: episode: 1812, duration: 0.003s, episode steps: 19, steps per second: 5616, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.077 [-1.544, 0.940], mean_best_reward: --
 47390/100000: episode: 1813, duration: 0.003s, episode steps: 19, steps per second: 5488, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.087 [-1.934, 0.978], mean_best_reward: --
 47414/100000: episode: 1814, duration: 0.004s, episode steps: 24, steps per second: 5757, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.162, 0.793], mean_best_reward: --
 47440/100000: episode: 1815, duration: 0.005s, episode steps: 26, steps per second: 5372, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.110 [-1.342, 0.575], mean_best_reward: --
 47458/100000: episode: 1816, duration: 0.004s, episode steps: 18, steps per second: 4896, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.043 [-1.013, 1.430], mean_best_reward: --
 47488/100000: episode: 1817, duration: 0.005s, episode steps: 30, steps per second: 5821, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.078 [-0.998, 1.656], mean_best_reward: --
 47502/100000: episode: 1818, duration: 0.003s, episode steps: 14, steps per second: 5321, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.090 [-2.464, 1.596], mean_best_reward: --
 47550/100000: episode: 1819, duration: 0.008s, episode steps: 48, steps per second: 5880, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.024 [-1.032, 1.076], mean_best_reward: --
 47571/100000: episode: 1820, duration: 0.004s, episode steps: 21, steps per second: 5579, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.057 [-0.793, 1.092], mean_best_reward: --
 47602/100000: episode: 1821, duration: 0.005s, episode steps: 31, steps per second: 5934, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.613 [0.000, 1.000], mean observation: -0.023 [-2.185, 1.349], mean_best_reward: --
 47703/100000: episode: 1822, duration: 0.016s, episode steps: 101, steps per second: 6216, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.253 [-0.565, 1.333], mean_best_reward: --
 47713/100000: episode: 1823, duration: 0.003s, episode steps: 10, steps per second: 3987, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.141 [-2.038, 1.137], mean_best_reward: --
 47774/100000: episode: 1824, duration: 0.010s, episode steps: 61, steps per second: 5955, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.068 [-1.028, 0.586], mean_best_reward: --
 47789/100000: episode: 1825, duration: 0.003s, episode steps: 15, steps per second: 5371, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.116 [-1.622, 0.934], mean_best_reward: --
 47812/100000: episode: 1826, duration: 0.004s, episode steps: 23, steps per second: 5786, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.064 [-0.991, 1.714], mean_best_reward: --
 47849/100000: episode: 1827, duration: 0.007s, episode steps: 37, steps per second: 5687, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.077 [-1.081, 0.598], mean_best_reward: --
 47916/100000: episode: 1828, duration: 0.011s, episode steps: 67, steps per second: 6226, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.048 [-0.603, 1.019], mean_best_reward: --
 47962/100000: episode: 1829, duration: 0.008s, episode steps: 46, steps per second: 5849, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.143 [-1.056, 0.632], mean_best_reward: --
 47993/100000: episode: 1830, duration: 0.006s, episode steps: 31, steps per second: 4922, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.117 [-0.755, 0.384], mean_best_reward: --
 48023/100000: episode: 1831, duration: 0.006s, episode steps: 30, steps per second: 5414, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.024 [-1.632, 0.998], mean_best_reward: --
 48043/100000: episode: 1832, duration: 0.004s, episode steps: 20, steps per second: 5612, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.745, 1.205], mean_best_reward: --
 48069/100000: episode: 1833, duration: 0.004s, episode steps: 26, steps per second: 5826, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.639, 0.989], mean_best_reward: --
 48120/100000: episode: 1834, duration: 0.009s, episode steps: 51, steps per second: 5910, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.027 [-1.201, 0.560], mean_best_reward: --
 48180/100000: episode: 1835, duration: 0.010s, episode steps: 60, steps per second: 6208, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.129 [-1.014, 0.530], mean_best_reward: --
 48216/100000: episode: 1836, duration: 0.006s, episode steps: 36, steps per second: 5745, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.019 [-1.333, 1.793], mean_best_reward: --
 48244/100000: episode: 1837, duration: 0.005s, episode steps: 28, steps per second: 5841, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.632, 1.224], mean_best_reward: --
 48301/100000: episode: 1838, duration: 0.010s, episode steps: 57, steps per second: 5893, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.004 [-0.653, 1.065], mean_best_reward: --
 48329/100000: episode: 1839, duration: 0.005s, episode steps: 28, steps per second: 5825, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.763, 1.288], mean_best_reward: --
 48369/100000: episode: 1840, duration: 0.007s, episode steps: 40, steps per second: 6063, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: 0.025 [-2.100, 1.739], mean_best_reward: --
 48398/100000: episode: 1841, duration: 0.005s, episode steps: 29, steps per second: 5626, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.020 [-1.974, 1.388], mean_best_reward: --
 48453/100000: episode: 1842, duration: 0.009s, episode steps: 55, steps per second: 6198, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.079 [-1.451, 0.839], mean_best_reward: --
 48464/100000: episode: 1843, duration: 0.002s, episode steps: 11, steps per second: 4974, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.116 [-0.976, 1.618], mean_best_reward: --
 48507/100000: episode: 1844, duration: 0.007s, episode steps: 43, steps per second: 6054, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.218 [-0.776, 1.298], mean_best_reward: --
 48525/100000: episode: 1845, duration: 0.003s, episode steps: 18, steps per second: 5542, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.061 [-1.301, 0.823], mean_best_reward: --
 48534/100000: episode: 1846, duration: 0.002s, episode steps: 9, steps per second: 4689, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.128 [-1.183, 1.928], mean_best_reward: --
 48562/100000: episode: 1847, duration: 0.005s, episode steps: 28, steps per second: 5179, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.778, 1.120], mean_best_reward: --
 48571/100000: episode: 1848, duration: 0.002s, episode steps: 9, steps per second: 4745, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.154 [-2.291, 1.333], mean_best_reward: --
 48589/100000: episode: 1849, duration: 0.003s, episode steps: 18, steps per second: 5576, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-2.012, 1.154], mean_best_reward: --
 48605/100000: episode: 1850, duration: 0.003s, episode steps: 16, steps per second: 5261, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.089 [-1.647, 0.954], mean_best_reward: --
 48624/100000: episode: 1851, duration: 0.004s, episode steps: 19, steps per second: 5138, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.052 [-1.357, 0.830], mean_best_reward: 152.000000
 48648/100000: episode: 1852, duration: 0.004s, episode steps: 24, steps per second: 5415, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.046 [-1.413, 0.976], mean_best_reward: --
 48773/100000: episode: 1853, duration: 0.020s, episode steps: 125, steps per second: 6382, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.010 [-1.401, 1.469], mean_best_reward: --
 48862/100000: episode: 1854, duration: 0.015s, episode steps: 89, steps per second: 6121, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.056 [-1.134, 0.825], mean_best_reward: --
 48932/100000: episode: 1855, duration: 0.011s, episode steps: 70, steps per second: 6272, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.266 [-1.683, 0.820], mean_best_reward: --
 49029/100000: episode: 1856, duration: 0.016s, episode steps: 97, steps per second: 6232, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.049 [-1.292, 1.192], mean_best_reward: --
 49138/100000: episode: 1857, duration: 0.018s, episode steps: 109, steps per second: 6121, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: 0.031 [-1.320, 1.609], mean_best_reward: --
 49243/100000: episode: 1858, duration: 0.017s, episode steps: 105, steps per second: 6199, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.013 [-1.573, 0.934], mean_best_reward: --
 49310/100000: episode: 1859, duration: 0.011s, episode steps: 67, steps per second: 6160, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.055 [-1.107, 1.132], mean_best_reward: --
 49462/100000: episode: 1860, duration: 0.024s, episode steps: 152, steps per second: 6278, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.040 [-0.932, 1.206], mean_best_reward: --
 49542/100000: episode: 1861, duration: 0.013s, episode steps: 80, steps per second: 6331, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.183 [-1.830, 1.100], mean_best_reward: --
 49652/100000: episode: 1862, duration: 0.018s, episode steps: 110, steps per second: 6241, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.015 [-1.521, 1.515], mean_best_reward: --
 49792/100000: episode: 1863, duration: 0.022s, episode steps: 140, steps per second: 6260, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.324 [-1.640, 0.970], mean_best_reward: --
 49816/100000: episode: 1864, duration: 0.004s, episode steps: 24, steps per second: 5779, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.078 [-0.765, 1.244], mean_best_reward: --
 49871/100000: episode: 1865, duration: 0.009s, episode steps: 55, steps per second: 6165, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.081 [-2.368, 1.061], mean_best_reward: --
 49931/100000: episode: 1866, duration: 0.010s, episode steps: 60, steps per second: 6260, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.070 [-1.187, 1.471], mean_best_reward: --
 49949/100000: episode: 1867, duration: 0.003s, episode steps: 18, steps per second: 5382, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.154, 0.761], mean_best_reward: --
 50087/100000: episode: 1868, duration: 0.022s, episode steps: 138, steps per second: 6152, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-1.296, 1.102], mean_best_reward: --
 50145/100000: episode: 1869, duration: 0.009s, episode steps: 58, steps per second: 6111, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.688, 1.150], mean_best_reward: --
 50303/100000: episode: 1870, duration: 0.025s, episode steps: 158, steps per second: 6349, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.017 [-1.295, 1.541], mean_best_reward: --
 50323/100000: episode: 1871, duration: 0.004s, episode steps: 20, steps per second: 4845, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.086 [-0.635, 1.152], mean_best_reward: --
 50358/100000: episode: 1872, duration: 0.006s, episode steps: 35, steps per second: 5968, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.109 [-0.960, 0.415], mean_best_reward: --
 50445/100000: episode: 1873, duration: 0.014s, episode steps: 87, steps per second: 6083, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.043 [-1.546, 1.870], mean_best_reward: --
 50473/100000: episode: 1874, duration: 0.005s, episode steps: 28, steps per second: 5831, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-0.918, 0.558], mean_best_reward: --
 50547/100000: episode: 1875, duration: 0.012s, episode steps: 74, steps per second: 6224, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: -0.010 [-1.657, 1.372], mean_best_reward: --
 50612/100000: episode: 1876, duration: 0.011s, episode steps: 65, steps per second: 5815, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.045 [-0.999, 1.115], mean_best_reward: --
 50664/100000: episode: 1877, duration: 0.009s, episode steps: 52, steps per second: 5844, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.004 [-1.106, 0.733], mean_best_reward: --
 50700/100000: episode: 1878, duration: 0.006s, episode steps: 36, steps per second: 5983, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-0.922, 0.372], mean_best_reward: --
 50771/100000: episode: 1879, duration: 0.012s, episode steps: 71, steps per second: 6108, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.005 [-0.945, 1.238], mean_best_reward: --
 50866/100000: episode: 1880, duration: 0.015s, episode steps: 95, steps per second: 6154, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.116 [-1.828, 1.357], mean_best_reward: --
 50950/100000: episode: 1881, duration: 0.014s, episode steps: 84, steps per second: 6003, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.159 [-1.368, 1.044], mean_best_reward: --
 51078/100000: episode: 1882, duration: 0.020s, episode steps: 128, steps per second: 6316, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.747, 1.360], mean_best_reward: --
 51217/100000: episode: 1883, duration: 0.022s, episode steps: 139, steps per second: 6199, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.390 [-2.077, 1.279], mean_best_reward: --
 51271/100000: episode: 1884, duration: 0.009s, episode steps: 54, steps per second: 5962, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.057 [-1.454, 1.142], mean_best_reward: --
 51313/100000: episode: 1885, duration: 0.007s, episode steps: 42, steps per second: 6095, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.121 [-1.144, 0.797], mean_best_reward: --
 51326/100000: episode: 1886, duration: 0.003s, episode steps: 13, steps per second: 5120, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.095 [-1.580, 2.472], mean_best_reward: --
 51355/100000: episode: 1887, duration: 0.005s, episode steps: 29, steps per second: 5880, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.088 [-0.591, 1.162], mean_best_reward: --
 51451/100000: episode: 1888, duration: 0.016s, episode steps: 96, steps per second: 6173, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.396 [0.000, 1.000], mean observation: -0.388 [-3.720, 2.431], mean_best_reward: --
 51464/100000: episode: 1889, duration: 0.002s, episode steps: 13, steps per second: 5229, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.082 [-0.790, 1.173], mean_best_reward: --
 51586/100000: episode: 1890, duration: 0.020s, episode steps: 122, steps per second: 6163, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.187 [-1.813, 1.307], mean_best_reward: --
 51677/100000: episode: 1891, duration: 0.014s, episode steps: 91, steps per second: 6302, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.006 [-1.720, 1.719], mean_best_reward: --
 51726/100000: episode: 1892, duration: 0.008s, episode steps: 49, steps per second: 6086, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.114 [-1.342, 1.211], mean_best_reward: --
 51767/100000: episode: 1893, duration: 0.007s, episode steps: 41, steps per second: 6057, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.115 [-1.194, 0.586], mean_best_reward: --
 51799/100000: episode: 1894, duration: 0.006s, episode steps: 32, steps per second: 5542, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.077 [-0.556, 1.116], mean_best_reward: --
 51868/100000: episode: 1895, duration: 0.011s, episode steps: 69, steps per second: 6053, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.012 [-0.911, 1.154], mean_best_reward: --
 51899/100000: episode: 1896, duration: 0.006s, episode steps: 31, steps per second: 5572, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.145 [-0.943, 0.589], mean_best_reward: --
 51931/100000: episode: 1897, duration: 0.005s, episode steps: 32, steps per second: 5830, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.116 [-0.978, 0.273], mean_best_reward: --
 51949/100000: episode: 1898, duration: 0.003s, episode steps: 18, steps per second: 5565, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.057 [-1.410, 2.180], mean_best_reward: --
 51964/100000: episode: 1899, duration: 0.003s, episode steps: 15, steps per second: 5386, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.080 [-0.968, 1.685], mean_best_reward: --
 52089/100000: episode: 1900, duration: 0.020s, episode steps: 125, steps per second: 6113, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.132 [-1.139, 1.549], mean_best_reward: --
 52142/100000: episode: 1901, duration: 0.009s, episode steps: 53, steps per second: 5866, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.042 [-1.667, 0.758], mean_best_reward: 169.500000
 52233/100000: episode: 1902, duration: 0.014s, episode steps: 91, steps per second: 6324, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.011 [-1.753, 2.369], mean_best_reward: --
 52252/100000: episode: 1903, duration: 0.003s, episode steps: 19, steps per second: 5606, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.083 [-0.602, 1.108], mean_best_reward: --
 52347/100000: episode: 1904, duration: 0.015s, episode steps: 95, steps per second: 6206, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.035 [-1.154, 1.137], mean_best_reward: --
 52404/100000: episode: 1905, duration: 0.010s, episode steps: 57, steps per second: 5691, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.085 [-1.578, 1.722], mean_best_reward: --
 52484/100000: episode: 1906, duration: 0.013s, episode steps: 80, steps per second: 6128, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.206 [-1.846, 1.198], mean_best_reward: --
 52568/100000: episode: 1907, duration: 0.013s, episode steps: 84, steps per second: 6296, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.222 [-1.692, 1.159], mean_best_reward: --
 52697/100000: episode: 1908, duration: 0.021s, episode steps: 129, steps per second: 6125, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.197 [-1.072, 0.625], mean_best_reward: --
 52778/100000: episode: 1909, duration: 0.013s, episode steps: 81, steps per second: 6220, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.205 [-1.488, 1.175], mean_best_reward: --
 52796/100000: episode: 1910, duration: 0.003s, episode steps: 18, steps per second: 5368, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.103 [-0.549, 1.128], mean_best_reward: --
 52841/100000: episode: 1911, duration: 0.007s, episode steps: 45, steps per second: 6091, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.212 [-1.213, 0.753], mean_best_reward: --
 52964/100000: episode: 1912, duration: 0.020s, episode steps: 123, steps per second: 6223, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.430 [-2.733, 1.094], mean_best_reward: --
 53009/100000: episode: 1913, duration: 0.007s, episode steps: 45, steps per second: 6015, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.077 [-0.948, 0.387], mean_best_reward: --
 53060/100000: episode: 1914, duration: 0.008s, episode steps: 51, steps per second: 6185, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.045 [-1.190, 0.892], mean_best_reward: --
 53201/100000: episode: 1915, duration: 0.023s, episode steps: 141, steps per second: 6248, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.512 [-2.402, 0.906], mean_best_reward: --
 53265/100000: episode: 1916, duration: 0.011s, episode steps: 64, steps per second: 6009, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.144 [-1.489, 1.251], mean_best_reward: --
 53307/100000: episode: 1917, duration: 0.007s, episode steps: 42, steps per second: 5888, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.742, 1.078], mean_best_reward: --
 53345/100000: episode: 1918, duration: 0.006s, episode steps: 38, steps per second: 6015, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-0.962, 0.614], mean_best_reward: --
 53383/100000: episode: 1919, duration: 0.006s, episode steps: 38, steps per second: 5972, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.056 [-0.885, 0.561], mean_best_reward: --
 53438/100000: episode: 1920, duration: 0.009s, episode steps: 55, steps per second: 6169, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.119 [-1.490, 0.677], mean_best_reward: --
 53532/100000: episode: 1921, duration: 0.015s, episode steps: 94, steps per second: 6143, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.261 [-1.874, 0.642], mean_best_reward: --
 53640/100000: episode: 1922, duration: 0.018s, episode steps: 108, steps per second: 6096, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.475 [-2.617, 0.866], mean_best_reward: --
 53727/100000: episode: 1923, duration: 0.014s, episode steps: 87, steps per second: 6099, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.281 [-0.638, 1.303], mean_best_reward: --
 53859/100000: episode: 1924, duration: 0.021s, episode steps: 132, steps per second: 6271, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.311 [-1.898, 1.222], mean_best_reward: --
 53938/100000: episode: 1925, duration: 0.013s, episode steps: 79, steps per second: 6140, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.026 [-1.576, 0.930], mean_best_reward: --
 54050/100000: episode: 1926, duration: 0.018s, episode steps: 112, steps per second: 6234, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.350 [-1.452, 0.779], mean_best_reward: --
 54197/100000: episode: 1927, duration: 0.024s, episode steps: 147, steps per second: 6228, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.002 [-1.130, 1.148], mean_best_reward: --
 54280/100000: episode: 1928, duration: 0.014s, episode steps: 83, steps per second: 6140, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.220 [-1.047, 1.301], mean_best_reward: --
 54293/100000: episode: 1929, duration: 0.002s, episode steps: 13, steps per second: 5272, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.107 [-1.364, 0.814], mean_best_reward: --
 54395/100000: episode: 1930, duration: 0.016s, episode steps: 102, steps per second: 6183, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.034 [-1.195, 1.799], mean_best_reward: --
 54457/100000: episode: 1931, duration: 0.010s, episode steps: 62, steps per second: 6022, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.237 [-1.650, 0.563], mean_best_reward: --
 54573/100000: episode: 1932, duration: 0.019s, episode steps: 116, steps per second: 6189, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.154 [-1.292, 1.164], mean_best_reward: --
 54693/100000: episode: 1933, duration: 0.019s, episode steps: 120, steps per second: 6275, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.220 [-1.652, 1.092], mean_best_reward: --
 54849/100000: episode: 1934, duration: 0.025s, episode steps: 156, steps per second: 6196, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.294 [-1.693, 0.857], mean_best_reward: --
 54873/100000: episode: 1935, duration: 0.005s, episode steps: 24, steps per second: 5086, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-1.402, 0.995], mean_best_reward: --
 55031/100000: episode: 1936, duration: 0.025s, episode steps: 158, steps per second: 6227, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.465 [-2.420, 1.210], mean_best_reward: --
 55073/100000: episode: 1937, duration: 0.008s, episode steps: 42, steps per second: 5559, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.046 [-1.474, 0.742], mean_best_reward: --
 55149/100000: episode: 1938, duration: 0.012s, episode steps: 76, steps per second: 6290, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.275 [-2.213, 1.329], mean_best_reward: --
 55175/100000: episode: 1939, duration: 0.004s, episode steps: 26, steps per second: 5782, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.131 [-1.027, 0.610], mean_best_reward: --
 55283/100000: episode: 1940, duration: 0.017s, episode steps: 108, steps per second: 6267, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.226 [-1.883, 0.536], mean_best_reward: --
 55317/100000: episode: 1941, duration: 0.006s, episode steps: 34, steps per second: 5907, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.067 [-0.938, 0.584], mean_best_reward: --
 55380/100000: episode: 1942, duration: 0.010s, episode steps: 63, steps per second: 6053, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.059 [-1.175, 0.961], mean_best_reward: --
 55408/100000: episode: 1943, duration: 0.005s, episode steps: 28, steps per second: 5872, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.082 [-1.395, 0.598], mean_best_reward: --
 55491/100000: episode: 1944, duration: 0.013s, episode steps: 83, steps per second: 6284, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.108 [-1.153, 1.191], mean_best_reward: --
 55606/100000: episode: 1945, duration: 0.018s, episode steps: 115, steps per second: 6246, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.337 [-0.924, 1.467], mean_best_reward: --
 55729/100000: episode: 1946, duration: 0.020s, episode steps: 123, steps per second: 6213, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.271 [-1.678, 1.080], mean_best_reward: --
 55872/100000: episode: 1947, duration: 0.023s, episode steps: 143, steps per second: 6305, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.353 [-1.922, 1.185], mean_best_reward: --
 55928/100000: episode: 1948, duration: 0.010s, episode steps: 56, steps per second: 5647, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.161 [-1.168, 0.664], mean_best_reward: --
 56044/100000: episode: 1949, duration: 0.019s, episode steps: 116, steps per second: 6268, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.094 [-1.868, 2.240], mean_best_reward: --
 56102/100000: episode: 1950, duration: 0.009s, episode steps: 58, steps per second: 6173, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.099 [-1.692, 1.552], mean_best_reward: --
 56118/100000: episode: 1951, duration: 0.003s, episode steps: 16, steps per second: 4945, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-0.994, 0.588], mean_best_reward: 125.000000
 56189/100000: episode: 1952, duration: 0.012s, episode steps: 71, steps per second: 6099, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.146 [-1.890, 1.449], mean_best_reward: --
 56233/100000: episode: 1953, duration: 0.008s, episode steps: 44, steps per second: 5862, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.081 [-0.442, 1.008], mean_best_reward: --
 56332/100000: episode: 1954, duration: 0.016s, episode steps: 99, steps per second: 6277, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.004 [-0.574, 1.005], mean_best_reward: --
 56357/100000: episode: 1955, duration: 0.005s, episode steps: 25, steps per second: 5483, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.106 [-0.986, 0.597], mean_best_reward: --
 56413/100000: episode: 1956, duration: 0.009s, episode steps: 56, steps per second: 6194, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.411 [0.000, 1.000], mean observation: -0.001 [-1.889, 2.508], mean_best_reward: --
 56459/100000: episode: 1957, duration: 0.008s, episode steps: 46, steps per second: 6045, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.094 [-1.569, 1.673], mean_best_reward: --
 56480/100000: episode: 1958, duration: 0.004s, episode steps: 21, steps per second: 5584, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.062 [-0.777, 1.232], mean_best_reward: --
 56609/100000: episode: 1959, duration: 0.021s, episode steps: 129, steps per second: 6287, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.251 [-2.005, 1.411], mean_best_reward: --
 56751/100000: episode: 1960, duration: 0.024s, episode steps: 142, steps per second: 5947, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.279 [-1.292, 0.907], mean_best_reward: --
 56765/100000: episode: 1961, duration: 0.004s, episode steps: 14, steps per second: 3552, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.114 [-1.115, 0.624], mean_best_reward: --
 56933/100000: episode: 1962, duration: 0.027s, episode steps: 168, steps per second: 6289, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.159 [-0.870, 1.491], mean_best_reward: --
 57133/100000: episode: 1963, duration: 0.032s, episode steps: 200, steps per second: 6207, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.306 [-2.181, 1.245], mean_best_reward: --
 57247/100000: episode: 1964, duration: 0.018s, episode steps: 114, steps per second: 6295, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.157 [-0.852, 1.721], mean_best_reward: --
 57350/100000: episode: 1965, duration: 0.016s, episode steps: 103, steps per second: 6350, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.148 [-1.673, 1.522], mean_best_reward: --
 57492/100000: episode: 1966, duration: 0.023s, episode steps: 142, steps per second: 6276, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.007 [-1.149, 1.029], mean_best_reward: --
 57593/100000: episode: 1967, duration: 0.016s, episode steps: 101, steps per second: 6316, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.173 [-1.462, 1.060], mean_best_reward: --
 57619/100000: episode: 1968, duration: 0.004s, episode steps: 26, steps per second: 5801, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.073 [-1.181, 0.442], mean_best_reward: --
 57752/100000: episode: 1969, duration: 0.022s, episode steps: 133, steps per second: 6116, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.108 [-1.273, 1.844], mean_best_reward: --
 57807/100000: episode: 1970, duration: 0.009s, episode steps: 55, steps per second: 6202, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.219 [-0.623, 1.552], mean_best_reward: --
 57832/100000: episode: 1971, duration: 0.005s, episode steps: 25, steps per second: 5376, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.081 [-1.267, 0.632], mean_best_reward: --
 57843/100000: episode: 1972, duration: 0.002s, episode steps: 11, steps per second: 5053, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.103 [-1.017, 1.553], mean_best_reward: --
 57976/100000: episode: 1973, duration: 0.022s, episode steps: 133, steps per second: 6182, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.122 [-1.483, 1.203], mean_best_reward: --
 57993/100000: episode: 1974, duration: 0.003s, episode steps: 17, steps per second: 5091, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.103 [-1.061, 0.608], mean_best_reward: --
 58114/100000: episode: 1975, duration: 0.019s, episode steps: 121, steps per second: 6210, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.234 [-1.498, 1.242], mean_best_reward: --
 58183/100000: episode: 1976, duration: 0.011s, episode steps: 69, steps per second: 6135, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.180 [-1.666, 1.358], mean_best_reward: --
 58198/100000: episode: 1977, duration: 0.003s, episode steps: 15, steps per second: 5431, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.095 [-1.027, 1.845], mean_best_reward: --
 58239/100000: episode: 1978, duration: 0.007s, episode steps: 41, steps per second: 5715, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.126 [-0.931, 0.377], mean_best_reward: --
 58273/100000: episode: 1979, duration: 0.006s, episode steps: 34, steps per second: 5973, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.083 [-0.416, 0.712], mean_best_reward: --
 58294/100000: episode: 1980, duration: 0.004s, episode steps: 21, steps per second: 4907, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.064 [-0.970, 1.382], mean_best_reward: --
 58318/100000: episode: 1981, duration: 0.005s, episode steps: 24, steps per second: 5320, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.084 [-0.964, 1.919], mean_best_reward: --
 58423/100000: episode: 1982, duration: 0.017s, episode steps: 105, steps per second: 6261, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.012 [-1.713, 2.105], mean_best_reward: --
 58446/100000: episode: 1983, duration: 0.004s, episode steps: 23, steps per second: 5753, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.088 [-0.627, 1.276], mean_best_reward: --
 58468/100000: episode: 1984, duration: 0.004s, episode steps: 22, steps per second: 5752, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.048 [-1.070, 0.636], mean_best_reward: --
 58525/100000: episode: 1985, duration: 0.009s, episode steps: 57, steps per second: 6055, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: 0.036 [-2.343, 1.779], mean_best_reward: --
 58646/100000: episode: 1986, duration: 0.019s, episode steps: 121, steps per second: 6271, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.175 [-2.076, 1.553], mean_best_reward: --
 58770/100000: episode: 1987, duration: 0.019s, episode steps: 124, steps per second: 6394, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.185 [-2.083, 1.257], mean_best_reward: --
 58799/100000: episode: 1988, duration: 0.005s, episode steps: 29, steps per second: 5554, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.106 [-1.286, 0.615], mean_best_reward: --
 58843/100000: episode: 1989, duration: 0.007s, episode steps: 44, steps per second: 6040, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.100 [-0.950, 1.444], mean_best_reward: --
 59015/100000: episode: 1990, duration: 0.028s, episode steps: 172, steps per second: 6181, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.321 [-1.910, 0.953], mean_best_reward: --
 59044/100000: episode: 1991, duration: 0.005s, episode steps: 29, steps per second: 5833, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.056 [-0.443, 1.133], mean_best_reward: --
 59067/100000: episode: 1992, duration: 0.004s, episode steps: 23, steps per second: 5749, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.041 [-2.241, 1.370], mean_best_reward: --
 59133/100000: episode: 1993, duration: 0.011s, episode steps: 66, steps per second: 6124, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.132 [-1.180, 1.023], mean_best_reward: --
 59163/100000: episode: 1994, duration: 0.005s, episode steps: 30, steps per second: 5767, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.378, 1.085], mean_best_reward: --
 59221/100000: episode: 1995, duration: 0.010s, episode steps: 58, steps per second: 5888, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.133 [-1.085, 0.644], mean_best_reward: --
 59384/100000: episode: 1996, duration: 0.025s, episode steps: 163, steps per second: 6425, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.113 [-1.172, 0.996], mean_best_reward: --
 59401/100000: episode: 1997, duration: 0.003s, episode steps: 17, steps per second: 5168, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.082 [-0.742, 1.339], mean_best_reward: --
 59418/100000: episode: 1998, duration: 0.003s, episode steps: 17, steps per second: 5509, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.100 [-1.337, 0.601], mean_best_reward: --
 59464/100000: episode: 1999, duration: 0.008s, episode steps: 46, steps per second: 5649, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.023 [-1.378, 1.781], mean_best_reward: --
 59569/100000: episode: 2000, duration: 0.017s, episode steps: 105, steps per second: 6214, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.218 [-1.351, 0.898], mean_best_reward: --
 59628/100000: episode: 2001, duration: 0.010s, episode steps: 59, steps per second: 5830, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.131 [-1.341, 1.542], mean_best_reward: 171.000000
 59645/100000: episode: 2002, duration: 0.003s, episode steps: 17, steps per second: 5512, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.069 [-1.455, 0.814], mean_best_reward: --
 59669/100000: episode: 2003, duration: 0.004s, episode steps: 24, steps per second: 5383, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.049 [-1.156, 0.793], mean_best_reward: --
 59692/100000: episode: 2004, duration: 0.004s, episode steps: 23, steps per second: 5748, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.092 [-1.473, 0.595], mean_best_reward: --
 59717/100000: episode: 2005, duration: 0.004s, episode steps: 25, steps per second: 5764, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.057 [-1.293, 0.625], mean_best_reward: --
 59751/100000: episode: 2006, duration: 0.006s, episode steps: 34, steps per second: 5661, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: -0.026 [-1.963, 2.745], mean_best_reward: --
 59823/100000: episode: 2007, duration: 0.012s, episode steps: 72, steps per second: 6247, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.017 [-0.843, 0.911], mean_best_reward: --
 59854/100000: episode: 2008, duration: 0.006s, episode steps: 31, steps per second: 5571, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.020 [-1.677, 0.973], mean_best_reward: --
 60011/100000: episode: 2009, duration: 0.025s, episode steps: 157, steps per second: 6334, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: 0.117 [-1.316, 1.144], mean_best_reward: --
 60063/100000: episode: 2010, duration: 0.009s, episode steps: 52, steps per second: 5705, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.070 [-1.850, 1.778], mean_best_reward: --
 60076/100000: episode: 2011, duration: 0.002s, episode steps: 13, steps per second: 5270, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.100 [-1.712, 1.003], mean_best_reward: --
 60204/100000: episode: 2012, duration: 0.020s, episode steps: 128, steps per second: 6303, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.060 [-1.174, 1.134], mean_best_reward: --
 60262/100000: episode: 2013, duration: 0.010s, episode steps: 58, steps per second: 5954, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.009 [-1.209, 0.802], mean_best_reward: --
 60280/100000: episode: 2014, duration: 0.003s, episode steps: 18, steps per second: 5556, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.067 [-1.461, 0.948], mean_best_reward: --
 60314/100000: episode: 2015, duration: 0.006s, episode steps: 34, steps per second: 5952, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.630, 1.360], mean_best_reward: --
 60378/100000: episode: 2016, duration: 0.011s, episode steps: 64, steps per second: 5945, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: -0.127 [-2.632, 2.597], mean_best_reward: --
 60421/100000: episode: 2017, duration: 0.007s, episode steps: 43, steps per second: 6094, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.372 [0.000, 1.000], mean observation: -0.091 [-2.519, 2.595], mean_best_reward: --
 60529/100000: episode: 2018, duration: 0.017s, episode steps: 108, steps per second: 6349, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.135 [-3.071, 2.992], mean_best_reward: --
 60573/100000: episode: 2019, duration: 0.008s, episode steps: 44, steps per second: 5835, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.202, 0.574], mean_best_reward: --
 60640/100000: episode: 2020, duration: 0.011s, episode steps: 67, steps per second: 5922, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.193 [-1.702, 1.168], mean_best_reward: --
 60715/100000: episode: 2021, duration: 0.012s, episode steps: 75, steps per second: 6304, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.064 [-0.991, 1.571], mean_best_reward: --
 60751/100000: episode: 2022, duration: 0.006s, episode steps: 36, steps per second: 5943, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.100 [-1.278, 0.462], mean_best_reward: --
 60772/100000: episode: 2023, duration: 0.004s, episode steps: 21, steps per second: 5714, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.051 [-1.766, 1.184], mean_best_reward: --
 60856/100000: episode: 2024, duration: 0.013s, episode steps: 84, steps per second: 6285, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.122 [-2.691, 2.981], mean_best_reward: --
 60879/100000: episode: 2025, duration: 0.004s, episode steps: 23, steps per second: 5724, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.084 [-0.853, 0.451], mean_best_reward: --
 60944/100000: episode: 2026, duration: 0.011s, episode steps: 65, steps per second: 5984, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: -0.089 [-2.843, 3.225], mean_best_reward: --
 60968/100000: episode: 2027, duration: 0.004s, episode steps: 24, steps per second: 5798, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.086 [-0.984, 0.376], mean_best_reward: --
 61080/100000: episode: 2028, duration: 0.018s, episode steps: 112, steps per second: 6330, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.420 [0.000, 1.000], mean observation: -0.232 [-3.382, 2.948], mean_best_reward: --
 61159/100000: episode: 2029, duration: 0.013s, episode steps: 79, steps per second: 6080, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.048 [-1.443, 1.333], mean_best_reward: --
 61242/100000: episode: 2030, duration: 0.014s, episode steps: 83, steps per second: 6077, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.398 [0.000, 1.000], mean observation: -0.175 [-3.233, 2.804], mean_best_reward: --
 61278/100000: episode: 2031, duration: 0.006s, episode steps: 36, steps per second: 6003, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.009 [-0.963, 1.362], mean_best_reward: --
 61343/100000: episode: 2032, duration: 0.010s, episode steps: 65, steps per second: 6244, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.072 [-2.504, 2.846], mean_best_reward: --
 61518/100000: episode: 2033, duration: 0.028s, episode steps: 175, steps per second: 6294, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.204 [-2.081, 1.369], mean_best_reward: --
 61677/100000: episode: 2034, duration: 0.025s, episode steps: 159, steps per second: 6374, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.092 [-1.343, 1.105], mean_best_reward: --
 61705/100000: episode: 2035, duration: 0.005s, episode steps: 28, steps per second: 5876, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.028, 0.395], mean_best_reward: --
 61734/100000: episode: 2036, duration: 0.005s, episode steps: 29, steps per second: 5841, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.049 [-1.417, 0.798], mean_best_reward: --
 61904/100000: episode: 2037, duration: 0.027s, episode steps: 170, steps per second: 6376, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.015 [-0.944, 1.178], mean_best_reward: --
 62003/100000: episode: 2038, duration: 0.016s, episode steps: 99, steps per second: 6355, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.339 [-1.508, 0.692], mean_best_reward: --
 62032/100000: episode: 2039, duration: 0.005s, episode steps: 29, steps per second: 5547, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.107 [-0.978, 0.631], mean_best_reward: --
 62073/100000: episode: 2040, duration: 0.007s, episode steps: 41, steps per second: 6033, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.082 [-1.056, 0.632], mean_best_reward: --
 62098/100000: episode: 2041, duration: 0.004s, episode steps: 25, steps per second: 5806, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.320 [0.000, 1.000], mean observation: -0.005 [-1.763, 2.425], mean_best_reward: --
 62153/100000: episode: 2042, duration: 0.009s, episode steps: 55, steps per second: 5879, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.050 [-0.832, 0.580], mean_best_reward: --
 62301/100000: episode: 2043, duration: 0.023s, episode steps: 148, steps per second: 6353, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.207 [-1.153, 0.884], mean_best_reward: --
 62399/100000: episode: 2044, duration: 0.016s, episode steps: 98, steps per second: 6311, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.159 [-1.103, 0.731], mean_best_reward: --
 62454/100000: episode: 2045, duration: 0.009s, episode steps: 55, steps per second: 5925, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.345 [0.000, 1.000], mean observation: -0.178 [-3.456, 3.332], mean_best_reward: --
 62511/100000: episode: 2046, duration: 0.009s, episode steps: 57, steps per second: 6196, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.404 [0.000, 1.000], mean observation: -0.221 [-2.107, 1.773], mean_best_reward: --
 62561/100000: episode: 2047, duration: 0.008s, episode steps: 50, steps per second: 5926, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.052 [-0.939, 0.407], mean_best_reward: --
 62597/100000: episode: 2048, duration: 0.006s, episode steps: 36, steps per second: 5992, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.165, 0.564], mean_best_reward: --
 62639/100000: episode: 2049, duration: 0.007s, episode steps: 42, steps per second: 5821, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.037 [-1.127, 0.632], mean_best_reward: --
 62652/100000: episode: 2050, duration: 0.003s, episode steps: 13, steps per second: 5196, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.081 [-1.526, 1.010], mean_best_reward: --
 62710/100000: episode: 2051, duration: 0.010s, episode steps: 58, steps per second: 5781, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.007 [-0.834, 0.719], mean_best_reward: 137.500000
 62732/100000: episode: 2052, duration: 0.004s, episode steps: 22, steps per second: 5614, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.030, 0.578], mean_best_reward: --
 62763/100000: episode: 2053, duration: 0.005s, episode steps: 31, steps per second: 5756, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.051 [-1.079, 0.607], mean_best_reward: --
 62813/100000: episode: 2054, duration: 0.008s, episode steps: 50, steps per second: 5994, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.080 [-1.353, 0.707], mean_best_reward: --
 62835/100000: episode: 2055, duration: 0.004s, episode steps: 22, steps per second: 5739, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.088 [-1.209, 0.745], mean_best_reward: --
 62872/100000: episode: 2056, duration: 0.006s, episode steps: 37, steps per second: 6064, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.045 [-1.380, 0.631], mean_best_reward: --
 62962/100000: episode: 2057, duration: 0.014s, episode steps: 90, steps per second: 6321, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.033 [-1.897, 1.430], mean_best_reward: --
 62984/100000: episode: 2058, duration: 0.005s, episode steps: 22, steps per second: 4428, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.036 [-1.453, 0.985], mean_best_reward: --
 63011/100000: episode: 2059, duration: 0.005s, episode steps: 27, steps per second: 5706, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.072 [-2.042, 1.038], mean_best_reward: --
 63059/100000: episode: 2060, duration: 0.008s, episode steps: 48, steps per second: 6142, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.054 [-0.971, 0.568], mean_best_reward: --
 63092/100000: episode: 2061, duration: 0.006s, episode steps: 33, steps per second: 5937, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.111 [-1.190, 0.588], mean_best_reward: --
 63234/100000: episode: 2062, duration: 0.022s, episode steps: 142, steps per second: 6317, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.439 [-2.428, 0.916], mean_best_reward: --
 63269/100000: episode: 2063, duration: 0.006s, episode steps: 35, steps per second: 5893, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.124 [-0.612, 1.131], mean_best_reward: --
 63338/100000: episode: 2064, duration: 0.012s, episode steps: 69, steps per second: 5901, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.030 [-0.720, 1.066], mean_best_reward: --
 63466/100000: episode: 2065, duration: 0.020s, episode steps: 128, steps per second: 6289, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.445 [-2.249, 1.049], mean_best_reward: --
 63587/100000: episode: 2066, duration: 0.020s, episode steps: 121, steps per second: 6048, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.440 [-2.046, 1.063], mean_best_reward: --
 63633/100000: episode: 2067, duration: 0.008s, episode steps: 46, steps per second: 5964, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.026 [-2.066, 1.500], mean_best_reward: --
 63684/100000: episode: 2068, duration: 0.008s, episode steps: 51, steps per second: 6102, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.100 [-0.766, 1.244], mean_best_reward: --
 63801/100000: episode: 2069, duration: 0.019s, episode steps: 117, steps per second: 6279, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.372 [-1.719, 1.026], mean_best_reward: --
 63820/100000: episode: 2070, duration: 0.004s, episode steps: 19, steps per second: 5120, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.082 [-1.071, 0.599], mean_best_reward: --
 63920/100000: episode: 2071, duration: 0.016s, episode steps: 100, steps per second: 6209, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-1.072, 0.858], mean_best_reward: --
 63965/100000: episode: 2072, duration: 0.007s, episode steps: 45, steps per second: 6087, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.033 [-1.464, 1.136], mean_best_reward: --
 64021/100000: episode: 2073, duration: 0.009s, episode steps: 56, steps per second: 6150, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.160 [-1.500, 1.740], mean_best_reward: --
 64198/100000: episode: 2074, duration: 0.028s, episode steps: 177, steps per second: 6290, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.208 [-1.133, 0.734], mean_best_reward: --
 64225/100000: episode: 2075, duration: 0.005s, episode steps: 27, steps per second: 5807, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.087 [-1.338, 0.786], mean_best_reward: --
 64292/100000: episode: 2076, duration: 0.011s, episode steps: 67, steps per second: 6201, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.418 [0.000, 1.000], mean observation: -0.228 [-2.312, 2.064], mean_best_reward: --
 64311/100000: episode: 2077, duration: 0.003s, episode steps: 19, steps per second: 5454, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.062 [-2.067, 1.343], mean_best_reward: --
 64369/100000: episode: 2078, duration: 0.010s, episode steps: 58, steps per second: 5791, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.102 [-0.980, 0.644], mean_best_reward: --
 64410/100000: episode: 2079, duration: 0.007s, episode steps: 41, steps per second: 6024, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.022 [-1.304, 0.959], mean_best_reward: --
 64471/100000: episode: 2080, duration: 0.010s, episode steps: 61, steps per second: 5824, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.251 [-2.293, 1.728], mean_best_reward: --
 64535/100000: episode: 2081, duration: 0.010s, episode steps: 64, steps per second: 6223, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.125 [-1.029, 0.543], mean_best_reward: --
 64556/100000: episode: 2082, duration: 0.004s, episode steps: 21, steps per second: 5333, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.059 [-1.449, 0.757], mean_best_reward: --
 64669/100000: episode: 2083, duration: 0.018s, episode steps: 113, steps per second: 6312, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.480 [-2.444, 1.130], mean_best_reward: --
 64738/100000: episode: 2084, duration: 0.011s, episode steps: 69, steps per second: 6176, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.016 [-0.995, 0.798], mean_best_reward: --
 64863/100000: episode: 2085, duration: 0.021s, episode steps: 125, steps per second: 6079, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.467 [-2.400, 0.722], mean_best_reward: --
 64937/100000: episode: 2086, duration: 0.012s, episode steps: 74, steps per second: 6212, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-0.929, 1.093], mean_best_reward: --
 65041/100000: episode: 2087, duration: 0.017s, episode steps: 104, steps per second: 6094, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.068 [-1.467, 1.062], mean_best_reward: --
 65134/100000: episode: 2088, duration: 0.015s, episode steps: 93, steps per second: 6298, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.015 [-1.363, 1.438], mean_best_reward: --
 65145/100000: episode: 2089, duration: 0.002s, episode steps: 11, steps per second: 4997, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.101 [-1.573, 1.026], mean_best_reward: --
 65173/100000: episode: 2090, duration: 0.005s, episode steps: 28, steps per second: 5728, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.097, 0.576], mean_best_reward: --
 65211/100000: episode: 2091, duration: 0.006s, episode steps: 38, steps per second: 5975, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.119 [-0.948, 0.634], mean_best_reward: --
 65267/100000: episode: 2092, duration: 0.009s, episode steps: 56, steps per second: 6138, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.071 [-0.455, 0.813], mean_best_reward: --
 65364/100000: episode: 2093, duration: 0.016s, episode steps: 97, steps per second: 6054, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.052 [-1.732, 1.698], mean_best_reward: --
 65529/100000: episode: 2094, duration: 0.026s, episode steps: 165, steps per second: 6410, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.300 [-1.812, 1.609], mean_best_reward: --
 65553/100000: episode: 2095, duration: 0.004s, episode steps: 24, steps per second: 5659, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-1.093, 0.592], mean_best_reward: --
 65638/100000: episode: 2096, duration: 0.014s, episode steps: 85, steps per second: 6134, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.169 [-0.695, 0.957], mean_best_reward: --
 65654/100000: episode: 2097, duration: 0.003s, episode steps: 16, steps per second: 5207, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.081 [-1.017, 1.703], mean_best_reward: --
 65676/100000: episode: 2098, duration: 0.004s, episode steps: 22, steps per second: 5695, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.015 [-1.950, 1.382], mean_best_reward: --
 65752/100000: episode: 2099, duration: 0.012s, episode steps: 76, steps per second: 6127, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.145 [-1.927, 1.895], mean_best_reward: --
 65782/100000: episode: 2100, duration: 0.005s, episode steps: 30, steps per second: 5840, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.772, 1.217], mean_best_reward: --
 65926/100000: episode: 2101, duration: 0.023s, episode steps: 144, steps per second: 6226, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.313 [-1.636, 1.004], mean_best_reward: 147.000000
 66079/100000: episode: 2102, duration: 0.025s, episode steps: 153, steps per second: 6185, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.235 [-1.291, 0.875], mean_best_reward: --
 66162/100000: episode: 2103, duration: 0.013s, episode steps: 83, steps per second: 6150, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.057 [-1.097, 1.011], mean_best_reward: --
 66196/100000: episode: 2104, duration: 0.006s, episode steps: 34, steps per second: 5864, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.738, 1.331], mean_best_reward: --
 66310/100000: episode: 2105, duration: 0.019s, episode steps: 114, steps per second: 6031, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.505 [-2.423, 1.176], mean_best_reward: --
 66380/100000: episode: 2106, duration: 0.011s, episode steps: 70, steps per second: 6189, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.021 [-0.970, 1.176], mean_best_reward: --
 66406/100000: episode: 2107, duration: 0.004s, episode steps: 26, steps per second: 5799, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.642, 1.140], mean_best_reward: --
 66447/100000: episode: 2108, duration: 0.007s, episode steps: 41, steps per second: 5752, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.035 [-1.596, 0.843], mean_best_reward: --
 66613/100000: episode: 2109, duration: 0.027s, episode steps: 166, steps per second: 6253, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.255 [-1.586, 0.749], mean_best_reward: --
 66693/100000: episode: 2110, duration: 0.013s, episode steps: 80, steps per second: 6129, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.003 [-1.449, 1.308], mean_best_reward: --
 66725/100000: episode: 2111, duration: 0.006s, episode steps: 32, steps per second: 5564, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.060, 0.428], mean_best_reward: --
 66752/100000: episode: 2112, duration: 0.005s, episode steps: 27, steps per second: 5807, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.042 [-0.844, 1.474], mean_best_reward: --
 66770/100000: episode: 2113, duration: 0.003s, episode steps: 18, steps per second: 5544, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.073 [-1.586, 0.987], mean_best_reward: --
 66788/100000: episode: 2114, duration: 0.003s, episode steps: 18, steps per second: 5352, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.070 [-1.747, 1.009], mean_best_reward: --
 66845/100000: episode: 2115, duration: 0.010s, episode steps: 57, steps per second: 5691, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.181 [-1.262, 1.352], mean_best_reward: --
 66953/100000: episode: 2116, duration: 0.017s, episode steps: 108, steps per second: 6201, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.022 [-2.134, 2.011], mean_best_reward: --
 66985/100000: episode: 2117, duration: 0.005s, episode steps: 32, steps per second: 5949, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.093 [-0.975, 0.419], mean_best_reward: --
 67060/100000: episode: 2118, duration: 0.012s, episode steps: 75, steps per second: 6105, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.012 [-1.766, 2.281], mean_best_reward: --
 67200/100000: episode: 2119, duration: 0.023s, episode steps: 140, steps per second: 6135, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.403 [-2.436, 1.135], mean_best_reward: --
 67286/100000: episode: 2120, duration: 0.014s, episode steps: 86, steps per second: 6231, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.077 [-0.629, 1.303], mean_best_reward: --
 67307/100000: episode: 2121, duration: 0.004s, episode steps: 21, steps per second: 5298, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.059 [-1.163, 0.634], mean_best_reward: --
 67382/100000: episode: 2122, duration: 0.013s, episode steps: 75, steps per second: 5869, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.298 [-2.101, 1.319], mean_best_reward: --
 67454/100000: episode: 2123, duration: 0.012s, episode steps: 72, steps per second: 6198, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.124 [-2.084, 1.796], mean_best_reward: --
 67499/100000: episode: 2124, duration: 0.007s, episode steps: 45, steps per second: 6052, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.578 [0.000, 1.000], mean observation: -0.020 [-2.263, 1.360], mean_best_reward: --
 67643/100000: episode: 2125, duration: 0.023s, episode steps: 144, steps per second: 6388, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.142 [-1.505, 1.705], mean_best_reward: --
 67823/100000: episode: 2126, duration: 0.029s, episode steps: 180, steps per second: 6185, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-1.207, 0.746], mean_best_reward: --
 67859/100000: episode: 2127, duration: 0.006s, episode steps: 36, steps per second: 5991, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.121 [-0.841, 1.241], mean_best_reward: --
 68001/100000: episode: 2128, duration: 0.023s, episode steps: 142, steps per second: 6257, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.247 [-1.319, 1.002], mean_best_reward: --
 68040/100000: episode: 2129, duration: 0.007s, episode steps: 39, steps per second: 5981, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.118 [-1.042, 0.463], mean_best_reward: --
 68065/100000: episode: 2130, duration: 0.004s, episode steps: 25, steps per second: 5799, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.045 [-1.615, 0.991], mean_best_reward: --
 68109/100000: episode: 2131, duration: 0.007s, episode steps: 44, steps per second: 6059, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.194 [-0.759, 1.372], mean_best_reward: --
 68199/100000: episode: 2132, duration: 0.015s, episode steps: 90, steps per second: 6178, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.014 [-1.044, 0.910], mean_best_reward: --
 68321/100000: episode: 2133, duration: 0.020s, episode steps: 122, steps per second: 5973, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.259 [-1.777, 0.938], mean_best_reward: --
 68480/100000: episode: 2134, duration: 0.025s, episode steps: 159, steps per second: 6371, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.337 [-3.015, 1.749], mean_best_reward: --
 68502/100000: episode: 2135, duration: 0.004s, episode steps: 22, steps per second: 5189, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.091 [-1.161, 0.599], mean_best_reward: --
 68602/100000: episode: 2136, duration: 0.016s, episode steps: 100, steps per second: 6114, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.201 [-1.751, 0.653], mean_best_reward: --
 68625/100000: episode: 2137, duration: 0.004s, episode steps: 23, steps per second: 5725, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.083 [-1.337, 0.629], mean_best_reward: --
 68735/100000: episode: 2138, duration: 0.017s, episode steps: 110, steps per second: 6317, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.382 [-1.713, 0.900], mean_best_reward: --
 68919/100000: episode: 2139, duration: 0.030s, episode steps: 184, steps per second: 6151, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.292 [-1.067, 1.677], mean_best_reward: --
 68964/100000: episode: 2140, duration: 0.007s, episode steps: 45, steps per second: 6084, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.137 [-0.574, 0.994], mean_best_reward: --
 69018/100000: episode: 2141, duration: 0.009s, episode steps: 54, steps per second: 6018, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.020 [-1.207, 1.829], mean_best_reward: --
 69079/100000: episode: 2142, duration: 0.010s, episode steps: 61, steps per second: 6164, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.026 [-1.130, 0.930], mean_best_reward: --
 69140/100000: episode: 2143, duration: 0.010s, episode steps: 61, steps per second: 6179, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.017 [-1.281, 0.994], mean_best_reward: --
 69170/100000: episode: 2144, duration: 0.005s, episode steps: 30, steps per second: 5538, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.124 [-1.054, 0.397], mean_best_reward: --
 69291/100000: episode: 2145, duration: 0.019s, episode steps: 121, steps per second: 6275, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.493 [-2.411, 0.768], mean_best_reward: --
 69381/100000: episode: 2146, duration: 0.014s, episode steps: 90, steps per second: 6314, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.108 [-1.569, 1.556], mean_best_reward: --
 69429/100000: episode: 2147, duration: 0.008s, episode steps: 48, steps per second: 6079, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.053 [-1.708, 1.215], mean_best_reward: --
 69546/100000: episode: 2148, duration: 0.019s, episode steps: 117, steps per second: 6047, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.223 [-2.052, 0.736], mean_best_reward: --
 69590/100000: episode: 2149, duration: 0.007s, episode steps: 44, steps per second: 6010, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-1.152, 0.619], mean_best_reward: --
 69606/100000: episode: 2150, duration: 0.003s, episode steps: 16, steps per second: 5497, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.069 [-2.127, 1.397], mean_best_reward: --
 69646/100000: episode: 2151, duration: 0.007s, episode steps: 40, steps per second: 5802, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.137 [-1.066, 0.535], mean_best_reward: 172.500000
 69772/100000: episode: 2152, duration: 0.020s, episode steps: 126, steps per second: 6225, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.099 [-2.325, 0.819], mean_best_reward: --
 69820/100000: episode: 2153, duration: 0.008s, episode steps: 48, steps per second: 5852, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.053 [-1.344, 0.580], mean_best_reward: --
 69910/100000: episode: 2154, duration: 0.014s, episode steps: 90, steps per second: 6240, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.048 [-1.202, 0.928], mean_best_reward: --
 69963/100000: episode: 2155, duration: 0.009s, episode steps: 53, steps per second: 6148, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.093 [-1.027, 0.968], mean_best_reward: --
 69979/100000: episode: 2156, duration: 0.003s, episode steps: 16, steps per second: 5296, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.434, 0.771], mean_best_reward: --
 70083/100000: episode: 2157, duration: 0.017s, episode steps: 104, steps per second: 6087, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.064 [-1.514, 1.245], mean_best_reward: --
 70165/100000: episode: 2158, duration: 0.014s, episode steps: 82, steps per second: 5996, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.402 [0.000, 1.000], mean observation: -0.224 [-3.076, 2.906], mean_best_reward: --
 70195/100000: episode: 2159, duration: 0.005s, episode steps: 30, steps per second: 5817, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-1.355, 0.825], mean_best_reward: --
 70258/100000: episode: 2160, duration: 0.011s, episode steps: 63, steps per second: 5962, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.190 [-1.480, 0.839], mean_best_reward: --
 70280/100000: episode: 2161, duration: 0.004s, episode steps: 22, steps per second: 5583, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.626, 1.003], mean_best_reward: --
 70311/100000: episode: 2162, duration: 0.005s, episode steps: 31, steps per second: 5885, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.039 [-1.403, 0.795], mean_best_reward: --
 70336/100000: episode: 2163, duration: 0.006s, episode steps: 25, steps per second: 4456, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.074 [-1.160, 0.595], mean_best_reward: --
 70397/100000: episode: 2164, duration: 0.011s, episode steps: 61, steps per second: 5673, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.410 [0.000, 1.000], mean observation: -0.096 [-2.054, 2.046], mean_best_reward: --
 70445/100000: episode: 2165, duration: 0.008s, episode steps: 48, steps per second: 5815, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.354 [0.000, 1.000], mean observation: -0.111 [-2.630, 2.790], mean_best_reward: --
 70503/100000: episode: 2166, duration: 0.009s, episode steps: 58, steps per second: 6161, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.112 [-1.113, 0.602], mean_best_reward: --
 70539/100000: episode: 2167, duration: 0.006s, episode steps: 36, steps per second: 5647, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.012 [-1.635, 1.138], mean_best_reward: --
 70563/100000: episode: 2168, duration: 0.004s, episode steps: 24, steps per second: 5774, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.024 [-2.448, 1.611], mean_best_reward: --
 70660/100000: episode: 2169, duration: 0.016s, episode steps: 97, steps per second: 6041, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.039 [-1.110, 0.916], mean_best_reward: --
 70693/100000: episode: 2170, duration: 0.006s, episode steps: 33, steps per second: 5772, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.068 [-1.718, 0.960], mean_best_reward: --
 70711/100000: episode: 2171, duration: 0.003s, episode steps: 18, steps per second: 5491, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.069 [-1.214, 2.067], mean_best_reward: --
 70829/100000: episode: 2172, duration: 0.019s, episode steps: 118, steps per second: 6163, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.132 [-1.381, 0.749], mean_best_reward: --
 70854/100000: episode: 2173, duration: 0.004s, episode steps: 25, steps per second: 5707, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.091 [-1.724, 0.785], mean_best_reward: --
 70901/100000: episode: 2174, duration: 0.009s, episode steps: 47, steps per second: 5398, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.362 [0.000, 1.000], mean observation: -0.146 [-2.622, 2.449], mean_best_reward: --
 70973/100000: episode: 2175, duration: 0.012s, episode steps: 72, steps per second: 6022, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-1.174, 0.968], mean_best_reward: --
 71000/100000: episode: 2176, duration: 0.005s, episode steps: 27, steps per second: 5432, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.024 [-1.196, 1.834], mean_best_reward: --
 71110/100000: episode: 2177, duration: 0.018s, episode steps: 110, steps per second: 6193, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: 0.064 [-2.706, 3.289], mean_best_reward: --
 71220/100000: episode: 2178, duration: 0.020s, episode steps: 110, steps per second: 5619, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.070 [-1.116, 1.018], mean_best_reward: --
 71245/100000: episode: 2179, duration: 0.004s, episode steps: 25, steps per second: 5715, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.038 [-1.199, 0.743], mean_best_reward: --
 71295/100000: episode: 2180, duration: 0.009s, episode steps: 50, steps per second: 5859, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-1.030, 0.756], mean_best_reward: --
 71376/100000: episode: 2181, duration: 0.013s, episode steps: 81, steps per second: 6263, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.037 [-1.735, 1.565], mean_best_reward: --
 71397/100000: episode: 2182, duration: 0.004s, episode steps: 21, steps per second: 5380, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.086 [-1.276, 0.737], mean_best_reward: --
 71498/100000: episode: 2183, duration: 0.017s, episode steps: 101, steps per second: 5968, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.124 [-1.772, 2.413], mean_best_reward: --
 71528/100000: episode: 2184, duration: 0.005s, episode steps: 30, steps per second: 5907, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.014 [-1.257, 0.653], mean_best_reward: --
 71555/100000: episode: 2185, duration: 0.005s, episode steps: 27, steps per second: 5879, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.018 [-1.165, 0.831], mean_best_reward: --
 71685/100000: episode: 2186, duration: 0.021s, episode steps: 130, steps per second: 6244, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.143 [-1.360, 1.328], mean_best_reward: --
 71755/100000: episode: 2187, duration: 0.011s, episode steps: 70, steps per second: 6235, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: -0.088 [-2.257, 2.424], mean_best_reward: --
 71838/100000: episode: 2188, duration: 0.014s, episode steps: 83, steps per second: 6024, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.152 [-1.373, 1.459], mean_best_reward: --
 71919/100000: episode: 2189, duration: 0.013s, episode steps: 81, steps per second: 6168, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.166 [-1.472, 1.427], mean_best_reward: --
 71935/100000: episode: 2190, duration: 0.003s, episode steps: 16, steps per second: 5484, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-0.922, 0.560], mean_best_reward: --
 72007/100000: episode: 2191, duration: 0.012s, episode steps: 72, steps per second: 6000, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.008 [-0.957, 1.219], mean_best_reward: --
 72180/100000: episode: 2192, duration: 0.028s, episode steps: 173, steps per second: 6174, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.029 [-1.503, 1.372], mean_best_reward: --
 72247/100000: episode: 2193, duration: 0.011s, episode steps: 67, steps per second: 6231, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.388 [0.000, 1.000], mean observation: -0.038 [-2.866, 3.276], mean_best_reward: --
 72318/100000: episode: 2194, duration: 0.012s, episode steps: 71, steps per second: 6094, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.170 [-1.449, 0.651], mean_best_reward: --
 72380/100000: episode: 2195, duration: 0.011s, episode steps: 62, steps per second: 5836, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.027 [-2.065, 2.204], mean_best_reward: --
 72459/100000: episode: 2196, duration: 0.013s, episode steps: 79, steps per second: 6217, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.392 [0.000, 1.000], mean observation: -0.105 [-3.270, 3.439], mean_best_reward: --
 72492/100000: episode: 2197, duration: 0.006s, episode steps: 33, steps per second: 5829, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.088 [-0.946, 0.413], mean_best_reward: --
 72520/100000: episode: 2198, duration: 0.005s, episode steps: 28, steps per second: 5817, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.018 [-1.665, 1.027], mean_best_reward: --
 72574/100000: episode: 2199, duration: 0.009s, episode steps: 54, steps per second: 5918, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.052 [-1.141, 1.008], mean_best_reward: --
 72606/100000: episode: 2200, duration: 0.005s, episode steps: 32, steps per second: 5914, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: -0.062 [-2.193, 1.185], mean_best_reward: --
 72714/100000: episode: 2201, duration: 0.018s, episode steps: 108, steps per second: 5920, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.180 [-1.219, 0.867], mean_best_reward: 138.500000
 72826/100000: episode: 2202, duration: 0.018s, episode steps: 112, steps per second: 6317, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.322 [-1.353, 0.691], mean_best_reward: --
 72838/100000: episode: 2203, duration: 0.002s, episode steps: 12, steps per second: 5062, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.115 [-1.939, 1.143], mean_best_reward: --
 72853/100000: episode: 2204, duration: 0.003s, episode steps: 15, steps per second: 5082, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.087 [-0.992, 1.571], mean_best_reward: --
 72940/100000: episode: 2205, duration: 0.014s, episode steps: 87, steps per second: 6079, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.010 [-0.946, 0.783], mean_best_reward: --
 72980/100000: episode: 2206, duration: 0.007s, episode steps: 40, steps per second: 5892, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.013 [-1.122, 0.793], mean_best_reward: --
 73088/100000: episode: 2207, duration: 0.017s, episode steps: 108, steps per second: 6259, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.098 [-1.704, 1.315], mean_best_reward: --
 73123/100000: episode: 2208, duration: 0.006s, episode steps: 35, steps per second: 5952, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.011 [-1.147, 0.809], mean_best_reward: --
 73174/100000: episode: 2209, duration: 0.008s, episode steps: 51, steps per second: 6075, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.075 [-1.171, 0.638], mean_best_reward: --
 73255/100000: episode: 2210, duration: 0.013s, episode steps: 81, steps per second: 6103, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.075 [-1.017, 1.561], mean_best_reward: --
 73384/100000: episode: 2211, duration: 0.020s, episode steps: 129, steps per second: 6375, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.086 [-1.343, 1.204], mean_best_reward: --
 73488/100000: episode: 2212, duration: 0.016s, episode steps: 104, steps per second: 6308, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.186 [-1.605, 1.104], mean_best_reward: --
 73592/100000: episode: 2213, duration: 0.017s, episode steps: 104, steps per second: 6048, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.042 [-0.976, 1.519], mean_best_reward: --
 73651/100000: episode: 2214, duration: 0.010s, episode steps: 59, steps per second: 6041, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.091 [-0.929, 0.604], mean_best_reward: --
 73678/100000: episode: 2215, duration: 0.005s, episode steps: 27, steps per second: 5736, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.085 [-1.286, 0.627], mean_best_reward: --
 73710/100000: episode: 2216, duration: 0.005s, episode steps: 32, steps per second: 5929, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.139, 0.566], mean_best_reward: --
 73756/100000: episode: 2217, duration: 0.008s, episode steps: 46, steps per second: 6082, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.044 [-1.632, 1.192], mean_best_reward: --
 73784/100000: episode: 2218, duration: 0.005s, episode steps: 28, steps per second: 5881, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.037 [-1.100, 0.627], mean_best_reward: --
 73824/100000: episode: 2219, duration: 0.008s, episode steps: 40, steps per second: 4763, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.004 [-1.086, 0.794], mean_best_reward: --
 73846/100000: episode: 2220, duration: 0.004s, episode steps: 22, steps per second: 5572, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.050 [-1.624, 1.166], mean_best_reward: --
 73947/100000: episode: 2221, duration: 0.016s, episode steps: 101, steps per second: 6302, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.206 [-1.809, 0.648], mean_best_reward: --
 74064/100000: episode: 2222, duration: 0.019s, episode steps: 117, steps per second: 6261, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.280 [-1.723, 1.064], mean_best_reward: --
 74194/100000: episode: 2223, duration: 0.021s, episode steps: 130, steps per second: 6054, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.098 [-2.172, 1.576], mean_best_reward: --
 74292/100000: episode: 2224, duration: 0.016s, episode steps: 98, steps per second: 6171, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.043 [-1.814, 1.528], mean_best_reward: --
 74337/100000: episode: 2225, duration: 0.008s, episode steps: 45, steps per second: 5703, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.137 [-2.911, 3.012], mean_best_reward: --
 74418/100000: episode: 2226, duration: 0.014s, episode steps: 81, steps per second: 5776, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: -0.139 [-3.057, 2.797], mean_best_reward: --
 74497/100000: episode: 2227, duration: 0.013s, episode steps: 79, steps per second: 6165, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.159 [-1.677, 1.141], mean_best_reward: --
 74623/100000: episode: 2228, duration: 0.020s, episode steps: 126, steps per second: 6326, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.256 [-1.698, 1.194], mean_best_reward: --
 74823/100000: episode: 2229, duration: 0.033s, episode steps: 200, steps per second: 6122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.156 [-1.344, 0.774], mean_best_reward: --
 74864/100000: episode: 2230, duration: 0.007s, episode steps: 41, steps per second: 6027, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.047 [-1.131, 0.590], mean_best_reward: --
 74902/100000: episode: 2231, duration: 0.007s, episode steps: 38, steps per second: 5734, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.279, 0.400], mean_best_reward: --
 75002/100000: episode: 2232, duration: 0.017s, episode steps: 100, steps per second: 5838, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.136 [-1.567, 1.367], mean_best_reward: --
 75143/100000: episode: 2233, duration: 0.023s, episode steps: 141, steps per second: 6237, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.177 [-1.505, 1.463], mean_best_reward: --
 75221/100000: episode: 2234, duration: 0.012s, episode steps: 78, steps per second: 6251, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.137 [-1.854, 1.640], mean_best_reward: --
 75255/100000: episode: 2235, duration: 0.006s, episode steps: 34, steps per second: 5955, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.060 [-1.361, 2.260], mean_best_reward: --
 75311/100000: episode: 2236, duration: 0.010s, episode steps: 56, steps per second: 5645, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.411 [0.000, 1.000], mean observation: -0.111 [-1.919, 1.971], mean_best_reward: --
 75484/100000: episode: 2237, duration: 0.027s, episode steps: 173, steps per second: 6389, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.173 [-1.447, 1.005], mean_best_reward: --
 75618/100000: episode: 2238, duration: 0.022s, episode steps: 134, steps per second: 6046, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.087 [-1.224, 0.945], mean_best_reward: --
 75693/100000: episode: 2239, duration: 0.012s, episode steps: 75, steps per second: 6104, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.117 [-1.725, 1.740], mean_best_reward: --
 75722/100000: episode: 2240, duration: 0.005s, episode steps: 29, steps per second: 5875, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.077 [-1.069, 0.571], mean_best_reward: --
 75814/100000: episode: 2241, duration: 0.015s, episode steps: 92, steps per second: 6159, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-1.034, 0.900], mean_best_reward: --
 76014/100000: episode: 2242, duration: 0.032s, episode steps: 200, steps per second: 6197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.132 [-0.925, 1.164], mean_best_reward: --
 76117/100000: episode: 2243, duration: 0.017s, episode steps: 103, steps per second: 6209, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.070 [-2.271, 1.726], mean_best_reward: --
 76151/100000: episode: 2244, duration: 0.006s, episode steps: 34, steps per second: 5962, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: 0.048 [-1.529, 1.389], mean_best_reward: --
 76180/100000: episode: 2245, duration: 0.005s, episode steps: 29, steps per second: 5572, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.075 [-1.352, 0.589], mean_best_reward: --
 76245/100000: episode: 2246, duration: 0.012s, episode steps: 65, steps per second: 5266, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.023 [-1.006, 0.961], mean_best_reward: --
 76322/100000: episode: 2247, duration: 0.013s, episode steps: 77, steps per second: 6089, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.171 [-2.257, 1.566], mean_best_reward: --
 76390/100000: episode: 2248, duration: 0.011s, episode steps: 68, steps per second: 6042, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.107 [-1.044, 0.765], mean_best_reward: --
 76556/100000: episode: 2249, duration: 0.027s, episode steps: 166, steps per second: 6081, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.188 [-1.312, 0.953], mean_best_reward: --
 76589/100000: episode: 2250, duration: 0.006s, episode steps: 33, steps per second: 5581, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.092 [-0.971, 0.573], mean_best_reward: --
 76645/100000: episode: 2251, duration: 0.009s, episode steps: 56, steps per second: 5982, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.018 [-1.092, 0.962], mean_best_reward: 154.500000
 76741/100000: episode: 2252, duration: 0.016s, episode steps: 96, steps per second: 6178, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.004 [-1.001, 0.994], mean_best_reward: --
 76860/100000: episode: 2253, duration: 0.020s, episode steps: 119, steps per second: 5870, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.022 [-1.499, 0.952], mean_best_reward: --
 76909/100000: episode: 2254, duration: 0.008s, episode steps: 49, steps per second: 6105, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.146 [-1.232, 0.749], mean_best_reward: --
 77109/100000: episode: 2255, duration: 0.033s, episode steps: 200, steps per second: 6131, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.132 [-1.142, 0.958], mean_best_reward: --
 77219/100000: episode: 2256, duration: 0.018s, episode steps: 110, steps per second: 6213, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: -0.181 [-2.324, 1.151], mean_best_reward: --
 77253/100000: episode: 2257, duration: 0.006s, episode steps: 34, steps per second: 5614, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.090 [-1.079, 0.624], mean_best_reward: --
 77265/100000: episode: 2258, duration: 0.002s, episode steps: 12, steps per second: 5074, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.091 [-1.700, 1.032], mean_best_reward: --
 77309/100000: episode: 2259, duration: 0.007s, episode steps: 44, steps per second: 6106, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-1.239, 0.813], mean_best_reward: --
 77369/100000: episode: 2260, duration: 0.011s, episode steps: 60, steps per second: 5431, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.101 [-0.606, 0.903], mean_best_reward: --
 77393/100000: episode: 2261, duration: 0.004s, episode steps: 24, steps per second: 5757, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.069 [-1.284, 0.645], mean_best_reward: --
 77515/100000: episode: 2262, duration: 0.019s, episode steps: 122, steps per second: 6303, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.238 [-1.158, 0.705], mean_best_reward: --
 77563/100000: episode: 2263, duration: 0.008s, episode steps: 48, steps per second: 6068, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.055 [-0.830, 1.156], mean_best_reward: --
 77678/100000: episode: 2264, duration: 0.019s, episode steps: 115, steps per second: 6064, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.252 [-1.692, 1.203], mean_best_reward: --
 77749/100000: episode: 2265, duration: 0.012s, episode steps: 71, steps per second: 6134, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.137 [-1.539, 1.568], mean_best_reward: --
 77851/100000: episode: 2266, duration: 0.016s, episode steps: 102, steps per second: 6286, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.283 [-1.647, 1.030], mean_best_reward: --
 77896/100000: episode: 2267, duration: 0.007s, episode steps: 45, steps per second: 6061, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.422 [0.000, 1.000], mean observation: -0.087 [-1.492, 1.603], mean_best_reward: --
 78028/100000: episode: 2268, duration: 0.025s, episode steps: 132, steps per second: 5198, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.210 [-0.897, 1.392], mean_best_reward: --
 78063/100000: episode: 2269, duration: 0.008s, episode steps: 35, steps per second: 4188, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.113 [-0.854, 0.615], mean_best_reward: --
 78115/100000: episode: 2270, duration: 0.009s, episode steps: 52, steps per second: 5857, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.067 [-1.416, 0.855], mean_best_reward: --
 78176/100000: episode: 2271, duration: 0.013s, episode steps: 61, steps per second: 4581, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.007 [-1.166, 0.805], mean_best_reward: --
 78346/100000: episode: 2272, duration: 0.027s, episode steps: 170, steps per second: 6348, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.021 [-0.960, 0.883], mean_best_reward: --
 78461/100000: episode: 2273, duration: 0.019s, episode steps: 115, steps per second: 6063, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.209 [-1.528, 1.041], mean_best_reward: --
 78499/100000: episode: 2274, duration: 0.008s, episode steps: 38, steps per second: 4578, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.002 [-1.673, 1.200], mean_best_reward: --
 78601/100000: episode: 2275, duration: 0.016s, episode steps: 102, steps per second: 6330, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.062 [-1.745, 1.763], mean_best_reward: --
 78625/100000: episode: 2276, duration: 0.004s, episode steps: 24, steps per second: 5750, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.060 [-1.163, 0.631], mean_best_reward: --
 78675/100000: episode: 2277, duration: 0.009s, episode steps: 50, steps per second: 5880, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.186, 0.622], mean_best_reward: --
 78799/100000: episode: 2278, duration: 0.021s, episode steps: 124, steps per second: 5939, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.050 [-1.346, 1.613], mean_best_reward: --
 78878/100000: episode: 2279, duration: 0.013s, episode steps: 79, steps per second: 6231, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.004 [-1.466, 1.382], mean_best_reward: --
 78976/100000: episode: 2280, duration: 0.016s, episode steps: 98, steps per second: 6185, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.321 [-1.926, 0.760], mean_best_reward: --
 78996/100000: episode: 2281, duration: 0.004s, episode steps: 20, steps per second: 5575, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.090 [-0.985, 1.560], mean_best_reward: --
 79017/100000: episode: 2282, duration: 0.004s, episode steps: 21, steps per second: 5684, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.041 [-1.623, 0.982], mean_best_reward: --
 79087/100000: episode: 2283, duration: 0.013s, episode steps: 70, steps per second: 5552, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.000 [-1.429, 0.817], mean_best_reward: --
 79115/100000: episode: 2284, duration: 0.005s, episode steps: 28, steps per second: 5818, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.128 [-1.487, 0.394], mean_best_reward: --
 79143/100000: episode: 2285, duration: 0.005s, episode steps: 28, steps per second: 5795, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.084 [-1.407, 0.592], mean_best_reward: --
 79198/100000: episode: 2286, duration: 0.009s, episode steps: 55, steps per second: 6188, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.094 [-1.308, 0.959], mean_best_reward: --
 79214/100000: episode: 2287, duration: 0.003s, episode steps: 16, steps per second: 5334, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.087 [-0.965, 1.476], mean_best_reward: --
 79314/100000: episode: 2288, duration: 0.016s, episode steps: 100, steps per second: 6294, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.089 [-1.742, 1.479], mean_best_reward: --
 79424/100000: episode: 2289, duration: 0.019s, episode steps: 110, steps per second: 5686, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.136 [-1.325, 1.256], mean_best_reward: --
 79601/100000: episode: 2290, duration: 0.028s, episode steps: 177, steps per second: 6434, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.299 [-1.102, 1.781], mean_best_reward: --
 79736/100000: episode: 2291, duration: 0.023s, episode steps: 135, steps per second: 5949, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.383 [-1.740, 0.703], mean_best_reward: --
 79833/100000: episode: 2292, duration: 0.016s, episode steps: 97, steps per second: 6179, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.010 [-0.813, 0.952], mean_best_reward: --
 79878/100000: episode: 2293, duration: 0.007s, episode steps: 45, steps per second: 6066, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.070 [-1.852, 1.703], mean_best_reward: --
 79895/100000: episode: 2294, duration: 0.003s, episode steps: 17, steps per second: 5493, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.064 [-1.065, 0.625], mean_best_reward: --
 79972/100000: episode: 2295, duration: 0.014s, episode steps: 77, steps per second: 5559, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.103 [-1.148, 1.083], mean_best_reward: --
 80057/100000: episode: 2296, duration: 0.014s, episode steps: 85, steps per second: 6137, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.117 [-1.873, 1.673], mean_best_reward: --
 80073/100000: episode: 2297, duration: 0.003s, episode steps: 16, steps per second: 5268, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.100 [-1.327, 0.606], mean_best_reward: --
 80093/100000: episode: 2298, duration: 0.004s, episode steps: 20, steps per second: 5464, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.097 [-1.067, 0.636], mean_best_reward: --
 80211/100000: episode: 2299, duration: 0.020s, episode steps: 118, steps per second: 5818, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.139 [-1.316, 1.029], mean_best_reward: --
 80356/100000: episode: 2300, duration: 0.023s, episode steps: 145, steps per second: 6243, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.218 [-1.836, 1.188], mean_best_reward: --
 80381/100000: episode: 2301, duration: 0.005s, episode steps: 25, steps per second: 5250, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.064 [-0.750, 1.175], mean_best_reward: 176.000000
 80569/100000: episode: 2302, duration: 0.031s, episode steps: 188, steps per second: 6163, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.049 [-1.904, 1.892], mean_best_reward: --
 80655/100000: episode: 2303, duration: 0.014s, episode steps: 86, steps per second: 6182, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.121 [-1.119, 0.810], mean_best_reward: --
 80761/100000: episode: 2304, duration: 0.017s, episode steps: 106, steps per second: 6316, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.262 [-1.295, 0.717], mean_best_reward: --
 80898/100000: episode: 2305, duration: 0.022s, episode steps: 137, steps per second: 6104, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.425 [-2.417, 0.758], mean_best_reward: --
 80909/100000: episode: 2306, duration: 0.002s, episode steps: 11, steps per second: 5079, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.082 [-1.306, 0.832], mean_best_reward: --
 80969/100000: episode: 2307, duration: 0.010s, episode steps: 60, steps per second: 6192, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.042 [-1.526, 0.807], mean_best_reward: --
 81003/100000: episode: 2308, duration: 0.006s, episode steps: 34, steps per second: 5639, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.048 [-1.601, 0.825], mean_best_reward: --
 81165/100000: episode: 2309, duration: 0.026s, episode steps: 162, steps per second: 6117, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.248 [-1.357, 0.802], mean_best_reward: --
 81298/100000: episode: 2310, duration: 0.021s, episode steps: 133, steps per second: 6250, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.088 [-1.522, 0.993], mean_best_reward: --
 81455/100000: episode: 2311, duration: 0.025s, episode steps: 157, steps per second: 6296, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.186 [-1.702, 1.287], mean_best_reward: --
 81490/100000: episode: 2312, duration: 0.006s, episode steps: 35, steps per second: 5691, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.134 [-1.063, 0.433], mean_best_reward: --
 81546/100000: episode: 2313, duration: 0.009s, episode steps: 56, steps per second: 6203, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.085 [-1.563, 0.640], mean_best_reward: --
 81578/100000: episode: 2314, duration: 0.006s, episode steps: 32, steps per second: 5573, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.140 [-0.347, 1.137], mean_best_reward: --
 81736/100000: episode: 2315, duration: 0.025s, episode steps: 158, steps per second: 6207, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.188 [-1.149, 1.827], mean_best_reward: --
 81771/100000: episode: 2316, duration: 0.006s, episode steps: 35, steps per second: 5667, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.119 [-0.914, 0.446], mean_best_reward: --
 81906/100000: episode: 2317, duration: 0.021s, episode steps: 135, steps per second: 6288, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.281 [-1.324, 0.759], mean_best_reward: --
 81988/100000: episode: 2318, duration: 0.014s, episode steps: 82, steps per second: 5805, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.026 [-1.502, 1.614], mean_best_reward: --
 82041/100000: episode: 2319, duration: 0.009s, episode steps: 53, steps per second: 6131, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.139 [-1.922, 0.690], mean_best_reward: --
 82105/100000: episode: 2320, duration: 0.011s, episode steps: 64, steps per second: 5926, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.157 [-2.292, 1.898], mean_best_reward: --
 82198/100000: episode: 2321, duration: 0.015s, episode steps: 93, steps per second: 6190, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.006 [-1.030, 0.954], mean_best_reward: --
 82218/100000: episode: 2322, duration: 0.004s, episode steps: 20, steps per second: 5635, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-0.922, 0.435], mean_best_reward: --
 82262/100000: episode: 2323, duration: 0.008s, episode steps: 44, steps per second: 5811, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.014 [-1.158, 0.797], mean_best_reward: --
 82288/100000: episode: 2324, duration: 0.005s, episode steps: 26, steps per second: 5167, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-1.121, 0.774], mean_best_reward: --
 82300/100000: episode: 2325, duration: 0.002s, episode steps: 12, steps per second: 5137, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.130 [-2.231, 1.337], mean_best_reward: --
 82358/100000: episode: 2326, duration: 0.010s, episode steps: 58, steps per second: 6035, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.011 [-1.380, 0.850], mean_best_reward: --
 82472/100000: episode: 2327, duration: 0.018s, episode steps: 114, steps per second: 6341, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.050 [-1.093, 1.166], mean_best_reward: --
 82509/100000: episode: 2328, duration: 0.006s, episode steps: 37, steps per second: 6000, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.119 [-1.279, 0.635], mean_best_reward: --
 82592/100000: episode: 2329, duration: 0.014s, episode steps: 83, steps per second: 6029, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.251 [-1.820, 1.167], mean_best_reward: --
 82632/100000: episode: 2330, duration: 0.007s, episode steps: 40, steps per second: 6071, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.144 [-0.885, 0.557], mean_best_reward: --
 82693/100000: episode: 2331, duration: 0.010s, episode steps: 61, steps per second: 6102, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.098 [-1.068, 0.592], mean_best_reward: --
 82773/100000: episode: 2332, duration: 0.013s, episode steps: 80, steps per second: 6144, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: -0.351 [-2.952, 1.735], mean_best_reward: --
 82822/100000: episode: 2333, duration: 0.008s, episode steps: 49, steps per second: 6068, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.198 [-1.460, 0.924], mean_best_reward: --
 82976/100000: episode: 2334, duration: 0.025s, episode steps: 154, steps per second: 6132, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.445 [-2.422, 0.732], mean_best_reward: --
 83045/100000: episode: 2335, duration: 0.011s, episode steps: 69, steps per second: 6010, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.185 [-1.845, 1.646], mean_best_reward: --
 83063/100000: episode: 2336, duration: 0.003s, episode steps: 18, steps per second: 5546, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.081 [-1.494, 0.972], mean_best_reward: --
 83181/100000: episode: 2337, duration: 0.019s, episode steps: 118, steps per second: 6183, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.341 [-0.730, 1.522], mean_best_reward: --
 83233/100000: episode: 2338, duration: 0.008s, episode steps: 52, steps per second: 6134, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.074 [-1.138, 0.523], mean_best_reward: --
 83297/100000: episode: 2339, duration: 0.010s, episode steps: 64, steps per second: 6218, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.040 [-0.818, 1.355], mean_best_reward: --
 83376/100000: episode: 2340, duration: 0.013s, episode steps: 79, steps per second: 6272, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.046 [-1.681, 0.935], mean_best_reward: --
 83493/100000: episode: 2341, duration: 0.019s, episode steps: 117, steps per second: 6164, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.214 [-1.527, 1.085], mean_best_reward: --
 83554/100000: episode: 2342, duration: 0.010s, episode steps: 61, steps per second: 6174, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.208 [-1.712, 1.248], mean_best_reward: --
 83593/100000: episode: 2343, duration: 0.006s, episode steps: 39, steps per second: 6035, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.029 [-1.512, 1.006], mean_best_reward: --
 83649/100000: episode: 2344, duration: 0.009s, episode steps: 56, steps per second: 6046, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-1.112, 1.244], mean_best_reward: --
 83666/100000: episode: 2345, duration: 0.003s, episode steps: 17, steps per second: 5514, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.062 [-1.381, 0.808], mean_best_reward: --
 83700/100000: episode: 2346, duration: 0.006s, episode steps: 34, steps per second: 5994, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.760, 1.050], mean_best_reward: --
 83794/100000: episode: 2347, duration: 0.016s, episode steps: 94, steps per second: 6015, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.149 [-1.169, 1.787], mean_best_reward: --
 83916/100000: episode: 2348, duration: 0.020s, episode steps: 122, steps per second: 5968, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.332 [-1.820, 1.209], mean_best_reward: --
 83930/100000: episode: 2349, duration: 0.003s, episode steps: 14, steps per second: 5172, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.088 [-2.103, 1.223], mean_best_reward: --
 83991/100000: episode: 2350, duration: 0.010s, episode steps: 61, steps per second: 6249, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.154 [-1.924, 1.735], mean_best_reward: --
 84157/100000: episode: 2351, duration: 0.027s, episode steps: 166, steps per second: 6126, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.257 [-1.667, 1.068], mean_best_reward: 172.000000
 84171/100000: episode: 2352, duration: 0.003s, episode steps: 14, steps per second: 4723, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.081 [-1.575, 0.998], mean_best_reward: --
 84312/100000: episode: 2353, duration: 0.022s, episode steps: 141, steps per second: 6272, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.058 [-1.063, 1.026], mean_best_reward: --
 84352/100000: episode: 2354, duration: 0.007s, episode steps: 40, steps per second: 5512, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.021 [-1.537, 1.121], mean_best_reward: --
 84383/100000: episode: 2355, duration: 0.005s, episode steps: 31, steps per second: 5895, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.085 [-1.045, 0.421], mean_best_reward: --
 84417/100000: episode: 2356, duration: 0.006s, episode steps: 34, steps per second: 5923, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.684, 1.056], mean_best_reward: --
 84434/100000: episode: 2357, duration: 0.003s, episode steps: 17, steps per second: 5501, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.067 [-1.264, 0.830], mean_best_reward: --
 84455/100000: episode: 2358, duration: 0.004s, episode steps: 21, steps per second: 5711, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.104 [-0.401, 1.131], mean_best_reward: --
 84573/100000: episode: 2359, duration: 0.019s, episode steps: 118, steps per second: 6307, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.159 [-2.438, 1.846], mean_best_reward: --
 84607/100000: episode: 2360, duration: 0.007s, episode steps: 34, steps per second: 4997, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.165 [-0.916, 0.610], mean_best_reward: --
 84782/100000: episode: 2361, duration: 0.028s, episode steps: 175, steps per second: 6352, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: 0.201 [-0.696, 1.873], mean_best_reward: --
 84865/100000: episode: 2362, duration: 0.014s, episode steps: 83, steps per second: 6076, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.131 [-2.029, 1.626], mean_best_reward: --
 84885/100000: episode: 2363, duration: 0.004s, episode steps: 20, steps per second: 5260, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.058 [-0.938, 1.616], mean_best_reward: --
 85056/100000: episode: 2364, duration: 0.027s, episode steps: 171, steps per second: 6348, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.272 [-1.890, 1.191], mean_best_reward: --
 85118/100000: episode: 2365, duration: 0.010s, episode steps: 62, steps per second: 6134, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.097 [-1.918, 1.554], mean_best_reward: --
 85139/100000: episode: 2366, duration: 0.004s, episode steps: 21, steps per second: 5621, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.101 [-0.744, 1.218], mean_best_reward: --
 85162/100000: episode: 2367, duration: 0.004s, episode steps: 23, steps per second: 5776, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.033 [-1.770, 1.169], mean_best_reward: --
 85176/100000: episode: 2368, duration: 0.003s, episode steps: 14, steps per second: 5325, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.105 [-0.631, 1.260], mean_best_reward: --
 85239/100000: episode: 2369, duration: 0.011s, episode steps: 63, steps per second: 5754, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.103 [-1.087, 0.450], mean_best_reward: --
 85324/100000: episode: 2370, duration: 0.014s, episode steps: 85, steps per second: 6286, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.192 [-1.846, 1.442], mean_best_reward: --
 85397/100000: episode: 2371, duration: 0.012s, episode steps: 73, steps per second: 6089, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.003 [-1.061, 0.959], mean_best_reward: --
 85416/100000: episode: 2372, duration: 0.003s, episode steps: 19, steps per second: 5550, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.087 [-1.468, 0.948], mean_best_reward: --
 85579/100000: episode: 2373, duration: 0.028s, episode steps: 163, steps per second: 5806, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.099 [-1.863, 1.290], mean_best_reward: --
 85699/100000: episode: 2374, duration: 0.019s, episode steps: 120, steps per second: 6321, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.389 [-1.783, 0.694], mean_best_reward: --
 85734/100000: episode: 2375, duration: 0.006s, episode steps: 35, steps per second: 5962, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.036 [-1.574, 0.944], mean_best_reward: --
 85767/100000: episode: 2376, duration: 0.006s, episode steps: 33, steps per second: 5809, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.043 [-1.294, 0.759], mean_best_reward: --
 85896/100000: episode: 2377, duration: 0.021s, episode steps: 129, steps per second: 6119, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.066 [-1.177, 1.612], mean_best_reward: --
 85929/100000: episode: 2378, duration: 0.006s, episode steps: 33, steps per second: 5957, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.010 [-1.501, 1.013], mean_best_reward: --
 86023/100000: episode: 2379, duration: 0.016s, episode steps: 94, steps per second: 6029, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.211 [-1.938, 1.599], mean_best_reward: --
 86155/100000: episode: 2380, duration: 0.022s, episode steps: 132, steps per second: 6060, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.138 [-1.487, 1.135], mean_best_reward: --
 86220/100000: episode: 2381, duration: 0.012s, episode steps: 65, steps per second: 5426, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.005 [-1.051, 1.255], mean_best_reward: --
 86300/100000: episode: 2382, duration: 0.013s, episode steps: 80, steps per second: 6055, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.167 [-1.642, 1.118], mean_best_reward: --
 86402/100000: episode: 2383, duration: 0.018s, episode steps: 102, steps per second: 5819, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.005 [-1.628, 0.969], mean_best_reward: --
 86417/100000: episode: 2384, duration: 0.003s, episode steps: 15, steps per second: 5380, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-0.995, 1.955], mean_best_reward: --
 86451/100000: episode: 2385, duration: 0.006s, episode steps: 34, steps per second: 5774, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.090 [-0.440, 0.987], mean_best_reward: --
 86571/100000: episode: 2386, duration: 0.019s, episode steps: 120, steps per second: 6238, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.098 [-2.096, 1.658], mean_best_reward: --
 86655/100000: episode: 2387, duration: 0.014s, episode steps: 84, steps per second: 6140, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.071 [-1.571, 1.730], mean_best_reward: --
 86745/100000: episode: 2388, duration: 0.015s, episode steps: 90, steps per second: 6127, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-1.260, 0.858], mean_best_reward: --
 86848/100000: episode: 2389, duration: 0.016s, episode steps: 103, steps per second: 6302, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.228 [-1.344, 0.903], mean_best_reward: --
 86913/100000: episode: 2390, duration: 0.010s, episode steps: 65, steps per second: 6227, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.177 [-2.042, 1.636], mean_best_reward: --
 87060/100000: episode: 2391, duration: 0.024s, episode steps: 147, steps per second: 6072, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.015 [-1.693, 1.287], mean_best_reward: --
 87073/100000: episode: 2392, duration: 0.002s, episode steps: 13, steps per second: 5234, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.106 [-1.840, 1.129], mean_best_reward: --
 87099/100000: episode: 2393, duration: 0.004s, episode steps: 26, steps per second: 5818, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.050 [-0.937, 1.391], mean_best_reward: --
 87252/100000: episode: 2394, duration: 0.025s, episode steps: 153, steps per second: 6192, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.163 [-0.740, 1.131], mean_best_reward: --
 87318/100000: episode: 2395, duration: 0.011s, episode steps: 66, steps per second: 6059, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.060 [-1.499, 0.778], mean_best_reward: --
 87347/100000: episode: 2396, duration: 0.005s, episode steps: 29, steps per second: 5823, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.094 [-1.042, 0.595], mean_best_reward: --
 87376/100000: episode: 2397, duration: 0.005s, episode steps: 29, steps per second: 5885, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.081 [-1.175, 0.558], mean_best_reward: --
 87396/100000: episode: 2398, duration: 0.004s, episode steps: 20, steps per second: 5663, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-0.819, 1.645], mean_best_reward: --
 87425/100000: episode: 2399, duration: 0.005s, episode steps: 29, steps per second: 5435, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.049 [-1.180, 0.636], mean_best_reward: --
 87454/100000: episode: 2400, duration: 0.005s, episode steps: 29, steps per second: 5854, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.121 [-0.773, 1.841], mean_best_reward: --
 87481/100000: episode: 2401, duration: 0.005s, episode steps: 27, steps per second: 5511, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.091 [-0.656, 1.619], mean_best_reward: 154.000000
 87603/100000: episode: 2402, duration: 0.020s, episode steps: 122, steps per second: 6038, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.082 [-1.191, 0.947], mean_best_reward: --
 87638/100000: episode: 2403, duration: 0.006s, episode steps: 35, steps per second: 5918, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.144 [-0.973, 0.596], mean_best_reward: --
 87708/100000: episode: 2404, duration: 0.011s, episode steps: 70, steps per second: 6202, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.004 [-0.841, 1.274], mean_best_reward: --
 87768/100000: episode: 2405, duration: 0.010s, episode steps: 60, steps per second: 6220, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.115 [-2.327, 2.614], mean_best_reward: --
 87862/100000: episode: 2406, duration: 0.015s, episode steps: 94, steps per second: 6173, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.259 [-1.636, 0.875], mean_best_reward: --
 87932/100000: episode: 2407, duration: 0.011s, episode steps: 70, steps per second: 6203, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.097 [-1.105, 0.864], mean_best_reward: --
 87949/100000: episode: 2408, duration: 0.003s, episode steps: 17, steps per second: 5508, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.058 [-0.994, 1.571], mean_best_reward: --
 87980/100000: episode: 2409, duration: 0.005s, episode steps: 31, steps per second: 5935, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.051 [-1.396, 0.839], mean_best_reward: --
 88081/100000: episode: 2410, duration: 0.016s, episode steps: 101, steps per second: 6239, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.018 [-1.497, 1.750], mean_best_reward: --
 88150/100000: episode: 2411, duration: 0.012s, episode steps: 69, steps per second: 6000, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.201 [-1.287, 0.546], mean_best_reward: --
 88195/100000: episode: 2412, duration: 0.007s, episode steps: 45, steps per second: 6055, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.019 [-1.169, 0.764], mean_best_reward: --
 88262/100000: episode: 2413, duration: 0.011s, episode steps: 67, steps per second: 6224, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.133 [-1.521, 1.391], mean_best_reward: --
 88384/100000: episode: 2414, duration: 0.020s, episode steps: 122, steps per second: 6202, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.386 [-1.716, 0.882], mean_best_reward: --
 88413/100000: episode: 2415, duration: 0.005s, episode steps: 29, steps per second: 5564, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.058 [-1.466, 0.817], mean_best_reward: --
 88457/100000: episode: 2416, duration: 0.008s, episode steps: 44, steps per second: 5828, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.033 [-1.443, 0.658], mean_best_reward: --
 88513/100000: episode: 2417, duration: 0.009s, episode steps: 56, steps per second: 5983, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.090 [-1.560, 1.596], mean_best_reward: --
 88546/100000: episode: 2418, duration: 0.005s, episode steps: 33, steps per second: 6000, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.035 [-1.273, 0.772], mean_best_reward: --
 88586/100000: episode: 2419, duration: 0.007s, episode steps: 40, steps per second: 5570, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.017 [-1.521, 2.275], mean_best_reward: --
 88704/100000: episode: 2420, duration: 0.019s, episode steps: 118, steps per second: 6166, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.214 [-1.183, 0.953], mean_best_reward: --
 88801/100000: episode: 2421, duration: 0.016s, episode steps: 97, steps per second: 6082, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.024 [-1.155, 1.289], mean_best_reward: --
 88952/100000: episode: 2422, duration: 0.024s, episode steps: 151, steps per second: 6260, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.036 [-1.178, 1.362], mean_best_reward: --
 88980/100000: episode: 2423, duration: 0.005s, episode steps: 28, steps per second: 5594, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.053 [-1.190, 0.794], mean_best_reward: --
 89071/100000: episode: 2424, duration: 0.015s, episode steps: 91, steps per second: 6116, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.074 [-1.519, 1.392], mean_best_reward: --
 89271/100000: episode: 2425, duration: 0.031s, episode steps: 200, steps per second: 6435, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.041, 0.853], mean_best_reward: --
 89345/100000: episode: 2426, duration: 0.013s, episode steps: 74, steps per second: 5870, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.013 [-1.174, 1.288], mean_best_reward: --
 89491/100000: episode: 2427, duration: 0.023s, episode steps: 146, steps per second: 6284, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.137 [-1.178, 0.907], mean_best_reward: --
 89665/100000: episode: 2428, duration: 0.027s, episode steps: 174, steps per second: 6334, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.019 [-1.634, 1.008], mean_best_reward: --
 89816/100000: episode: 2429, duration: 0.024s, episode steps: 151, steps per second: 6249, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: -0.067 [-1.442, 0.975], mean_best_reward: --
 89924/100000: episode: 2430, duration: 0.018s, episode steps: 108, steps per second: 5988, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.342 [-2.786, 1.424], mean_best_reward: --
 89997/100000: episode: 2431, duration: 0.012s, episode steps: 73, steps per second: 6177, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.156 [-1.565, 1.559], mean_best_reward: --
 90041/100000: episode: 2432, duration: 0.007s, episode steps: 44, steps per second: 6075, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.004 [-1.160, 1.543], mean_best_reward: --
 90091/100000: episode: 2433, duration: 0.008s, episode steps: 50, steps per second: 5965, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.107 [-0.983, 1.339], mean_best_reward: --
 90162/100000: episode: 2434, duration: 0.011s, episode steps: 71, steps per second: 6267, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.074 [-0.966, 1.340], mean_best_reward: --
 90362/100000: episode: 2435, duration: 0.032s, episode steps: 200, steps per second: 6318, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-1.484, 1.205], mean_best_reward: --
 90493/100000: episode: 2436, duration: 0.021s, episode steps: 131, steps per second: 6339, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: -0.082 [-1.829, 1.362], mean_best_reward: --
 90555/100000: episode: 2437, duration: 0.010s, episode steps: 62, steps per second: 6027, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.124 [-1.176, 1.209], mean_best_reward: --
 90569/100000: episode: 2438, duration: 0.003s, episode steps: 14, steps per second: 5399, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.106 [-1.610, 0.803], mean_best_reward: --
 90708/100000: episode: 2439, duration: 0.022s, episode steps: 139, steps per second: 6267, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.037 [-1.363, 1.119], mean_best_reward: --
 90892/100000: episode: 2440, duration: 0.029s, episode steps: 184, steps per second: 6422, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.259 [-2.229, 1.433], mean_best_reward: --
 90996/100000: episode: 2441, duration: 0.016s, episode steps: 104, steps per second: 6342, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.177, 0.919], mean_best_reward: --
 91124/100000: episode: 2442, duration: 0.020s, episode steps: 128, steps per second: 6351, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.092 [-1.768, 2.364], mean_best_reward: --
 91188/100000: episode: 2443, duration: 0.011s, episode steps: 64, steps per second: 5867, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.406 [0.000, 1.000], mean observation: -0.100 [-2.299, 2.694], mean_best_reward: --
 91358/100000: episode: 2444, duration: 0.027s, episode steps: 170, steps per second: 6359, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.001 [-1.163, 1.079], mean_best_reward: --
 91450/100000: episode: 2445, duration: 0.016s, episode steps: 92, steps per second: 5919, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.005 [-1.191, 1.390], mean_best_reward: --
 91483/100000: episode: 2446, duration: 0.006s, episode steps: 33, steps per second: 5900, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: -0.048 [-1.366, 1.557], mean_best_reward: --
 91508/100000: episode: 2447, duration: 0.005s, episode steps: 25, steps per second: 5525, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.053 [-1.649, 1.164], mean_best_reward: --
 91556/100000: episode: 2448, duration: 0.008s, episode steps: 48, steps per second: 6057, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.012 [-1.429, 0.818], mean_best_reward: --
 91708/100000: episode: 2449, duration: 0.025s, episode steps: 152, steps per second: 6165, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.173 [-1.306, 1.070], mean_best_reward: --
 91727/100000: episode: 2450, duration: 0.004s, episode steps: 19, steps per second: 4627, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.083 [-1.705, 1.007], mean_best_reward: --
 91742/100000: episode: 2451, duration: 0.003s, episode steps: 15, steps per second: 4841, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.087 [-1.472, 0.842], mean_best_reward: 137.500000
 91832/100000: episode: 2452, duration: 0.015s, episode steps: 90, steps per second: 6159, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.005 [-1.489, 1.136], mean_best_reward: --
 91848/100000: episode: 2453, duration: 0.003s, episode steps: 16, steps per second: 5430, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.098 [-1.032, 0.613], mean_best_reward: --
 91916/100000: episode: 2454, duration: 0.011s, episode steps: 68, steps per second: 6018, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: -0.163 [-3.368, 3.216], mean_best_reward: --
 91973/100000: episode: 2455, duration: 0.009s, episode steps: 57, steps per second: 6127, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.037 [-1.011, 0.818], mean_best_reward: --
 92026/100000: episode: 2456, duration: 0.009s, episode steps: 53, steps per second: 5748, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.091 [-0.583, 0.916], mean_best_reward: --
 92071/100000: episode: 2457, duration: 0.008s, episode steps: 45, steps per second: 5670, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.038 [-1.113, 0.742], mean_best_reward: --
 92101/100000: episode: 2458, duration: 0.005s, episode steps: 30, steps per second: 5676, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.089 [-1.198, 0.457], mean_best_reward: --
 92126/100000: episode: 2459, duration: 0.004s, episode steps: 25, steps per second: 5759, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.057 [-1.539, 0.953], mean_best_reward: --
 92172/100000: episode: 2460, duration: 0.008s, episode steps: 46, steps per second: 5726, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.013 [-1.530, 1.001], mean_best_reward: --
 92280/100000: episode: 2461, duration: 0.018s, episode steps: 108, steps per second: 6147, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.086 [-1.185, 0.564], mean_best_reward: --
 92424/100000: episode: 2462, duration: 0.024s, episode steps: 144, steps per second: 6084, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.097 [-1.666, 1.170], mean_best_reward: --
 92565/100000: episode: 2463, duration: 0.022s, episode steps: 141, steps per second: 6364, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.037 [-0.968, 0.671], mean_best_reward: --
 92591/100000: episode: 2464, duration: 0.005s, episode steps: 26, steps per second: 5150, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.018 [-1.533, 1.023], mean_best_reward: --
 92639/100000: episode: 2465, duration: 0.008s, episode steps: 48, steps per second: 5983, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.107 [-1.549, 1.523], mean_best_reward: --
 92753/100000: episode: 2466, duration: 0.018s, episode steps: 114, steps per second: 6168, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.030 [-1.014, 1.527], mean_best_reward: --
 92781/100000: episode: 2467, duration: 0.005s, episode steps: 28, steps per second: 5717, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-0.933, 0.601], mean_best_reward: --
 92804/100000: episode: 2468, duration: 0.004s, episode steps: 23, steps per second: 5747, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.083 [-1.234, 0.609], mean_best_reward: --
 92930/100000: episode: 2469, duration: 0.020s, episode steps: 126, steps per second: 6169, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.273 [-1.320, 0.888], mean_best_reward: --
 92962/100000: episode: 2470, duration: 0.006s, episode steps: 32, steps per second: 5613, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.051 [-1.814, 1.032], mean_best_reward: --
 93134/100000: episode: 2471, duration: 0.027s, episode steps: 172, steps per second: 6366, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.372 [-2.621, 1.145], mean_best_reward: --
 93164/100000: episode: 2472, duration: 0.005s, episode steps: 30, steps per second: 5539, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.069, 0.570], mean_best_reward: --
 93268/100000: episode: 2473, duration: 0.017s, episode steps: 104, steps per second: 6115, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.129 [-1.338, 1.002], mean_best_reward: --
 93333/100000: episode: 2474, duration: 0.011s, episode steps: 65, steps per second: 6188, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.121 [-2.527, 2.644], mean_best_reward: --
 93362/100000: episode: 2475, duration: 0.005s, episode steps: 29, steps per second: 5743, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.039 [-1.271, 0.798], mean_best_reward: --
 93432/100000: episode: 2476, duration: 0.011s, episode steps: 70, steps per second: 6142, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.126 [-1.314, 1.207], mean_best_reward: --
 93496/100000: episode: 2477, duration: 0.011s, episode steps: 64, steps per second: 5981, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.024 [-1.004, 0.702], mean_best_reward: --
 93557/100000: episode: 2478, duration: 0.010s, episode steps: 61, steps per second: 6025, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.082 [-1.414, 0.683], mean_best_reward: --
 93577/100000: episode: 2479, duration: 0.004s, episode steps: 20, steps per second: 5636, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.456, 0.953], mean_best_reward: --
 93625/100000: episode: 2480, duration: 0.008s, episode steps: 48, steps per second: 5897, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.292, 0.579], mean_best_reward: --
 93684/100000: episode: 2481, duration: 0.009s, episode steps: 59, steps per second: 6245, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.146 [-0.867, 0.446], mean_best_reward: --
 93728/100000: episode: 2482, duration: 0.008s, episode steps: 44, steps per second: 5856, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.117 [-0.847, 0.385], mean_best_reward: --
 93818/100000: episode: 2483, duration: 0.015s, episode steps: 90, steps per second: 6044, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-1.527, 1.225], mean_best_reward: --
 93847/100000: episode: 2484, duration: 0.005s, episode steps: 29, steps per second: 5824, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.049 [-1.258, 0.781], mean_best_reward: --
 93872/100000: episode: 2485, duration: 0.004s, episode steps: 25, steps per second: 5624, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.280 [0.000, 1.000], mean observation: 0.029 [-2.159, 3.122], mean_best_reward: --
 94027/100000: episode: 2486, duration: 0.024s, episode steps: 155, steps per second: 6384, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.233 [-1.671, 0.923], mean_best_reward: --
 94119/100000: episode: 2487, duration: 0.015s, episode steps: 92, steps per second: 6139, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.167 [-1.493, 0.918], mean_best_reward: --
 94142/100000: episode: 2488, duration: 0.004s, episode steps: 23, steps per second: 5721, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.103 [-0.947, 0.455], mean_best_reward: --
 94200/100000: episode: 2489, duration: 0.009s, episode steps: 58, steps per second: 6234, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: -0.129 [-2.661, 2.623], mean_best_reward: --
 94311/100000: episode: 2490, duration: 0.018s, episode steps: 111, steps per second: 6114, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.215 [-1.818, 1.029], mean_best_reward: --
 94371/100000: episode: 2491, duration: 0.010s, episode steps: 60, steps per second: 5909, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.134 [-1.132, 1.351], mean_best_reward: --
 94509/100000: episode: 2492, duration: 0.022s, episode steps: 138, steps per second: 6350, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.242 [-1.362, 0.995], mean_best_reward: --
 94552/100000: episode: 2493, duration: 0.007s, episode steps: 43, steps per second: 5908, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.098 [-1.352, 0.564], mean_best_reward: --
 94626/100000: episode: 2494, duration: 0.012s, episode steps: 74, steps per second: 6221, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.035 [-1.998, 1.186], mean_best_reward: --
 94641/100000: episode: 2495, duration: 0.003s, episode steps: 15, steps per second: 4981, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.116 [-1.179, 0.565], mean_best_reward: --
 94801/100000: episode: 2496, duration: 0.025s, episode steps: 160, steps per second: 6320, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.234 [-1.478, 0.861], mean_best_reward: --
 94880/100000: episode: 2497, duration: 0.013s, episode steps: 79, steps per second: 6138, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.271 [-1.907, 1.159], mean_best_reward: --
 94957/100000: episode: 2498, duration: 0.013s, episode steps: 77, steps per second: 5830, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.416 [0.000, 1.000], mean observation: -0.034 [-2.517, 2.895], mean_best_reward: --
 95046/100000: episode: 2499, duration: 0.015s, episode steps: 89, steps per second: 6137, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.132 [-1.108, 0.921], mean_best_reward: --
 95068/100000: episode: 2500, duration: 0.004s, episode steps: 22, steps per second: 5693, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.053 [-2.427, 1.568], mean_best_reward: --
 95087/100000: episode: 2501, duration: 0.004s, episode steps: 19, steps per second: 5161, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.099 [-1.709, 0.946], mean_best_reward: 144.500000
 95114/100000: episode: 2502, duration: 0.005s, episode steps: 27, steps per second: 5411, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.055 [-1.189, 0.623], mean_best_reward: --
 95248/100000: episode: 2503, duration: 0.022s, episode steps: 134, steps per second: 6152, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.225 [-1.259, 0.809], mean_best_reward: --
 95280/100000: episode: 2504, duration: 0.005s, episode steps: 32, steps per second: 5965, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.819, 1.409], mean_best_reward: --
 95327/100000: episode: 2505, duration: 0.008s, episode steps: 47, steps per second: 6110, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.121 [-1.472, 0.572], mean_best_reward: --
 95437/100000: episode: 2506, duration: 0.017s, episode steps: 110, steps per second: 6401, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.022 [-3.093, 3.071], mean_best_reward: --
 95620/100000: episode: 2507, duration: 0.029s, episode steps: 183, steps per second: 6352, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.031 [-0.962, 1.145], mean_best_reward: --
 95642/100000: episode: 2508, duration: 0.004s, episode steps: 22, steps per second: 5742, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.039 [-1.670, 1.178], mean_best_reward: --
 95771/100000: episode: 2509, duration: 0.020s, episode steps: 129, steps per second: 6434, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.180 [-0.833, 1.174], mean_best_reward: --
 95884/100000: episode: 2510, duration: 0.018s, episode steps: 113, steps per second: 6153, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.154 [-2.265, 1.748], mean_best_reward: --
 96084/100000: episode: 2511, duration: 0.031s, episode steps: 200, steps per second: 6363, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.343 [-2.140, 1.279], mean_best_reward: --
 96221/100000: episode: 2512, duration: 0.022s, episode steps: 137, steps per second: 6154, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.020 [-1.123, 1.335], mean_best_reward: --
 96251/100000: episode: 2513, duration: 0.005s, episode steps: 30, steps per second: 5909, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.087 [-1.937, 0.849], mean_best_reward: --
 96376/100000: episode: 2514, duration: 0.020s, episode steps: 125, steps per second: 6386, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.026 [-0.799, 1.095], mean_best_reward: --
 96496/100000: episode: 2515, duration: 0.019s, episode steps: 120, steps per second: 6167, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.040 [-1.304, 1.488], mean_best_reward: --
 96559/100000: episode: 2516, duration: 0.010s, episode steps: 63, steps per second: 6023, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.129 [-1.110, 1.293], mean_best_reward: --
 96587/100000: episode: 2517, duration: 0.005s, episode steps: 28, steps per second: 5864, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.089, 0.729], mean_best_reward: --
 96608/100000: episode: 2518, duration: 0.004s, episode steps: 21, steps per second: 5284, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.064 [-0.987, 1.612], mean_best_reward: --
 96667/100000: episode: 2519, duration: 0.010s, episode steps: 59, steps per second: 6153, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.052 [-1.142, 0.935], mean_best_reward: --
 96716/100000: episode: 2520, duration: 0.008s, episode steps: 49, steps per second: 6100, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.388 [0.000, 1.000], mean observation: -0.006 [-2.114, 2.887], mean_best_reward: --
 96821/100000: episode: 2521, duration: 0.017s, episode steps: 105, steps per second: 6111, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.003 [-1.515, 1.310], mean_best_reward: --
 96846/100000: episode: 2522, duration: 0.004s, episode steps: 25, steps per second: 5767, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.080 [-0.952, 1.643], mean_best_reward: --
 97002/100000: episode: 2523, duration: 0.025s, episode steps: 156, steps per second: 6319, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.077 [-2.709, 2.943], mean_best_reward: --
 97119/100000: episode: 2524, duration: 0.019s, episode steps: 117, steps per second: 6209, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.140 [-1.406, 1.142], mean_best_reward: --
 97163/100000: episode: 2525, duration: 0.007s, episode steps: 44, steps per second: 6061, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.038 [-0.948, 0.556], mean_best_reward: --
 97206/100000: episode: 2526, duration: 0.007s, episode steps: 43, steps per second: 6017, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.110 [-1.239, 0.405], mean_best_reward: --
 97269/100000: episode: 2527, duration: 0.010s, episode steps: 63, steps per second: 6205, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.065 [-0.894, 0.915], mean_best_reward: --
 97335/100000: episode: 2528, duration: 0.011s, episode steps: 66, steps per second: 6182, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.107 [-1.020, 1.271], mean_best_reward: --
 97449/100000: episode: 2529, duration: 0.018s, episode steps: 114, steps per second: 6202, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.275 [-1.018, 1.509], mean_best_reward: --
 97551/100000: episode: 2530, duration: 0.016s, episode steps: 102, steps per second: 6190, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.271, 0.923], mean_best_reward: --
 97582/100000: episode: 2531, duration: 0.005s, episode steps: 31, steps per second: 5901, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.063 [-1.750, 0.841], mean_best_reward: --
 97635/100000: episode: 2532, duration: 0.009s, episode steps: 53, steps per second: 5818, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.022 [-1.485, 1.121], mean_best_reward: --
 97755/100000: episode: 2533, duration: 0.020s, episode steps: 120, steps per second: 6093, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.209 [-1.845, 1.394], mean_best_reward: --
 97840/100000: episode: 2534, duration: 0.014s, episode steps: 85, steps per second: 6096, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.157 [-0.989, 1.270], mean_best_reward: --
 97913/100000: episode: 2535, duration: 0.012s, episode steps: 73, steps per second: 6090, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.040 [-1.133, 1.124], mean_best_reward: --
 97949/100000: episode: 2536, duration: 0.007s, episode steps: 36, steps per second: 5428, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-0.888, 1.074], mean_best_reward: --
 98055/100000: episode: 2537, duration: 0.024s, episode steps: 106, steps per second: 4448, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-1.197, 0.947], mean_best_reward: --
 98132/100000: episode: 2538, duration: 0.012s, episode steps: 77, steps per second: 6254, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.012 [-1.168, 1.168], mean_best_reward: --
 98217/100000: episode: 2539, duration: 0.014s, episode steps: 85, steps per second: 5930, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.008 [-1.200, 1.040], mean_best_reward: --
 98338/100000: episode: 2540, duration: 0.020s, episode steps: 121, steps per second: 6088, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.047 [-0.927, 0.851], mean_best_reward: --
 98390/100000: episode: 2541, duration: 0.009s, episode steps: 52, steps per second: 6055, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.092 [-1.059, 0.792], mean_best_reward: --
 98505/100000: episode: 2542, duration: 0.019s, episode steps: 115, steps per second: 6148, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.183 [-1.673, 0.798], mean_best_reward: --
 98646/100000: episode: 2543, duration: 0.023s, episode steps: 141, steps per second: 6179, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.179 [-1.730, 1.289], mean_best_reward: --
 98681/100000: episode: 2544, duration: 0.006s, episode steps: 35, steps per second: 5951, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.047 [-1.185, 0.627], mean_best_reward: --
 98838/100000: episode: 2545, duration: 0.026s, episode steps: 157, steps per second: 6136, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.089 [-1.163, 1.481], mean_best_reward: --
 98919/100000: episode: 2546, duration: 0.013s, episode steps: 81, steps per second: 6258, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.140 [-1.700, 2.122], mean_best_reward: --
 99067/100000: episode: 2547, duration: 0.024s, episode steps: 148, steps per second: 6209, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.316 [-0.840, 1.685], mean_best_reward: --
 99149/100000: episode: 2548, duration: 0.014s, episode steps: 82, steps per second: 5948, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-1.230, 0.903], mean_best_reward: --
 99347/100000: episode: 2549, duration: 0.031s, episode steps: 198, steps per second: 6292, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.176 [-2.289, 1.651], mean_best_reward: --
 99449/100000: episode: 2550, duration: 0.018s, episode steps: 102, steps per second: 5710, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.056 [-1.529, 1.686], mean_best_reward: --
 99557/100000: episode: 2551, duration: 0.017s, episode steps: 108, steps per second: 6173, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.167 [-1.445, 1.297], mean_best_reward: 166.000000
 99604/100000: episode: 2552, duration: 0.008s, episode steps: 47, steps per second: 6103, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.108 [-1.353, 1.281], mean_best_reward: --
 99636/100000: episode: 2553, duration: 0.006s, episode steps: 32, steps per second: 5542, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.087 [-0.886, 0.388], mean_best_reward: --
 99742/100000: episode: 2554, duration: 0.017s, episode steps: 106, steps per second: 6115, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.082 [-0.924, 0.832], mean_best_reward: --
 99859/100000: episode: 2555, duration: 0.019s, episode steps: 117, steps per second: 6248, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.137 [-1.856, 1.516], mean_best_reward: --
 99909/100000: episode: 2556, duration: 0.008s, episode steps: 50, steps per second: 6149, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.121, 1.011], mean_best_reward: --
done, took 18.798 seconds
Testing for 10 episodes ...
Episode 1: reward: 200.000, steps: 200
Episode 2: reward: 200.000, steps: 200
Episode 3: reward: 200.000, steps: 200
Episode 4: reward: 200.000, steps: 200
Episode 5: reward: 200.000, steps: 200
Episode 6: reward: 200.000, steps: 200
Episode 7: reward: 200.000, steps: 200
Episode 8: reward: 200.000, steps: 200
Episode 9: reward: 200.000, steps: 200
Episode 10: reward: 200.000, steps: 200
